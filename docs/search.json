[
  {
    "objectID": "Datamining/new-posting/indesx.html",
    "href": "Datamining/new-posting/indesx.html",
    "title": "데이터 마이닝 정리",
    "section": "",
    "text": "파이썬 기초 정리 입니다."
  },
  {
    "objectID": "Datamining/new-posting/indesx.html#np.linspace-np.rand-np.randn",
    "href": "Datamining/new-posting/indesx.html#np.linspace-np.rand-np.randn",
    "title": "데이터 마이닝 정리",
    "section": "np.linspace / np.rand & np.randn",
    "text": "np.linspace / np.rand & np.randn\n1 > 부동 소수를 사용할 땐 arrange 대신 linspace함수를 사용 하는것이 좋음\nlinspace ? : 지정된 개수만큼 두 값 사이를 나눈 배열을 반환\n2 > np.rand & np.randn random 모듈안 ndarray를 랜덤한 값으로 초기화할 수 있음 -> np.fromfunction도 같은 함수\n\n## Ex code >\n# 1 > linspace\nprint(np.linspace(0, 5/3,6)) #0에서 5/3으로 나눔\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n\n\n\n# 2 > np.rand & np.randn\nnp.random.rand(3,4)\n\nnp.random.randn(3,4)\n\narray([[ 1.89418119e+00,  2.23107175e-02,  1.08644059e+00,\n        -3.33164009e-01],\n       [-1.13725933e+00,  4.57012494e-02,  3.71311743e-01,\n        -2.33917719e+00],\n       [-8.86186536e-02, -5.38598393e-01,  7.66998033e-03,\n         6.59483982e-04]])"
  },
  {
    "objectID": "Datamining/new-posting/indesx.html#배열-데이터",
    "href": "Datamining/new-posting/indesx.html#배열-데이터",
    "title": "데이터 마이닝 정리",
    "section": "배열 데이터",
    "text": "배열 데이터\ndtype : 데이터 타입 확인 및 지정\n\n데이터 타입 종류 : int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128\n\n\n#확인\nc = np.arange(1.0,5.0)\nprint(c.dtype,c)\n\n#지정\nd = np.arange(1,5,dtype = np.complex64)\nprint(d.dtype,d)\n\nfloat64 [1. 2. 3. 4.]\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n\n\nitemsize : 아이템의 크기(바이트) 반환\n\ne = np.arange(1, 5, dtype=np.complex64)\ne.itemsize\n\n8"
  },
  {
    "objectID": "Datamining/new-posting/indesx.html#배열-크기-변경",
    "href": "Datamining/new-posting/indesx.html#배열-크기-변경",
    "title": "데이터 마이닝 정리",
    "section": "배열 크기 변경",
    "text": "배열 크기 변경\n\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\n\n(1,2,3) -> 1: 차원 2: 행 , 3: 열\n\ng = np.arange(24)  #행\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n랭크: 1\n\n\n\ng.shape = (6, 4)  #행,열\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n랭크: 2\n\n\n\ng.shape = (2, 3, 4)   #차원, 행, 열\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n#Ex > 위의 17을 뽑아라\ng[1,1,1]\n\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n랭크: 3\n\n\n17\n\n\nreshape : 새로운 객체 반환\n\ng2 = g.reshape(4,6)  #4행 6열로 변환\nprint(g2)\nprint(\"랭크:\", g2.ndim)\n\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n랭크: 2\n\n\n지정 숫자 변환\n\ng[1,2] = 999\ng2  #결과 보면 1행2열이 999로 변환한것을 확인\n\narray([[  0,   1,   2,   3,   4,   5],\n       [  6,   7,   8,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19, 999, 999, 999, 999]])\n\n\nravel : 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환\n\ng.ravel()\n\narray([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19, 999, 999, 999, 999])"
  },
  {
    "objectID": "Datamining/new-posting/indesx.html#산술-연산",
    "href": "Datamining/new-posting/indesx.html#산술-연산",
    "title": "데이터 마이닝 정리",
    "section": "산술 연산",
    "text": "산술 연산\n: +,-,*,/,//,** 등"
  },
  {
    "objectID": "Datamining/new-posting/indesx.html#규칙",
    "href": "Datamining/new-posting/indesx.html#규칙",
    "title": "데이터 마이닝 정리",
    "section": "규칙",
    "text": "규칙\n1 배열의 랭크가 동일하지 않으면 랭크가 맞을 때 까지 랭크가 작은 배열 앞에 1을 추가함\n\nh = np.arange(5).reshape(1,1,5)\nh\n\narray([[[0, 1, 2, 3, 4]]])\n\n\n수를 더할수 있음\n\nh + [10, 20, 30, 40, 50]  \n\narray([[[10, 21, 32, 43, 54]]])\n\n\n2 가장 큰 배열의 크기에 맞춰 동작해야함\n\nk = np.arange(6).reshape(2, 3)\nk\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\n\narray([[100, 101, 102],\n       [203, 204, 205]])\n\n\n규칙 1 + 2\n\nk + [100, 200, 300]  # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\n\narray([[100, 201, 302],\n       [103, 204, 305]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html",
    "href": "Datamining/numpy/numpy.html",
    "title": "Numpy",
    "section": "",
    "text": "“numpy 기본 코드 실습(한글)”\n\n\ntoc:true\nbranch: master\nbadges: true\ncomments: true\ncategories: [jupyter, python]\n\n도구 - 넘파이(NumPy)\n*넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다. 넘파이의 핵심은 강력한 N-차원 배열 객체입니다. 또한 선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.”\n\n\n\n구글 코랩에서 실행하기"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#np.zeros",
    "href": "Datamining/numpy/numpy.html#np.zeros",
    "title": "Numpy",
    "section": "np.zeros",
    "text": "np.zeros\nzeros 함수는 0으로 채워진 배열을 만듭니다:\n\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\n2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 \\(3 \\times 4\\) 크기의 행렬입니다:\n\nnp.zeros((3,4))  \n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#용어",
    "href": "Datamining/numpy/numpy.html#용어",
    "title": "Numpy",
    "section": "용어",
    "text": "용어\n\n넘파이에서 각 차원을 축(axis) 이라고 합니다\n축의 개수를 랭크(rank) 라고 합니다.\n\n예를 들어, 위의 \\(3 \\times 4\\) 행렬은 랭크 2인 배열입니다(즉 2차원입니다).\n첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다.\n\n배열의 축 길이를 배열의 크기(shape)라고 합니다.\n\n예를 들어, 위 행렬의 크기는 (3, 4)입니다.\n랭크는 크기의 길이와 같습니다.\n\n배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, \\(3 \\times 4=12\\)).\n\n\na = np.zeros((3,4))\na\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\n\na.shape\n\n(3, 4)\n\n\n\na.ndim  # len(a.shape)와 같습니다\n\n2\n\n\n\na.size\n\n12"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#n-차원-배열",
    "href": "Datamining/numpy/numpy.html#n-차원-배열",
    "title": "Numpy",
    "section": "N-차원 배열",
    "text": "N-차원 배열\n임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다:\n\nnp.zeros((2,3,4))  #뒤에서 읽기 \n\narray([[[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#배열-타입",
    "href": "Datamining/numpy/numpy.html#배열-타입",
    "title": "Numpy",
    "section": "배열 타입",
    "text": "배열 타입\n넘파이 배열의 타입은 ndarray입니다:\n\ntype(np.zeros((3,4))) \n\nnumpy.ndarray"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#np.ones",
    "href": "Datamining/numpy/numpy.html#np.ones",
    "title": "Numpy",
    "section": "np.ones",
    "text": "np.ones\nndarray를 만들 수 있는 넘파이 함수가 많습니다.\n다음은 1로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다:\n\nnp.ones((3,4))  #0대신1로 채움\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#np.full",
    "href": "Datamining/numpy/numpy.html#np.full",
    "title": "Numpy",
    "section": "np.full",
    "text": "np.full\n주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\n\nnp.full((3,4), np.pi)  #다른 값으로 채울수 있는거\n\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#np.empty",
    "href": "Datamining/numpy/numpy.html#np.empty",
    "title": "Numpy",
    "section": "np.empty",
    "text": "np.empty\n초기화되지 않은 \\(2 \\times 3\\) 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다):\n\nnp.empty((2,3))\n\narray([[1.18508957e-316, 0.00000000e+000, 0.00000000e+000],\n       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#np.array",
    "href": "Datamining/numpy/numpy.html#np.array",
    "title": "Numpy",
    "section": "np.array",
    "text": "np.array\narray 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다:\n\nnp.array([[1,2,3,4], [10, 20, 30, 40]])  \n\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40]])\n\n\n\n[[1,2,3,4],[10,20,30,40]] #이렇게도 씀"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#np.arange",
    "href": "Datamining/numpy/numpy.html#np.arange",
    "title": "Numpy",
    "section": "np.arange",
    "text": "np.arange\n파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다:\n\nnp.arange(1, 5)  #1부터 5까지 (마지막 빼고)\n\narray([1, 2, 3, 4])\n\n\n부동 소수도 가능합니다:\n\nnp.arange(1.0, 5.0)\n\narray([1., 2., 3., 4.])\n\n\n파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다:\n\nnp.arange(1, 5, 0.5)  #간격 정하기 \n\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n\n\n부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다:\n\nprint(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다.  #\nprint(np.arange(0, 5/3, 0.333333333))\nprint(np.arange(0, 5/3, 0.333333334))\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333334]\n\n\nfor loops를 사용하지 않고 전체array에 대한 인산 수행이 가능합니다. 평균적으로 numpy based알고리즘은 10~100배 정도 속도가 더 빠르고 적은 메모리를 사용합니다\n\nmy_arr= np.arange(10000)\nmy_list= list(range(10000))\n\n%time for _ in range(10): my_arr2 = my_arr * 2\n%time for _ in range(10): my_list2 = [x * 2 for x in my_list]\n\nWall time: 995 µs\nWall time: 15.3 ms\n\n\n\nimport sys\n\nsize = 10\n\n%timeit for x in range(size): x ** 2\n#out :10 loops, best of:3 136 ms per loop\n\n%timeit for x in np.arange(size): x ** 2\n\n%timeit np.arange(size) ** 2\n\n4.48 µs ± 937 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n5.13 µs ± 1.32 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n3.35 µs ± 654 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#np.linspace",
    "href": "Datamining/numpy/numpy.html#np.linspace",
    "title": "Numpy",
    "section": "np.linspace",
    "text": "np.linspace\n이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다):\n\nprint(np.linspace(0, 5/3, 6)) #0에서 5/3으로 나눔\n\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#np.rand와-np.randn",
    "href": "Datamining/numpy/numpy.html#np.rand와-np.randn",
    "title": "Numpy",
    "section": "np.rand와 np.randn",
    "text": "np.rand와 np.randn\n넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 \\(3 \\times 4\\) 행렬을 초기화합니다:\n\nnp.random.rand(3,4)\n\narray([[0.71933697, 0.22431105, 0.5619093 , 0.79713127],\n       [0.20912102, 0.0087656 , 0.24665546, 0.09543205],\n       [0.52346983, 0.09626499, 0.56395157, 0.77344584]])\n\n\n다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 \\(3 \\times 4\\) 행렬입니다:\n\nnp.random.randn(3,4)\n\narray([[ 0.63005861, -0.7885544 , -0.57166913,  0.91264422],\n       [ 0.68545673,  0.32831736, -1.43995984, -0.87100504],\n       [ 0.71137961, -0.60209212,  0.12301674,  1.34292602]])\n\n\n이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요):\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\nplt.hist(np.random.rand(100000), density=True, bins=100, histtype=\"step\", color=\"blue\", label=\"rand\")\nplt.hist(np.random.randn(100000), density=True, bins=100, histtype=\"step\", color=\"red\", label=\"randn\")\nplt.axis([-2.5, 2.5, 0, 1.1])\nplt.legend(loc = \"upper left\")\nplt.title(\"Random distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#np.fromfunction",
    "href": "Datamining/numpy/numpy.html#np.fromfunction",
    "title": "Numpy",
    "section": "np.fromfunction",
    "text": "np.fromfunction\n함수를 사용하여 ndarray를 초기화할 수도 있습니다:\n\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\nnp.fromfunction(my_function, (3, 2, 10))\n\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n\n\n넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다:\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\n위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다."
  },
  {
    "objectID": "Datamining/numpy/numpy.html#dtype",
    "href": "Datamining/numpy/numpy.html#dtype",
    "title": "Numpy",
    "section": "dtype",
    "text": "dtype\n넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다:\n\nc = np.arange(1, 5)\nprint(c.dtype, c)\n\nint64 [1 2 3 4]\n\n\n\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\n\nfloat64 [1. 2. 3. 4.]\n\n\n넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다:\n\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\n\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n\n\n가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "Datamining/numpy/numpy.html#itemsize",
    "href": "Datamining/numpy/numpy.html#itemsize",
    "title": "Numpy",
    "section": "itemsize",
    "text": "itemsize\nitemsize 속성은 각 아이템의 크기(바이트)를 반환합니다:\n\ne = np.arange(1, 5, dtype=np.complex64)\ne.itemsize\n\n8"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#data-버퍼",
    "href": "Datamining/numpy/numpy.html#data-버퍼",
    "title": "Numpy",
    "section": "data 버퍼",
    "text": "data 버퍼\n배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요).\n\nf = np.array([[1,2],[1000, 2000]], dtype=np.int32)\nf.data\n\n<memory at 0x7f97929dd790>\n\n\n파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다.\n\nif (hasattr(f.data, \"tobytes\")):\n    data_bytes = f.data.tobytes() # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes() # python 2\n\ndata_bytes\n\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n\n\n여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다."
  },
  {
    "objectID": "Datamining/numpy/numpy.html#자신을-변경",
    "href": "Datamining/numpy/numpy.html#자신을-변경",
    "title": "Numpy",
    "section": "자신을 변경",
    "text": "자신을 변경\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\n\ng = np.arange(24)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n랭크: 1\n\n\n\ng.shape = (6, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n랭크: 2\n\n\n\ng.shape = (2, 3, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n랭크: 3\n\n\n\n#17뽑고 싶을때\ng[1,1,1]  #[범위 , 행, 그 행에 있는 자리]\n\n17"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#reshape",
    "href": "Datamining/numpy/numpy.html#reshape",
    "title": "Numpy",
    "section": "reshape",
    "text": "reshape\nreshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다.\n\ng2 = g.reshape(4,6)\nprint(g2)\nprint(\"랭크:\", g2.ndim)\n\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n랭크: 2\n\n\n행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요).\n\ng2[1, 2] = 999\ng2\n\narray([[  0,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n\n\n이에 상응하는 g의 원소도 수정됩니다.\n\ng\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#ravel",
    "href": "Datamining/numpy/numpy.html#ravel",
    "title": "Numpy",
    "section": "ravel",
    "text": "ravel\n마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다:\n\ng.ravel() \n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#규칙-1",
    "href": "Datamining/numpy/numpy.html#규칙-1",
    "title": "Numpy",
    "section": "규칙 1",
    "text": "규칙 1\n배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다.\n\nh = np.arange(5).reshape(1, 1, 5)\nh \n\narray([[[0, 1, 2, 3, 4]]])\n\n\n여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다!\n\nh + [10, 20, 30, 40, 50]  # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]]\n\narray([[[10, 21, 32, 43, 54]]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#규칙-2",
    "href": "Datamining/numpy/numpy.html#규칙-2",
    "title": "Numpy",
    "section": "규칙 2",
    "text": "규칙 2\n특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다.\n\nk = np.arange(6).reshape(2, 3)\nk\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n(2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다:\n\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\n\narray([[100, 101, 102],\n       [203, 204, 205]])\n\n\n규칙 1과 2를 합치면 다음과 같이 동작합니다:\n\nk + [100, 200, 300]  # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\n\narray([[100, 201, 302],\n       [103, 204, 305]])\n\n\n또 매우 간단히 다음 처럼 해도 됩니다:\n\nk + 1000  # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\n\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#규칙-3",
    "href": "Datamining/numpy/numpy.html#규칙-3",
    "title": "Numpy",
    "section": "규칙 3",
    "text": "규칙 3\n규칙 1 & 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다.\n\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\n\noperands could not be broadcast together with shapes (2,3) (2,) \n\n\n브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "Datamining/numpy/numpy.html#업캐스팅",
    "href": "Datamining/numpy/numpy.html#업캐스팅",
    "title": "Numpy",
    "section": "업캐스팅",
    "text": "업캐스팅\ndtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다.\n\nk1 = np.arange(0, 5, dtype=np.uint8)\nprint(k1.dtype, k1)\n\nuint8 [0 1 2 3 4]\n\n\n\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\n\nint16 [ 5  7  9 11 13]\n\n\n모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다.\n\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\n\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#ndarray-메서드",
    "href": "Datamining/numpy/numpy.html#ndarray-메서드",
    "title": "Numpy",
    "section": "ndarray 메서드",
    "text": "ndarray 메서드\n일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\nprint(\"평균 =\", a.mean())\n\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n평균 = 6.766666666666667\n\n\n(None, array([3.75, 7.05, 9.5 ]))\n\n\n이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다.\n다음은 유용한 ndarray 메서드입니다:\n\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var):\n    print(func.__name__, \"=\", func())\n\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n\n\n이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면:\n\nc=np.arange(24).reshape(2,3,4) \nc\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nc.sum(axis=0)  # 첫 번째 축을 따라 더함, 결과는 3x4 배열\n\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\n\n\n\nc.sum(axis=1) #두 번재 축을 따라 더함,결과는 2x4 #(2,3,4)에서 3부분을 의미해 열로 더함\n\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\n\n\n\nc.sum(axis=2)  # 세 번째 축을 따라 더함, 결과는 2x3 #(2,3,4)에서 4부분을 의미함-> 행으로 더함\n\narray([[ 6, 22, 38],\n       [54, 70, 86]])\n\n\n여러 축에 대해서 더할 수도 있습니다:\n\nc.sum(axis=(0,2))  # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열\n\narray([ 60,  92, 124])\n\n\n\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n\n(60, 92, 124)"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#일반-함수",
    "href": "Datamining/numpy/numpy.html#일반-함수",
    "title": "Numpy",
    "section": "일반 함수",
    "text": "일반 함수\n넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다:\n\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nnp.square(a)\n\narray([[  6.25,   9.61,  49.  ],\n       [100.  , 121.  , 144.  ]])\n\n\n다음은 유용한 단항 일반 함수들입니다:\n\nprint(\"원본 ndarray\")\nprint(a)\nfor func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos):\n    print(\"\\n\", func.__name__)\n    print(func(a))\n\n원본 ndarray\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n absolute\n[[ 2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n sqrt\n[[       nan 1.76068169 2.64575131]\n [3.16227766 3.31662479 3.46410162]]\n\n exp\n[[8.20849986e-02 2.21979513e+01 1.09663316e+03]\n [2.20264658e+04 5.98741417e+04 1.62754791e+05]]\n\n log\n[[       nan 1.13140211 1.94591015]\n [2.30258509 2.39789527 2.48490665]]\n\n sign\n[[-1.  1.  1.]\n [ 1.  1.  1.]]\n\n ceil\n[[-2.  4.  7.]\n [10. 11. 12.]]\n\n modf\n(array([[-0.5,  0.1,  0. ],\n       [ 0. ,  0. ,  0. ]]), array([[-2.,  3.,  7.],\n       [10., 11., 12.]]))\n\n isnan\n[[False False False]\n [False False False]]\n\n cos\n[[-0.80114362 -0.99913515  0.75390225]\n [-0.83907153  0.0044257   0.84385396]]\n\n\nRuntimeWarning: invalid value encountered in sqrt\n  print(func(a))\n<ipython-input-59-d791c8e37e6f>:5: RuntimeWarning: invalid value encountered in log\n  print(func(a))"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#이항-일반-함수",
    "href": "Datamining/numpy/numpy.html#이항-일반-함수",
    "title": "Numpy",
    "section": "이항 일반 함수",
    "text": "이항 일반 함수\n두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다:\n\na = np.array([1, -2, 3, 4])\nb = np.array([2, 8, -1, 7])\nnp.add(a, b)  # a + b 와 동일\n\narray([ 3,  6,  2, 11])\n\n\n\nnp.greater(a, b)  # a > b 와 동일\n\narray([False, False,  True, False])\n\n\n\nnp.maximum(a, b)\n\narray([2, 8, 3, 7])\n\n\n\nnp.copysign(a, b)\n\narray([ 1.,  2., -3.,  4.])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#차원-배열",
    "href": "Datamining/numpy/numpy.html#차원-배열",
    "title": "Numpy",
    "section": "1차원 배열",
    "text": "1차원 배열\n1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다:\n\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n\n19\n\n\n\na[2:5]\n\narray([ 3, 19, 13])\n\n\n\na[2:-1]\n\narray([ 3, 19, 13,  7])\n\n\n\na[:2]\n\narray([1, 5])\n\n\n\na[2::2]  #앞 뒤로 해당하는거\n\narray([ 3, 13,  3])\n\n\n\na[::-1]\n\narray([ 3,  7, 13, 19,  3,  5,  1])\n\n\n물론 원소를 수정할 수 있죠:\n\na[3]=999\na\n\narray([  1,   5,   3, 999,  13,   7,   3])\n\n\n슬라이싱을 사용해 ndarray를 수정할 수 있습니다:\n\na[2:5] = [997, 998, 999]\na\n\narray([  1,   5, 997, 998, 999,   7,   3])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#보통의-파이썬-배열과-차이점",
    "href": "Datamining/numpy/numpy.html#보통의-파이썬-배열과-차이점",
    "title": "Numpy",
    "section": "보통의 파이썬 배열과 차이점",
    "text": "보통의 파이썬 배열과 차이점\n보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다.\n\na[2:5] = -1\na\n\narray([ 1,  5, -1, -1, -1,  7,  3])\n\n\n또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다:\n\ntry:\n    a[2:5] = [1,2,3,4,5,6]  # 너무 길어요\nexcept ValueError as e:\n    print(e)\n\ncannot copy sequence with size 6 to array axis with dimension 3\n\n\n원소를 삭제할 수도 없습니다:\n\ntry:\n    del a[2:5]\nexcept ValueError as e:\n    print(e)\n\ncannot delete array elements\n\n\n중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다!\n\na_slice = a[2:6]\na_slice[1] = 1000\na  # 원본 배열이 수정됩니다!\n\narray([   1,    5,   -1, 1000,   -1,    7,    3])\n\n\n\na[3] = 2000\na_slice  # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다!\n\narray([  -1, 2000,   -1,    7])\n\n\n데이터를 복사하려면 copy 메서드를 사용해야 합니다:\n\nanother_slice = a[2:6].copy()\nanother_slice[1] = 3000\na  # 원본 배열이 수정되지 않습니다\n\narray([   1,    5,   -1, 2000,   -1,    7,    3])\n\n\n\na[3] = 4000\nanother_slice  # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다\n\narray([  -1, 3000,   -1,    7])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#다차원-배열",
    "href": "Datamining/numpy/numpy.html#다차원-배열",
    "title": "Numpy",
    "section": "다차원 배열",
    "text": "다차원 배열\n다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다:\n\nb = np.arange(48).reshape(4, 12)  #np.arrange(48)  #1차원\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\n#24,25,36,37뽑는 방법 1)\nb[(2,3),0:2]\n\narray([[24, 25],\n       [36, 37]])\n\n\n\n#뽑는 방법 2)\nb[2:,0:2]\n\narray([[24, 25],\n       [36, 37]])\n\n\n\nb[1, 2]  # 행 1, 열 2\n\n14\n\n\n\nb[1, :]  # 행 1, 모든 열\n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[:, 1]  # 모든 행, 열 1\n\narray([ 1, 13, 25, 37])\n\n\n\nb[1,:].shape  #1차원\n\n(12,)\n\n\n\nb[1:2,:].shape  #2차원\n\n(1, 12)\n\n\n주의: 다음 두 표현에는 미묘한 차이가 있습니다:\n\nb[1, :] \n\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n\n\n\nb[1:2, :]\n\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n\n\n첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다."
  },
  {
    "objectID": "Datamining/numpy/numpy.html#팬시-인덱싱fancy-indexing",
    "href": "Datamining/numpy/numpy.html#팬시-인덱싱fancy-indexing",
    "title": "Numpy",
    "section": "팬시 인덱싱(Fancy indexing)",
    "text": "팬시 인덱싱(Fancy indexing)\n관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다.\n\nb[(0,2), 2:5]  # 행 0과 2, 열 2에서 4(5-1)까지\n\narray([[ 2,  3,  4],\n       [26, 27, 28]])\n\n\n\nb[:, (-1, 2, -1)]  # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로)\n\narray([[11,  2, 11],\n       [23, 14, 23],\n       [35, 26, 35],\n       [47, 38, 47]])\n\n\n여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다.\n\nb[(-1, 2, -1, 2), (5, 9, 1, 9)]  # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again)\n\narray([41, 33, 37, 33])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#고차원",
    "href": "Datamining/numpy/numpy.html#고차원",
    "title": "Numpy",
    "section": "고차원",
    "text": "고차원\n고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다:\n\nc = b.reshape(4,2,6)\nc\n\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]],\n\n       [[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]],\n\n       [[36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]]])\n\n\n\nc[2, 1, 4]  # 행렬 2, 행 1, 열 4\n\n34\n\n\n\nc[2, :, 3]  # 행렬 2, 모든 행, 열 3\n\narray([27, 33])\n\n\n어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다:\n\nc[2, 1]  # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다.\n\narray([30, 31, 32, 33, 34, 35])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#생략-부호-...",
    "href": "Datamining/numpy/numpy.html#생략-부호-...",
    "title": "Numpy",
    "section": "생략 부호 (...)",
    "text": "생략 부호 (...)\n생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다.\n\nc[2, ...]  #  행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일\n\narray([[24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n\n\n\nc[2, 1, ...]  # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일\n\narray([30, 31, 32, 33, 34, 35])\n\n\n\nc[2, ..., 3]  # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일\n\narray([27, 33])\n\n\n\nc[..., 3]  # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일\n\narray([[ 3,  9],\n       [15, 21],\n       [27, 33],\n       [39, 45]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#불리언-인덱싱",
    "href": "Datamining/numpy/numpy.html#불리언-인덱싱",
    "title": "Numpy",
    "section": "불리언 인덱싱",
    "text": "불리언 인덱싱\n불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다.\n\nb = np.arange(48).reshape(4, 12)\nb\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n\n\n\nrows_on = np.array([True, False, True, False]) #열맏 True / False가 나뉨\nb[rows_on, :]  # 행 0과 2, 모든 열. b[(0, 2), :]와 동일\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n\n\n\ncols_on = np.array([False, True, False] * 4) #  * :  4번 반복한다는 뜻\nb[:, cols_on]  # 모든 행, 열 1, 4, 7, 10\n\narray([[ 1,  4,  7, 10],\n       [13, 16, 19, 22],\n       [25, 28, 31, 34],\n       [37, 40, 43, 46]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#np.ix_",
    "href": "Datamining/numpy/numpy.html#np.ix_",
    "title": "Numpy",
    "section": "np.ix_",
    "text": "np.ix_\n여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다:\n\nb[np.ix_((0,2),(1,4,7,10))]\n\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\n\n\nb[np.ix_(rows_on, cols_on)]\n\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\n\n\n\nnp.ix_(rows_on, cols_on)\n\n(array([[0],\n        [2]], dtype=int64),\n array([[ 1,  4,  7, 10]], dtype=int64))\n\n\nndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다:\n\nb[b % 3 == 1]  #True/ False 가 아니라 조건을 넣어도 됨\n\narray([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#vstack",
    "href": "Datamining/numpy/numpy.html#vstack",
    "title": "Numpy",
    "section": "vstack",
    "text": "vstack\nvstack 함수를 사용하여 수직으로 쌓아보죠:\n\nq4 = np.vstack((q1, q2, q3))\nq4\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq4.shape\n\n(10, 4)\n\n\nq1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다)."
  },
  {
    "objectID": "Datamining/numpy/numpy.html#hstack",
    "href": "Datamining/numpy/numpy.html#hstack",
    "title": "Numpy",
    "section": "hstack",
    "text": "hstack\nhstack을 사용해 수평으로도 쌓을 수 있습니다:\n\nq5 = np.hstack((q1, q3))\nq5\n\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n\n\n\nq5.shape\n\n(3, 8)\n\n\nq1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다:\n\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\n\nall the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#concatenate",
    "href": "Datamining/numpy/numpy.html#concatenate",
    "title": "Numpy",
    "section": "concatenate",
    "text": "concatenate\nconcatenate 함수는 지정한 축으로도 배열을 쌓습니다.\n\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstack과 동일\nq7  #결과 -> 10,4가 됨\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n\n\nq7.shape\n\n(10, 4)\n\n\n예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다."
  },
  {
    "objectID": "Datamining/numpy/numpy.html#stack",
    "href": "Datamining/numpy/numpy.html#stack",
    "title": "Numpy",
    "section": "stack",
    "text": "stack\nstack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다.\n\nq8 = np.stack((q1, q3))\nq8\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#행렬-전치",
    "href": "Datamining/numpy/numpy.html#행렬-전치",
    "title": "Numpy",
    "section": "행렬 전치",
    "text": "행렬 전치\nT 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다:\n\nm1 = np.arange(10).reshape(2,5)\nm1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\nm1.T\n\narray([[0, 5],\n       [1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\n\n\nT 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다:\n\nm2 = np.arange(5)\nm2\n\narray([0, 1, 2, 3, 4])\n\n\n\nm2.T\n\narray([0, 1, 2, 3, 4])\n\n\n먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다:\n\nm2r = m2.reshape(1,5)\nm2r\n\narray([[0, 1, 2, 3, 4]])\n\n\n\nm2r.T\n\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#행렬-곱셈",
    "href": "Datamining/numpy/numpy.html#행렬-곱셈",
    "title": "Numpy",
    "section": "행렬 곱셈",
    "text": "행렬 곱셈\n두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠.\n\nn1 = np.arange(10).reshape(2, 5)\nn1\n\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n\n\n\n#np.save(\"my_array\",a)  #결과갑 저장하는것\n\n\n#a_loaded = np.loae(\"my_array.npy\")  #불러오기\n#a_loaded\n\n\n#np.savez(\"my_arrays\",my_a=a,my_b=b) #여러개 저장하는건 z붙여서\n\n\n#list(my_arrays.key()) #key값으로 이름 지정하기\n\n\nn2 = np.arange(15).reshape(5,3)\nn2\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\n\n\n\nn1.dot(n2)\n\narray([[ 90, 100, 110],\n       [240, 275, 310]])\n\n\n주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다."
  },
  {
    "objectID": "Datamining/numpy/numpy.html#역행렬과-유사-역행렬",
    "href": "Datamining/numpy/numpy.html#역행렬과-유사-역행렬",
    "title": "Numpy",
    "section": "역행렬과 유사 역행렬",
    "text": "역행렬과 유사 역행렬\nnumpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다:\n\nimport numpy.linalg as linalg\n\nm3 = np.array([[1,2,3],[5,7,11],[21,29,31]])\nm3\n\narray([[ 1,  2,  3],\n       [ 5,  7, 11],\n       [21, 29, 31]])\n\n\n\nlinalg.inv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])\n\n\npinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다:\n\nlinalg.pinv(m3)\n\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#단위-행렬",
    "href": "Datamining/numpy/numpy.html#단위-행렬",
    "title": "Numpy",
    "section": "단위 행렬",
    "text": "단위 행렬\n행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다):\n\nm3.dot(linalg.inv(m3))\n\narray([[ 1.00000000e+00, -1.66533454e-16,  0.00000000e+00],\n       [ 6.31439345e-16,  1.00000000e+00, -1.38777878e-16],\n       [ 5.21110932e-15, -2.38697950e-15,  1.00000000e+00]])\n\n\neye 함수는 NxN 크기의 단위 행렬을 만듭니다:\n\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#qr-분해",
    "href": "Datamining/numpy/numpy.html#qr-분해",
    "title": "Numpy",
    "section": "QR 분해",
    "text": "QR 분해\nqr 함수는 행렬을 QR 분해합니다:\n\nq, r = linalg.qr(m3)\nq\n\narray([[-0.04627448,  0.98786672,  0.14824986],\n       [-0.23137241,  0.13377362, -0.96362411],\n       [-0.97176411, -0.07889213,  0.22237479]])\n\n\n\nr\n\narray([[-21.61018278, -29.89331494, -32.80860727],\n       [  0.        ,   0.62427688,   1.9894538 ],\n       [  0.        ,   0.        ,  -3.26149699]])\n\n\n\nq.dot(r)  # q.r는 m3와 같습니다\n\narray([[ 1.,  2.,  3.],\n       [ 5.,  7., 11.],\n       [21., 29., 31.]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#행렬식",
    "href": "Datamining/numpy/numpy.html#행렬식",
    "title": "Numpy",
    "section": "행렬식",
    "text": "행렬식\ndet 함수는 행렬식을 계산합니다:\n\nlinalg.det(m3)  # 행렬식 계산\n\n43.99999999999997"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#고윳값과-고유벡터",
    "href": "Datamining/numpy/numpy.html#고윳값과-고유벡터",
    "title": "Numpy",
    "section": "고윳값과 고유벡터",
    "text": "고윳값과 고유벡터\neig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다:\n\neigenvalues, eigenvectors = linalg.eig(m3)\neigenvalues # λ\n\narray([42.26600592, -0.35798416, -2.90802176])\n\n\n\neigenvectors # v\n\narray([[-0.08381182, -0.76283526, -0.18913107],\n       [-0.3075286 ,  0.64133975, -0.6853186 ],\n       [-0.94784057, -0.08225377,  0.70325518]])\n\n\n\nm3.dot(eigenvectors) - eigenvalues * eigenvectors  # m3.v - λ*v = 0\n\narray([[ 8.88178420e-15,  2.22044605e-16, -3.10862447e-15],\n       [ 3.55271368e-15,  2.02615702e-15, -1.11022302e-15],\n       [ 3.55271368e-14,  3.33413852e-15, -8.43769499e-15]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#특잇값-분해",
    "href": "Datamining/numpy/numpy.html#특잇값-분해",
    "title": "Numpy",
    "section": "특잇값 분해",
    "text": "특잇값 분해\nsvd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다:\n\nm4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]])\nm4\n\narray([[1, 0, 0, 0, 2],\n       [0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0]])\n\n\n\nU, S_diag, V = linalg.svd(m4)\nU\n\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.],\n       [ 0.,  0.,  1.,  0.]])\n\n\n\nS_diag\n\narray([3.        , 2.23606798, 2.        , 0.        ])\n\n\nsvd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다:\n\nS = np.zeros((4, 5))\nS[np.diag_indices(4)] = S_diag\nS  # Σ\n\narray([[3.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 2.23606798, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 2.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ]])\n\n\n\nV\n\narray([[-0.        ,  0.        ,  1.        , -0.        ,  0.        ],\n       [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n       [-0.        ,  1.        ,  0.        , -0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]])\n\n\n\nU.dot(S).dot(V) # U.Σ.V == m4\n\narray([[1., 0., 0., 0., 2.],\n       [0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0.]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#대각원소와-대각합",
    "href": "Datamining/numpy/numpy.html#대각원소와-대각합",
    "title": "Numpy",
    "section": "대각원소와 대각합",
    "text": "대각원소와 대각합\n\nnp.diag(m3)  # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래)\n\narray([ 1,  7, 31])\n\n\n\nnp.trace(m3)  # np.diag(m3).sum()와 같습니다\n\n39"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#선형-방정식-풀기",
    "href": "Datamining/numpy/numpy.html#선형-방정식-풀기",
    "title": "Numpy",
    "section": "선형 방정식 풀기",
    "text": "선형 방정식 풀기\nsolve 함수는 다음과 같은 선형 방정식을 풉니다:\n\n\\(2x + 6y = 6\\)\n\\(5x + 3y = -9\\)\n\n\ncoeffs  = np.array([[2, 6], [5, 3]])\ndepvars = np.array([6, -9])\nsolution = linalg.solve(coeffs, depvars)\nsolution\n\narray([-3.,  2.])\n\n\nsolution을 확인해 보죠:\n\ncoeffs.dot(solution), depvars  # 네 같네요\n\n(array([ 6., -9.]), array([ 6, -9]))\n\n\n좋습니다! 다른 방식으로도 solution을 확인해 보죠:\n\nnp.allclose(coeffs.dot(solution), depvars)\n\nTrue"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#바이너리-.npy-포맷",
    "href": "Datamining/numpy/numpy.html#바이너리-.npy-포맷",
    "title": "Numpy",
    "section": "바이너리 .npy 포맷",
    "text": "바이너리 .npy 포맷\n랜덤 배열을 만들고 저장해 보죠.\n\na = np.random.rand(2,3)\na\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])\n\n\n\nnp.save(\"my_array\", a)\n\n끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다:\n\nwith open(\"my_array.npy\", \"rb\") as f:\n    content = f.read()\n\ncontent\n\nb\"\\x93NUMPY\\x01\\x00v\\x00{'descr': '<f8', 'fortran_order': False, 'shape': (2, 3), }                                                          \\nY\\xc1\\xfc\\xd0\\x1ee\\xe1?\\xde{3\\t?\\xb9\\xed?\\x80V\\x08\\xef\\xa5p\\x8f?\\x96I}\\xe0J\\x9b\\xda?\\xe0U\\xfaav \\xed?\\xd8\\xe50\\xc59\\xa4\\xe1?\"\n\n\n이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다:\n\na_loaded = np.load(\"my_array.npy\")\na_loaded\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#텍스트-포맷",
    "href": "Datamining/numpy/numpy.html#텍스트-포맷",
    "title": "Numpy",
    "section": "텍스트 포맷",
    "text": "텍스트 포맷\n배열을 텍스트 포맷으로 저장해 보죠:\n\nnp.savetxt(\"my_array.csv\", a)\n\n파일 내용을 확인해 보겠습니다:\n\nwith open(\"my_array.csv\", \"rt\") as f:\n    print(f.read())\n\n5.435937959464737235e-01 9.288630656918674955e-01 1.535157809943688001e-02\n4.157283012656532994e-01 9.102126992826775620e-01 5.512970782648904944e-01\n\n\n\n이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다:\n\nnp.savetxt(\"my_array.csv\", a, delimiter=\",\")\n\n이 파일을 로드하려면 loadtxt 함수를 사용합니다:\n\na_loaded = np.loadtxt(\"my_array.csv\", delimiter=\",\")\na_loaded\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Datamining/numpy/numpy.html#압축된-.npz-포맷",
    "href": "Datamining/numpy/numpy.html#압축된-.npz-포맷",
    "title": "Numpy",
    "section": "압축된 .npz 포맷",
    "text": "압축된 .npz 포맷\n여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다:\n\nb = np.arange(24, dtype=np.uint8).reshape(2, 3, 4)\nb\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]], dtype=uint8)\n\n\n\nnp.savez(\"my_arrays\", my_a=a, my_b=b)\n\n파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다.\n\nwith open(\"my_arrays.npz\", \"rb\") as f:\n    content = f.read()\n\nrepr(content)[:180] + \"[...]\"\n\n'b\"PK\\\\x03\\\\x04\\\\x14\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00!\\\\x00\\\\x063\\\\xcf\\\\xb9\\\\xb0\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x08\\\\x00\\\\x14\\\\x00my_a.npy\\\\x01\\\\x00\\\\x10\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x[...]'\n\n\n다음과 같이 이 파일을 로드할 수 있습니다:\n\nmy_arrays = np.load(\"my_arrays.npz\")\nmy_arrays\n\n<numpy.lib.npyio.NpzFile at 0x7f9791c73d60>\n\n\n게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다:\n\nmy_arrays.keys()\n\nKeysView(<numpy.lib.npyio.NpzFile object at 0x7f9791c73d60>)\n\n\n\nmy_arrays[\"my_a\"]\n\narray([[0.5435938 , 0.92886307, 0.01535158],\n       [0.4157283 , 0.9102127 , 0.55129708]])"
  },
  {
    "objectID": "Datamining/pandas/pandas.html",
    "href": "Datamining/pandas/pandas.html",
    "title": "Pandas",
    "section": "",
    "text": "“pandas 기본 코드 실습(한글)”\n\n\ntoc:true\nbranch: master\nbadges: true\ncomments: true\nauthor: Jiho Yeo\ncategories: [jupyter, python]\n\n도구 - 판다스(pandas)\npandas 라이브러리는 사용하기 쉬운 고성능 데이터 구조와 데이터 분석 도구를 제공합니다. 주 데이터 구조는 DataFrame입니다. 이를 인-메모리(in-memory) 2D 테이블로 생각할 수 있습니다(열 이름과 행 레이블이 있는 스프레드시트와 비슷합니다). 엑셀에 있는 많은 기능을 프로그램에서 사용할 수 있습니다. 여기에는 피봇 테이블이나 다른 열을 기반으로 열을 계산하고 그래프 출력하는 기능 등이 포함됩니다. 열 값으로 행을 그룹핑할 수도 있습니다. 또한 SQL과 비슷하게 테이블을 조인할 수 있습니다. 판다스는 시계열 데이터를 다루는데도 뛰어납니다.\n필요 라이브러리:\n\n넘파이(NumPy) – 넘파이에 익숙하지 않다면 지금 넘파이 튜토리얼을 둘러 보세요.\n\n\n\n\n구글 코랩에서 실행하기"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#series-만들기",
    "href": "Datamining/pandas/pandas.html#series-만들기",
    "title": "Pandas",
    "section": "Series 만들기",
    "text": "Series 만들기\n첫 번째 Series 객체를 만들어 보죠!\n\nimport numpy as np\nnp.array([2,-1,3,5])\n\narray([ 2, -1,  3,  5])\n\n\n\ns = pd.Series([2,-1,3,5])\ns\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#d-ndarray와-비슷합니다",
    "href": "Datamining/pandas/pandas.html#d-ndarray와-비슷합니다",
    "title": "Pandas",
    "section": "1D ndarray와 비슷합니다",
    "text": "1D ndarray와 비슷합니다\nSeries 객체는 넘파이 ndarray와 비슷하게 동작합니다. 넘파이 함수에 매개변수로 종종 전달할 수 있습니다:\n\nimport numpy as np\nnp.exp(s)  #지수 함수 값을 반환 한것\n\n0      7.389056\n1      0.367879\n2     20.085537\n3    148.413159\ndtype: float64\n\n\nSeries 객체에 대한 산술 연산도 가능합니다. ndarray와 비슷하게 원소별로 적용됩니다:\n\ns + [1000,2000,3000,4000]\n\n0    1002\n1    1999\n2    3003\n3    4005\ndtype: int64\n\n\n넘파이와 비슷하게 Series에 하나의 숫자를 더하면 Series에 있는 모든 원소에 더해집니다. 이를 브로드캐스팅(broadcasting)이라고 합니다:\n\ns\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64\n\n\n\ns + 1000\n\n0    1002\n1     999\n2    1003\n3    1005\ndtype: int64\n\n\n*나 / 같은 모든 이항 연산과 심지어 조건 연산에서도 마찬가지입니다:\n\ns < 0\n\n0    False\n1     True\n2    False\n3    False\ndtype: bool"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#인덱스-레이블",
    "href": "Datamining/pandas/pandas.html#인덱스-레이블",
    "title": "Pandas",
    "section": "인덱스 레이블",
    "text": "인덱스 레이블\nSeries 객체에 있는 각 원소는 인덱스 레이블(index label)이라 불리는 고유한 식별자를 가지고 있습니다. 기본적으로 Series에 있는 원소의 순서입니다(0에서 시작합니다). 하지만 수동으로 인덱스 레이블을 지정할 수도 있습니다:\n\ns2 = pd.Series([68, 83, 112, 68], index=[\"alice\", \"bob\", \"charles\", \"darwin\"])\ns2\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\n그다음 dict처럼 Series를 사용할 수 있습니다:\n\ns2[\"bob\"]\n\n83\n\n\n일반 배열처럼 정수 인덱스를 사용하여 계속 원소에 접근할 수 있습니다:\n\ns2[1]\n\n83\n\n\n레이블이나 정수를 사용해 접근할 때 명확하게 하기 위해 레이블은 loc 속성을 사용하고 정수는 iloc 속성을 사용하는 것이 좋습니다:\n\ns2.loc[\"bob\"]\n\n83\n\n\n\ns2.iloc[1]\n\n83\n\n\nSeries는 인덱스 레이블을 슬라이싱할 수도 있습니다:\n\ns2.iloc[1:3]\n\nbob         83\ncharles    112\ndtype: int64\n\n\n기본 정수 레이블을 사용할 때 예상 외의 결과를 만들 수 있기 때문에 주의해야 합니다:\n\nsurprise = pd.Series([1000, 1001, 1002, 1003])\n# surprise\n\n\nsurprise_slice = surprise[2:]\nsurprise_slice\n\n2    1002\n3    1003\ndtype: int64\n\n\n보세요. 첫 번째 원소의 인덱스 레이블이 2입니다. 따라서 슬라이싱 결과에서 인덱스 레이블 0인 원소는 없습니다:\n\ntry:\n    surprise_slice[0]\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 0\n\n\n하지만 iloc 속성을 사용해 정수 인덱스로 원소에 접근할 수 있습니다. Series 객체를 사용할 때 loc와 iloc를 사용하는 것이 좋은 이유입니다:\n\nsurprise_slice.iloc[0]\n\n1002\n\n\n\nsurprise_slice.loc[2]\n\n1002"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#dict에서-초기화",
    "href": "Datamining/pandas/pandas.html#dict에서-초기화",
    "title": "Pandas",
    "section": "dict에서 초기화",
    "text": "dict에서 초기화\ndict에서 Series 객체를 만들 수 있습니다. 키는 인덱스 레이블로 사용됩니다:\n\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}  ### 값 변경한것을 s3이랑 통합할려면? 어쩌지?\ns3 = pd.Series(weights)\ns3\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\nSeries에 포함할 원소를 제어하고 index를 지정하여 명시적으로 순서를 결정할 수 있습니다:\n\ns4 = pd.Series(weights, index = [\"colin\", \"alice\"])\ns4\n\ncolin    86\nalice    68\ndtype: int64"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#자동-정렬",
    "href": "Datamining/pandas/pandas.html#자동-정렬",
    "title": "Pandas",
    "section": "자동 정렬",
    "text": "자동 정렬\n여러 개의 Series 객체를 다룰 때 pandas는 자동으로 인덱스 레이블에 따라 원소를 정렬합니다.\n\ns2\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\n\ns3\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\n\nprint(s2.keys())\nprint(s3.keys())\n\ns2 + s3\n\nIndex(['alice', 'bob', 'charles', 'darwin'], dtype='object')\nIndex(['alice', 'bob', 'colin', 'darwin'], dtype='object')\n\n\nalice      136.0\nbob        166.0\ncharles      NaN\ncolin        NaN\ndarwin     136.0\ndtype: float64\n\n\n만들어진 Series는 s2와 s3의 인덱스 레이블의 합집합을 담고 있습니다. s2에 \"colin\"이 없고 s3에 \"charles\"가 없기 때문에 이 원소는 NaN 값을 가집니다(Not-a-Number는 누락이란 의미입니다).\n자동 정렬은 구조가 다르고 누락된 값이 있는 여러 데이터를 다룰 때 매우 편리합니다. 하지만 올바른 인덱스 레이블을 지정하는 것을 잊는다면 원치않는 결과를 얻을 수 있습니다:\n\ns5 = pd.Series([1000,1000,1000,1000])\nprint(\"s2 =\", s2.values)\nprint(\"s5 =\", s5.values)\n\ns2 + s5\n\ns2 = [ 68  83 112  68]\ns5 = [1000 1000 1000 1000]\n\n\nalice     NaN\nbob       NaN\ncharles   NaN\ndarwin    NaN\n0         NaN\n1         NaN\n2         NaN\n3         NaN\ndtype: float64\n\n\n레이블이 하나도 맞지 않기 때문에 판다스가 이 Series를 정렬할 수 없습니다. 따라서 모두 NaN이 되었습니다."
  },
  {
    "objectID": "Datamining/pandas/pandas.html#스칼라로-초기화",
    "href": "Datamining/pandas/pandas.html#스칼라로-초기화",
    "title": "Pandas",
    "section": "스칼라로 초기화",
    "text": "스칼라로 초기화\n스칼라와 인덱스 레이블의 리스트로 Series 객체를 초기화할 수도 있습니다: 모든 원소가 이 스칼라 값으로 설정됩니다.\n\nmeaning = pd.Series(42, [\"life\", \"universe\", \"everything\"])\nmeaning\n\nlife          42\nuniverse      42\neverything    42\ndtype: int64"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#series-이름",
    "href": "Datamining/pandas/pandas.html#series-이름",
    "title": "Pandas",
    "section": "Series 이름",
    "text": "Series 이름\nSeries는 name을 가질 수 있습니다:\n\ns6 = pd.Series([83, 68], index=[\"bob\", \"alice\"], name=\"weights\")\ns6\n\nbob      83\nalice    68\nName: weights, dtype: int64\n\n\n\ns6.name\n\n'weights'"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#series-그래프-출력",
    "href": "Datamining/pandas/pandas.html#series-그래프-출력",
    "title": "Pandas",
    "section": "Series 그래프 출력",
    "text": "Series 그래프 출력\n맷플롯립을 사용해 Series 데이터를 쉽게 그래프로 출력할 수 있습니다(맷플롯립에 대한 자세한 설명은 맷플롯립 튜토리얼을 참고하세요). 맷플롯립을 임포트하고 plot() 메서드를 호출하면 끝입니다:\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\ntemperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5]\ns7 = pd.Series(temperatures, name=\"Temperature\")\n\ns7.plot()\nplt.show()\n\n\n\n\n데이터를 그래프로 출력하는데 많은 옵션이 있습니다. 여기에서 모두 나열할 필요는 없습니다. 특정 종류의 그래프(히스토그램, 파이 차트 등)가 필요하면 판다스 문서의 시각화 섹션에서 예제 코드를 참고하세요."
  },
  {
    "objectID": "Datamining/pandas/pandas.html#시간-범위",
    "href": "Datamining/pandas/pandas.html#시간-범위",
    "title": "Pandas",
    "section": "시간 범위",
    "text": "시간 범위\n먼저 pd.date_range()를 사용해 시계열을 만들어 보죠. 이 함수는 2016년 10월 29일 5:30pm에서 시작하여 12시간마다 하나의 datetime을 담고 있는 DatetimeIndex를 반환합니다.\n\ndates = pd.date_range('2016/10/29 5:30pm', periods=12, freq='H') # 'H'가 시간\ndates\n\nDatetimeIndex(['2016-10-29 17:30:00', '2016-10-29 18:30:00',\n               '2016-10-29 19:30:00', '2016-10-29 20:30:00',\n               '2016-10-29 21:30:00', '2016-10-29 22:30:00',\n               '2016-10-29 23:30:00', '2016-10-30 00:30:00',\n               '2016-10-30 01:30:00', '2016-10-30 02:30:00',\n               '2016-10-30 03:30:00', '2016-10-30 04:30:00'],\n              dtype='datetime64[ns]', freq='H')\n\n\n\npd.date_range('2020-10-07', '2020-10-20', freq='3D')\n\nDatetimeIndex(['2020-10-07', '2020-10-10', '2020-10-13', '2020-10-16',\n               '2020-10-19'],\n              dtype='datetime64[ns]', freq='3D')\n\n\n이 DatetimeIndex를 Series의 인덱스로 사용할수 있습니다:\n\ntemp_series = pd.Series(temperatures, dates)\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n이 시리즈를 그래프로 출력해 보죠:\n\ntemp_series.plot(kind=\"bar\")\n\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#리샘플링",
    "href": "Datamining/pandas/pandas.html#리샘플링",
    "title": "Pandas",
    "section": "리샘플링",
    "text": "리샘플링\n판다스는 매우 간단하게 시계열을 리샘플링할 수 있습니다. resample() 메서드를 호출하고 새로운 주기를 지정하면 됩니다:\n\ntemp_series_freq_2H = temp_series.resample(\"2H\")\ntemp_series_freq_2H\n\n<pandas.core.resample.DatetimeIndexResampler object at 0x000001308586E6A0>\n\n\n리샘플링 연산은 사실 지연된 연산입니다. (https://ko.wikipedia.org/wiki/%EB%8A%90%EA%B8%8B%ED%95%9C_%EA%B3%84%EC%82%B0%EB%B2%95) 그래서 Series 객체 대신 DatetimeIndexResampler 객체가 반환됩니다. 실제 리샘플링 연산을 수행하려면 mean() 같은 메서드를 호출할 수 있습니다. 이 메서드는 연속적인 시간 쌍에 대해 평균을 계산합니다:\n\ntemp_series_freq_2H = temp_series_freq_2H.mean() #2시간 간격으로 리샘플 해주어서 평균 간격으로 만들어줌\n\n\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n\n\n결과를 그래프로 출력해 보죠:\n\ntemp_series_freq_2H.plot(kind=\"bar\")\nplt.show()\n\n\n\n\n2시간 간격으로 어떻게 값이 수집되었는지 확인해 보세요. 예를 들어 6-8pm 간격을 보면 6:30pm에서 5.1이고 7:30pm에서 6.1입니다. 리샘플링 후에 5.1과 6.1의 평균인 5.6 하나를 얻었습니다. 평균말고 어떤 집계 함수(aggregation function)도 사용할 수 있습니다. 예를 들어 각 기간에서 최솟값을 찾을 수 있습니다:\n\ntemp_series_freq_2H = temp_series.resample(\"2H\").mean() \ntemp_series_freq_2H    \n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n\n\n또는 동일한 효과를 내는 apply() 메서드를 사용할 수 있습니다:\n\ntemp_series_freq_2H = temp_series.resample(\"2H\").apply(np.min)\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.4\n2016-10-29 18:00:00    5.1\n2016-10-29 20:00:00    6.1\n2016-10-29 22:00:00    5.7\n2016-10-30 00:00:00    4.7\n2016-10-30 02:00:00    3.9\n2016-10-30 04:00:00    3.5\nFreq: 2H, dtype: float64"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#업샘플링과-보간",
    "href": "Datamining/pandas/pandas.html#업샘플링과-보간",
    "title": "Pandas",
    "section": "업샘플링과 보간",
    "text": "업샘플링과 보간\n다운샘플링의 예를 보았습니다. 하지만 업샘플링(즉, 빈도를 높입니다)도 할 수 있습니다. 하지만 데이터에 구멍을 만듭니다:\n\ntemp_series_freq_15min = temp_series.resample(\"15Min\").mean()\ntemp_series_freq_15min.head(n=10) # `head`는 상위 n 개의 값만 출력합니다\n\n2016-10-29 17:30:00    4.4\n2016-10-29 17:45:00    NaN\n2016-10-29 18:00:00    NaN\n2016-10-29 18:15:00    NaN\n2016-10-29 18:30:00    5.1\n2016-10-29 18:45:00    NaN\n2016-10-29 19:00:00    NaN\n2016-10-29 19:15:00    NaN\n2016-10-29 19:30:00    6.1\n2016-10-29 19:45:00    NaN\nFreq: 15T, dtype: float64\n\n\n한가지 방법은 보간으로 사이를 채우는 것입니다. 이렇게 하려면 interpolate() 메서드를 호출합니다. 기본값은 선형 보간이지만 3차 보간(cubic interpolation) 같은 다른 방법을 선택할 수 있습니다: https://bskyvision.com/789\n\ntemp_series_freq_15min = temp_series.resample(\"15Min\").interpolate(method=\"cubic\")\ntemp_series_freq_15min.head(n=10)\n\n2016-10-29 17:30:00    4.400000\n2016-10-29 17:45:00    4.452911\n2016-10-29 18:00:00    4.605113\n2016-10-29 18:15:00    4.829758\n2016-10-29 18:30:00    5.100000\n2016-10-29 18:45:00    5.388992\n2016-10-29 19:00:00    5.669887\n2016-10-29 19:15:00    5.915839\n2016-10-29 19:30:00    6.100000\n2016-10-29 19:45:00    6.203621\nFreq: 15T, dtype: float64\n\n\n\ntemp_series.plot(label=\"Period: 1 hour\")\ntemp_series_freq_15min.plot(label=\"Period: 15 minutes\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#시간대-pass",
    "href": "Datamining/pandas/pandas.html#시간대-pass",
    "title": "Pandas",
    "section": "시간대 pass",
    "text": "시간대 pass\n기본적으로 datetime은 단순합니다. 시간대(timezone)을 고려하지 않죠. 따라서 2016-10-30 02:30는 파리나 뉴욕이나 2016년 10월 30일 2:30pm입니다. tz_localize() 메서드로 시간대를 고려한 datetime을 만들 수 있습니다: https://www.timeanddate.com/time/map/\n\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n\ntemp_series_ny = temp_series.tz_localize(\"America/New_York\")\ntemp_series_ny\n\n2016-10-29 17:30:00-04:00    4.4\n2016-10-29 18:30:00-04:00    5.1\n2016-10-29 19:30:00-04:00    6.1\n2016-10-29 20:30:00-04:00    6.2\n2016-10-29 21:30:00-04:00    6.1\n2016-10-29 22:30:00-04:00    6.1\n2016-10-29 23:30:00-04:00    5.7\n2016-10-30 00:30:00-04:00    5.2\n2016-10-30 01:30:00-04:00    4.7\n2016-10-30 02:30:00-04:00    4.1\n2016-10-30 03:30:00-04:00    3.9\n2016-10-30 04:30:00-04:00    3.5\ndtype: float64\n\n\n모든 datetime에 -04:00이 추가됩니다. 즉 모든 시간은 UTC - 4시간을 의미합니다.\n다음처럼 파리 시간대로 바꿀 수 있습니다:\n\ntemp_series_paris = temp_series_ny.tz_convert(\"Europe/Paris\")\ntemp_series_paris\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64\n\n\nUTC와의 차이가 +02:00에서 +01:00으로 바뀐 것을 알 수 있습니다. 이는 프랑스가 10월 30일 3am에 겨울 시간으로 바꾸기 때문입니다(2am으로 바뀝니다). 따라서 2:30am이 두 번 등장합니다! 시간대가 없는 표현으로 돌아가 보죠(시간대가 없이 지역 시간으로 매시간 로그를 기록하는 경우 이와 비슷할 것입니다):\n\ntemp_series_paris_naive = temp_series_paris.tz_localize(None)\ntemp_series_paris_naive\n\n2016-10-29 23:30:00    4.4\n2016-10-30 00:30:00    5.1\n2016-10-30 01:30:00    6.1\n2016-10-30 02:30:00    6.2\n2016-10-30 02:30:00    6.1\n2016-10-30 03:30:00    6.1\n2016-10-30 04:30:00    5.7\n2016-10-30 05:30:00    5.2\n2016-10-30 06:30:00    4.7\n2016-10-30 07:30:00    4.1\n2016-10-30 08:30:00    3.9\n2016-10-30 09:30:00    3.5\ndtype: float64\n\n\n이렇게 되면 02:30이 정말 애매합니다. 시간대가 없는 datetime을 파리 시간대로 바꿀 때 에러가 발생합니다:\n\ntry:\n    temp_series_paris_naive.tz_localize(\"Europe/Paris\")\nexcept Exception as e:\n    print(type(e))\n    print(e)\n\n<class 'pytz.exceptions.AmbiguousTimeError'>\nCannot infer dst time from 2016-10-30 02:30:00, try using the 'ambiguous' argument\n\n\n다행히 ambiguous 매개변수를 사용하면 판다스가 타임스탬프의 순서를 기반으로 적절한 DST(일광 절약 시간제)를 추측합니다:\nhttps://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=tori-tours&logNo=221221361831\n\ntemp_series_paris_naive.tz_localize(\"Europe/Paris\", ambiguous=\"infer\")\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#기간",
    "href": "Datamining/pandas/pandas.html#기간",
    "title": "Pandas",
    "section": "기간",
    "text": "기간\npd.period_range() 함수는 DatetimeIndex가 아니라 PeriodIndex를 반환합니다. 예를 들어 2016과 2017년의 전체 분기를 가져와 보죠:\n\nquarters = pd.period_range('2016Q1', periods=8, freq='Q')\nquarters\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\nPeriodIndex에 숫자 N을 추가하면 PeriodIndex 빈도의 N 배만큼 이동시킵니다:\n\nquarters + 3\n\nPeriodIndex(['2016Q4', '2017Q1', '2017Q2', '2017Q3', '2017Q4', '2018Q1',\n             '2018Q2', '2018Q3'],\n            dtype='period[Q-DEC]')\n\n\nasfreq() 메서드를 사용하면 PeriodIndex의 빈도를 바꿀 수 있습니다. 모든 기간이 늘어나거나 줄어듭니다. 예를 들어 분기 기간을 모두 월별 기간으로 바꾸어 보죠:\n\nquarters.asfreq(\"M\")  #asfrq이용해서 빈도 변경 가능\n\nPeriodIndex(['2016-03', '2016-06', '2016-09', '2016-12', '2017-03', '2017-06',\n             '2017-09', '2017-12'],\n            dtype='period[M]')\n\n\n\nquarters\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\n기본적으로 asfreq는 각 기간의 끝에 맞춥니다. 기간의 시작에 맞추도록 변경할 수 있습니다:\n\nquarters.asfreq(\"M\", how=\"start\")  #시작하는 날을 바꿀려면 how라는걸 변경하면 가능\n\nPeriodIndex(['2016-01', '2016-04', '2016-07', '2016-10', '2017-01', '2017-04',\n             '2017-07', '2017-10'],\n            dtype='period[M]')\n\n\n간격을 늘릴 수도 있습니다: pandas 공식 메뉴얼 참조: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n\nquarters.asfreq(\"A\")  #a : anure약자\n\nPeriodIndex(['2016', '2016', '2016', '2016', '2017', '2017', '2017', '2017'], dtype='period[A-DEC]')\n\n\n물론 PeriodIndex로 Series를 만들 수 있습니다:\n\nquarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters)\nquarterly_revenue\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\nquarterly_revenue.plot(kind=\"line\")\nplt.show()\n\n\n\n\nto_timestamp를 호출해서 기간을 타임스탬프로 변경할 수 있습니다. 기본적으로 기간의 첫 번째 날을 반환합니다. 하지만 how와 freq를 지정해서 기간의 마지막 시간을 얻을 수 있습니다:\n\nquarterly_revenue\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\nlast_hours = quarterly_revenue.to_timestamp(how=\"end\", freq=\"H\")  \nlast_hours  #end로 해서 마지막 시간이니깐11시59분 59초가나온것\n             #strart로 바꾸면 1월1일이 나옴\n\n2016-03-31 23:59:59.999999999    300\n2016-06-30 23:59:59.999999999    320\n2016-09-30 23:59:59.999999999    290\n2016-12-31 23:59:59.999999999    390\n2017-03-31 23:59:59.999999999    320\n2017-06-30 23:59:59.999999999    360\n2017-09-30 23:59:59.999999999    310\n2017-12-31 23:59:59.999999999    410\ndtype: int64\n\n\nto_peroid를 호출하면 다시 기간으로 돌아갑니다:\n\nlast_hours.to_period()\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n판다스는 여러 가지 시간 관련 함수를 많이 제공합니다. 온라인 문서를 확인해 보세요. 예를 하나 들면 2016년 매월 마지막 업무일의 9시를 얻는 방법은 다음과 같습니다:\n\nmonths_2022 = pd.period_range(\"2022\", periods=12, freq=\"M\")\none_day_after_last_days = months_2022.asfreq(\"D\") + 1\nlast_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay(n=1)  #BDay : 비즈니스 데이 (근무하는 날)\nlast_bdays.to_period(\"H\") + 9  #다시 시간 간격으로 바꿔서 9를 더하는것\n\nPeriodIndex(['2022-01-31 09:00', '2022-02-28 09:00', '2022-03-31 09:00',\n             '2022-04-29 09:00', '2022-05-31 09:00', '2022-06-30 09:00',\n             '2022-07-29 09:00', '2022-08-31 09:00', '2022-09-30 09:00',\n             '2022-10-31 09:00', '2022-11-30 09:00', '2022-12-30 09:00'],\n            dtype='period[H]')\n\n\n\nmonths_2022\n\nPeriodIndex(['2022-01', '2022-02', '2022-03', '2022-04', '2022-05', '2022-06',\n             '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12'],\n            dtype='period[M]')\n\n\n\none_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay(n=1)\n\nDatetimeIndex(['2022-01-31', '2022-02-28', '2022-03-31', '2022-04-29',\n               '2022-05-31', '2022-06-30', '2022-07-29', '2022-08-31',\n               '2022-09-30', '2022-10-31', '2022-11-30', '2022-12-30'],\n              dtype='datetime64[ns]', freq=None)"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#dataframe-만들기",
    "href": "Datamining/pandas/pandas.html#dataframe-만들기",
    "title": "Pandas",
    "section": "DataFrame 만들기",
    "text": "DataFrame 만들기\nSeries 객체의 딕셔너리를 전달하여 데이터프레임을 만들 수 있습니다:\n\npeople_dict = {\n    \"weight\": pd.Series([68, 83, 112], index=[\"alice\", \"bob\", \"charles\"]),\n    \"birthyear\": pd.Series([1984, 1985, 1992], index=[\"bob\", \"alice\", \"charles\"], name=\"year\"),\n    \"children\": pd.Series([0, 3], index=[\"charles\", \"bob\"]),\n    \"hobby\": pd.Series([\"Biking\", \"Dancing\"], index=[\"alice\", \"bob\"]),\n}\npeople = pd.DataFrame(people_dict)\npeople  #datafame에서 행을 index 열을 column 으로 부름\n\n\n\n\n\n  \n    \n      \n      weight\n      birthyear\n      children\n      hobby\n    \n  \n  \n    \n      alice\n      68\n      1985\n      NaN\n      Biking\n    \n    \n      bob\n      83\n      1984\n      3.0\n      Dancing\n    \n    \n      charles\n      112\n      1992\n      0.0\n      NaN\n    \n  \n\n\n\n\n몇가지 알아 두어야 할 것은 다음과 같습니다:\n\nSeries는 인덱스를 기반으로 자동으로 정렬됩니다.\n누란된 값은 NaN으로 표현됩니다.\nSeries 이름은 무시됩니다(\"year\"란 이름은 삭제됩니다).\nDataFrame은 주피터 노트북에서 멋지게 출력됩니다!\n\n예상하는 방식으로 열을 참조할 수 있고 Series 객체가 반환됩니다:\n\npeople[\"birthyear\"]  #첫 번째 방법\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\npeople.birthyear   # 두번째 방법\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n동시에 여러 개의 열을 선택할 수 있습니다:\n\npeople.iloc[[0,1],[0,1]]\n\n\n\n\n\n  \n    \n      \n      weight\n      birthyear\n    \n  \n  \n    \n      alice\n      68\n      1985\n    \n    \n      bob\n      83\n      1984\n    \n  \n\n\n\n\n\npeople.loc[[\"alice\",\"bob\"],[\"weight\",\"birthyear\"]]\n\n\n\n\n\n  \n    \n      \n      weight\n      birthyear\n    \n  \n  \n    \n      alice\n      68\n      1985\n    \n    \n      bob\n      83\n      1984\n    \n  \n\n\n\n\n열 리스트나 행 인덱스 레이블을 DataFrame 생성자에 전달하면 해당 열과 행으로 채워진 데이터프레임이 반환됩니다. 예를 들면:\n\npeople_dict\n\n{'weight': alice       68\n bob         83\n charles    112\n dtype: int64,\n 'birthyear': bob        1984\n alice      1985\n charles    1992\n Name: year, dtype: int64,\n 'children': charles    0\n bob        3\n dtype: int64,\n 'hobby': alice     Biking\n bob      Dancing\n dtype: object}\n\n\n\nd2 = pd.DataFrame(\n        people_dict,\n        columns=[\"birthyear\", \"weight\", \"height\"],\n        index=[\"bob\", \"alice\", \"eugene\"]\n     )\n\n\nd2\n\n\n\n\n\n  \n    \n      \n      birthyear\n      weight\n      height\n    \n  \n  \n    \n      bob\n      1984.0\n      83.0\n      NaN\n    \n    \n      alice\n      1985.0\n      68.0\n      NaN\n    \n    \n      eugene\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nDataFrame을 만드는 또 다른 편리한 방법은 ndarray나 리스트의 리스트로 모든 값을 생성자에게 전달하고 열 이름과 행 인덱스 레이블을 각기 지정하는 것입니다:\n\nvalues = [\n            [1985, np.nan, \"Biking\",   68],\n            [1984, 3,      \"Dancing\",  83],\n            [1992, 0,      np.nan,    112]\n         ]\nd3 = pd.DataFrame(\n        values,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n\n\n\n  \n    \n      \n      birthyear\n      children\n      hobby\n      weight\n    \n  \n  \n    \n      alice\n      1985\n      NaN\n      Biking\n      68\n    \n    \n      bob\n      1984\n      3.0\n      Dancing\n      83\n    \n    \n      charles\n      1992\n      0.0\n      NaN\n      112\n    \n  \n\n\n\n\n누락된 값을 지정하려면 np.nan이나 넘파이 마스크 배열을 사용합니다:\ndtype = object는 문자열 데이터를 의미\n\nmasked_array = np.ma.asarray(values, dtype=object)\nmasked_array\n\nmasked_array(\n  data=[[1985, nan, 'Biking', 68],\n        [1984, 3, 'Dancing', 83],\n        [1992, 0, nan, 112]],\n  mask=False,\n  fill_value='?',\n  dtype=object)\n\n\n\nmasked_array = np.ma.asarray(\"alice\", dtype=object)\nmasked_array\n\nmasked_array(data='alice',\n             mask=False,\n       fill_value='?',\n            dtype=object)\n\n\n\nmasked_array = np.ma.asarray(values, dtype=object)\nmasked_array[(0, 2), (1, 2)] = np.ma.masked\nd3 = pd.DataFrame(\n        masked_array,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n\n\n\n  \n    \n      \n      birthyear\n      children\n      hobby\n      weight\n    \n  \n  \n    \n      alice\n      1985\n      NaN\n      Biking\n      68\n    \n    \n      bob\n      1984\n      3\n      Dancing\n      83\n    \n    \n      charles\n      1992\n      0\n      NaN\n      112\n    \n  \n\n\n\n\nndarray 대신에 DataFrame 객체를 전달할 수도 있습니다:\n\nd3\n\n\n\n\n\n  \n    \n      \n      birthyear\n      children\n      hobby\n      weight\n    \n  \n  \n    \n      alice\n      1985\n      NaN\n      Biking\n      68\n    \n    \n      bob\n      1984\n      3\n      Dancing\n      83\n    \n    \n      charles\n      1992\n      0\n      NaN\n      112\n    \n  \n\n\n\n\n\nd4 = pd.DataFrame(\n         d3,\n         columns=[\"hobby\", \"children\"],\n         index=[\"alice\", \"bob\"]\n     )\nd4\n\n\n\n\n\n  \n    \n      \n      hobby\n      children\n    \n  \n  \n    \n      alice\n      Biking\n      NaN\n    \n    \n      bob\n      Dancing\n      3\n    \n  \n\n\n\n\n딕셔너리의 딕셔너리(또는 리스트의 리스트)로 DataFrame을 만들 수 있습니다:\n\npeople = pd.DataFrame({\n    \"birthyear\": {\"alice\":1985, \"bob\": 1984, \"charles\": 1992},\n    \"hobby\": {\"alice\":\"Biking\", \"bob\": \"Dancing\"},\n    \"weight\": {\"alice\":68, \"bob\": 83, \"charles\": 112},\n    \"children\": {\"bob\": 3, \"charles\": 0}\n})\n\npeople\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#멀티-인덱싱",
    "href": "Datamining/pandas/pandas.html#멀티-인덱싱",
    "title": "Pandas",
    "section": "멀티 인덱싱",
    "text": "멀티 인덱싱\n모든 열이 같은 크기의 튜플이면 멀티 인덱스로 인식합니다. 열 인덱스 레이블에도 같은 방식이 적용됩니다. 예를 들면:\n\nd5 = pd.DataFrame(\n  {\n    (\"public\", \"birthyear\"):\n        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},  #value값은 딕셔너리/ 앞에 있는거 인덱스 따로 뒤에 있는거 인덱스 따로\n    (\"public\", \"hobby\"):\n        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"},\n    (\"private\", \"weight\"):\n        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112},\n    (\"private\", \"children\"):\n        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0}\n  }\n)\nd5\n\n\n\n\n\n  \n    \n      \n      \n      public\n      private\n    \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n이제 \"public\" 열을 모두 담은 DataFrame을 손쉽게 만들 수 있습니다:\n\nd5[\"public\"] #원하는 값 추출 가능\n\n\n\n\n\n  \n    \n      \n      \n      birthyear\n      hobby\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n    \n    \n      bob\n      1984\n      Dancing\n    \n    \n      London\n      charles\n      1992\n      NaN\n    \n  \n\n\n\n\n\nd5[\"public\", \"hobby\"]  # d5[\"public\"][\"hobby\"]와 같습니다. \n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: (public, hobby), dtype: object\n\n\n\nd5[\"public\"]['hobby']\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: hobby, dtype: object"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#레벨-낮추기",
    "href": "Datamining/pandas/pandas.html#레벨-낮추기",
    "title": "Pandas",
    "section": "레벨 낮추기",
    "text": "레벨 낮추기\nd5를 다시 확인해 보죠:\n\nd5\n\n\n\n\n\n  \n    \n      \n      \n      public\n      private\n    \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n열의 레벨(level)이 2개이고 인덱스 레벨이 2개입니다. droplevel()을 사용해 열 레벨을 낮출 수 있습니다(인덱스도 마찬가지입니다):\n\nd5.columns = d5.columns.droplevel(level = 0)  #없앨때 drop 사용\nd5\n\n\n\n\n\n  \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\nd6 = d5.copy()\nd6.index = d6.index.droplevel(level = 0)\nd6\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#전치",
    "href": "Datamining/pandas/pandas.html#전치",
    "title": "Pandas",
    "section": "전치",
    "text": "전치\nT 속성을 사용해 열과 인덱스를 바꿀 수 있습니다:\n\nd5\n\n\n\n\n\n  \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\nd6 = d5.T  #행열 전환\nd6\n\n\n\n\n\n  \n    \n      \n      Paris\n      London\n    \n    \n      \n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      1985\n      1984\n      1992\n    \n    \n      hobby\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      68\n      83\n      112\n    \n    \n      children\n      NaN\n      3.0\n      0.0"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#레벨-스택과-언스택",
    "href": "Datamining/pandas/pandas.html#레벨-스택과-언스택",
    "title": "Pandas",
    "section": "레벨 스택과 언스택",
    "text": "레벨 스택과 언스택\nstack() 메서드는 가장 낮은 열 레벨을 가장 낮은 인덱스 뒤에 추가합니다:\n\nd6\n\n\n\n\n\n  \n    \n      \n      Paris\n      London\n    \n    \n      \n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      1985\n      1984\n      1992\n    \n    \n      hobby\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      68\n      83\n      112\n    \n    \n      children\n      NaN\n      3.0\n      0.0\n    \n  \n\n\n\n\n\nd7 = d6.stack()  #가장 낮은 열 추가\nd7\n\n\n\n\n\n  \n    \n      \n      \n      London\n      Paris\n    \n  \n  \n    \n      birthyear\n      alice\n      NaN\n      1985\n    \n    \n      bob\n      NaN\n      1984\n    \n    \n      charles\n      1992\n      NaN\n    \n    \n      hobby\n      alice\n      NaN\n      Biking\n    \n    \n      bob\n      NaN\n      Dancing\n    \n    \n      weight\n      alice\n      NaN\n      68\n    \n    \n      bob\n      NaN\n      83\n    \n    \n      charles\n      112\n      NaN\n    \n    \n      children\n      bob\n      NaN\n      3.0\n    \n    \n      charles\n      0.0\n      NaN\n    \n  \n\n\n\n\nNaN 값이 생겼습니다. 이전에 없던 조합이 생겼기 때문입니다(예를 들어 London에 bob이 없었습니다).\nunstack()을 호출하면 반대가 됩니다. 여기에서도 많은 NaN 값이 생성됩니다.\n\nd8 = d7.unstack()\nd8\n\n\n\n\n\n  \n    \n      \n      London\n      Paris\n    \n    \n      \n      alice\n      bob\n      charles\n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      NaN\n      NaN\n      1992\n      1985\n      1984\n      NaN\n    \n    \n      children\n      NaN\n      NaN\n      0.0\n      NaN\n      3.0\n      NaN\n    \n    \n      hobby\n      NaN\n      NaN\n      NaN\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      NaN\n      NaN\n      112\n      68\n      83\n      NaN\n    \n  \n\n\n\n\nunstack을 다시 호출하면 Series 객체가 만들어 집니다:\n\nd9 = d8.unstack()\nd9\n\nLondon  alice    birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        bob      birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        charles  birthyear       1992\n                 children         0.0\n                 hobby            NaN\n                 weight           112\nParis   alice    birthyear       1985\n                 children         NaN\n                 hobby         Biking\n                 weight            68\n        bob      birthyear       1984\n                 children         3.0\n                 hobby        Dancing\n                 weight            83\n        charles  birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\ndtype: object\n\n\nstack()과 unstack() 메서드를 사용할 때 스택/언스택할 level을 선택할 수 있습니다. 심지어 한 번에 여러 개의 레벨을 스택/언스택할 수도 있습니다:\n\nd10 = d9.unstack(level = (0,1))\nd10\n\n\n\n\n\n  \n    \n      \n      London\n      Paris\n    \n    \n      \n      alice\n      bob\n      charles\n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      NaN\n      NaN\n      1992\n      1985\n      1984\n      NaN\n    \n    \n      children\n      NaN\n      NaN\n      0.0\n      NaN\n      3.0\n      NaN\n    \n    \n      hobby\n      NaN\n      NaN\n      NaN\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      NaN\n      NaN\n      112\n      68\n      83\n      NaN"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "href": "Datamining/pandas/pandas.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "title": "Pandas",
    "section": "대부분의 메서드는 수정된 복사본을 반환합니다",
    "text": "대부분의 메서드는 수정된 복사본을 반환합니다\n눈치챘겠지만 stack()과 unstack() 메서드는 객체를 수정하지 않습니다. 대신 복사본을 만들어 반환합니다. 판다스에 있는 대부분의 메서드들이 이렇게 동작합니다.\nStack & Unstack + Pivot에 대한 설명 참고 https://pandas.pydata.org/docs/user_guide/reshaping.html\nData Reshaping!\n\nPivot\n\nimport pandas._testing as tm\n\ndef unpivot(frame):\n    N, K = frame.shape\n    data = {\n        \"value\": frame.to_numpy().ravel(\"F\"),\n        \"variable\": np.asarray(frame.columns).repeat(N),\n        \"date\": np.tile(np.asarray(frame.index), K),\n    }\n    return pd.DataFrame(data, columns=[\"date\", \"variable\", \"value\"])\n\ndf = unpivot(tm.makeTimeDataFrame(3))\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      0.052103\n    \n    \n      1\n      2000-01-04\n      A\n      1.657830\n    \n    \n      2\n      2000-01-05\n      A\n      -0.868082\n    \n    \n      3\n      2000-01-03\n      B\n      -0.110016\n    \n    \n      4\n      2000-01-04\n      B\n      0.720658\n    \n    \n      5\n      2000-01-05\n      B\n      -0.244802\n    \n    \n      6\n      2000-01-03\n      C\n      1.223895\n    \n    \n      7\n      2000-01-04\n      C\n      -2.032696\n    \n    \n      8\n      2000-01-05\n      C\n      1.846622\n    \n    \n      9\n      2000-01-03\n      D\n      0.229538\n    \n    \n      10\n      2000-01-04\n      D\n      -0.263178\n    \n    \n      11\n      2000-01-05\n      D\n      1.229025\n    \n  \n\n\n\n\nTo select out everything for variable A we could do:\n\nfiltered = df[df[\"variable\"] == \"A\"]\nfiltered\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      0.052103\n    \n    \n      1\n      2000-01-04\n      A\n      1.657830\n    \n    \n      2\n      2000-01-05\n      A\n      -0.868082\n    \n  \n\n\n\n\nBut suppose we wish to do time series operations with the variables. A better representation would be where the columns are the unique variables and an index of dates identifies individual observations. To reshape the data into this form, we use the DataFrame.pivot() method (also implemented as a top level function pivot()):\n\npivoted = df.pivot(index=\"date\", columns=\"variable\", values=\"value\")\n\npivoted\n\n\n\n\n\n  \n    \n      variable\n      A\n      B\n      C\n      D\n    \n    \n      date\n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-03\n      0.052103\n      -0.110016\n      1.223895\n      0.229538\n    \n    \n      2000-01-04\n      1.657830\n      0.720658\n      -2.032696\n      -0.263178\n    \n    \n      2000-01-05\n      -0.868082\n      -0.244802\n      1.846622\n      1.229025\n    \n  \n\n\n\n\n\npivoted.columns\n\nIndex(['A', 'B', 'C', 'D'], dtype='object', name='variable')\n\n\n\npivoted.index\n\nDatetimeIndex(['2000-01-03', '2000-01-04', '2000-01-05'], dtype='datetime64[ns]', name='date', freq=None)\n\n\nIf the values argument is omitted, and the input DataFrame has more than one column of values which are not used as column or index inputs to pivot(), then the resulting “pivoted” DataFrame will have hierarchical columns whose topmost level indicates the respective value column:\n\ndf[\"value2\"] = df[\"value\"] * 2\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n      value2\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      0.052103\n      0.104205\n    \n    \n      1\n      2000-01-04\n      A\n      1.657830\n      3.315660\n    \n    \n      2\n      2000-01-05\n      A\n      -0.868082\n      -1.736164\n    \n    \n      3\n      2000-01-03\n      B\n      -0.110016\n      -0.220031\n    \n    \n      4\n      2000-01-04\n      B\n      0.720658\n      1.441316\n    \n    \n      5\n      2000-01-05\n      B\n      -0.244802\n      -0.489603\n    \n    \n      6\n      2000-01-03\n      C\n      1.223895\n      2.447789\n    \n    \n      7\n      2000-01-04\n      C\n      -2.032696\n      -4.065392\n    \n    \n      8\n      2000-01-05\n      C\n      1.846622\n      3.693245\n    \n    \n      9\n      2000-01-03\n      D\n      0.229538\n      0.459076\n    \n    \n      10\n      2000-01-04\n      D\n      -0.263178\n      -0.526357\n    \n    \n      11\n      2000-01-05\n      D\n      1.229025\n      2.458049\n    \n  \n\n\n\n\n\npivoted = df.pivot(index=\"date\", columns=\"variable\")\n\npivoted\n\n\n\n\n\n  \n    \n      \n      value\n      value2\n    \n    \n      variable\n      A\n      B\n      C\n      D\n      A\n      B\n      C\n      D\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-03\n      0.052103\n      -0.110016\n      1.223895\n      0.229538\n      0.104205\n      -0.220031\n      2.447789\n      0.459076\n    \n    \n      2000-01-04\n      1.657830\n      0.720658\n      -2.032696\n      -0.263178\n      3.315660\n      1.441316\n      -4.065392\n      -0.526357\n    \n    \n      2000-01-05\n      -0.868082\n      -0.244802\n      1.846622\n      1.229025\n      -1.736164\n      -0.489603\n      3.693245\n      2.458049\n    \n  \n\n\n\n\n\npivoted['value']\n\n\n\n\n\n  \n    \n      variable\n      A\n      B\n      C\n      D\n    \n    \n      date\n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-03\n      0.052103\n      -0.110016\n      1.223895\n      0.229538\n    \n    \n      2000-01-04\n      1.657830\n      0.720658\n      -2.032696\n      -0.263178\n    \n    \n      2000-01-05\n      -0.868082\n      -0.244802\n      1.846622\n      1.229025"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#행-참조하기",
    "href": "Datamining/pandas/pandas.html#행-참조하기",
    "title": "Pandas",
    "section": "행 참조하기",
    "text": "행 참조하기\npeople DataFrame으로 돌아가 보죠:\n\npeople\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\nloc 속성으로 열 대신 행을 참조할 수 있습니다. DataFrame의 열 이름이 행 인덱스 레이블로 매핑된 Series 객체가 반환됩니다:\n\npeople=['birthyear']\n\n\npeople.loc[\"charles\"]\n\nAttributeError: 'list' object has no attribute 'loc'\n\n\niloc 속성을 사용해 정수 인덱스로 행을 참조할 수 있습니다:\n\npeople.iloc[2]\n\nAttributeError: 'list' object has no attribute 'iloc'\n\n\n행을 슬라이싱할 수 있으며 DataFrame 객체가 반환됩니다:\n\npeople\n\n['birthyear']\n\n\n\npeople.iloc[1:3]\n\n\n\n\n\n  \n    \n      \n      weight\n      height\n      hobby\n      age\n      over 30\n      pets\n      body_mass_index\n    \n  \n  \n    \n      bob\n      83\n      181\n      Dancing\n      38\n      True\n      0.0\n      25.335002\n    \n    \n      charles\n      112\n      185\n      NaN\n      30\n      False\n      5.0\n      32.724617\n    \n  \n\n\n\n\n마자믹으로 불리언 배열을 전달하여 해당하는 행을 가져올 수 있습니다:\n\npeople[np.array([True, False, True])]\n\n\n\n\n\n  \n    \n      \n      weight\n      height\n      hobby\n      age\n      over 30\n      pets\n      body_mass_index\n    \n  \n  \n    \n      alice\n      68\n      172\n      Biking\n      37\n      True\n      NaN\n      22.985398\n    \n    \n      charles\n      112\n      185\n      NaN\n      30\n      False\n      5.0\n      32.724617\n    \n  \n\n\n\n\n불리언 표현식을 사용할 때 아주 유용합니다:\n\npeople[\"birthyear\"] < 1990\n\nKeyError: 'birthyear'\n\n\n\npeople[people[\"birthyear\"] < 1990]\n\nKeyError: 'birthyear'"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#열-추가-삭제",
    "href": "Datamining/pandas/pandas.html#열-추가-삭제",
    "title": "Pandas",
    "section": "열 추가, 삭제",
    "text": "열 추가, 삭제\nDataFrame을 Series의 딕셔너리처럼 다룰 수 있습니다. 따라서 다음 같이 쓸 수 있습니다:\n\npeople\n\n\n\n\n\n  \n    \n      \n      weight\n      height\n      hobby\n      age\n      over 30\n      pets\n      body_mass_index\n    \n  \n  \n    \n      alice\n      68\n      172\n      Biking\n      37\n      True\n      NaN\n      22.985398\n    \n    \n      bob\n      83\n      181\n      Dancing\n      38\n      True\n      0.0\n      25.335002\n    \n    \n      charles\n      112\n      185\n      NaN\n      30\n      False\n      5.0\n      32.724617\n    \n  \n\n\n\n\n\npeople[\"age\"] = 2022 - people[\"birthyear\"]  # \"age\" 열을 추가합니다\npeople[\"over 30\"] = people[\"age\"] > 30      # \"over 30\" 열을 추가합니다\n\npeople\n\nKeyError: 'birthyear'\n\n\n\nbirthyears = people.pop(\"birthyear\")\ndel people[\"children\"]\n\nKeyError: 'birthyear'\n\n\n\nbirthyears\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\npeople\n\n\n\n\n\n  \n    \n      \n      weight\n      hobby\n      age\n      over 30\n    \n  \n  \n    \n      alice\n      68\n      Biking\n      37\n      True\n    \n    \n      bob\n      83\n      Dancing\n      38\n      True\n    \n    \n      charles\n      112\n      NaN\n      30\n      False\n    \n  \n\n\n\n\n\n# 딕셔너리도 유사함\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\n\n\nweights.pop(\"alice\")\n\n68\n\n\n\nweights\n\n{'bob': 83, 'colin': 86, 'darwin': 68}\n\n\n\ndel weights[\"bob\"]\n\n\nweights\n\n{'colin': 86, 'darwin': 68}\n\n\n새로운 열을 추가할 때 행의 개수는 같아야 합니다. 누락된 행은 NaN으로 채워지고 추가적인 행은 무시됩니다:\n\npeople.index\n\nIndex(['alice', 'bob', 'charles'], dtype='object')\n\n\n\npeople[\"pets\"] = pd.Series({\"bob\": 0, \"charles\": 5, \"eugene\":1})  # alice 누락됨, eugene은 무시됨\npeople\n\n\n\n\n\n  \n    \n      \n      weight\n      hobby\n      age\n      over 30\n      pets\n    \n  \n  \n    \n      alice\n      68\n      Biking\n      37\n      True\n      NaN\n    \n    \n      bob\n      83\n      Dancing\n      38\n      True\n      0.0\n    \n    \n      charles\n      112\n      NaN\n      30\n      False\n      5.0\n    \n  \n\n\n\n\n새로운 열을 추가할 때 기본적으로 (오른쪽) 끝에 추가됩니다. insert() 메서드를 사용해 다른 곳에 열을 추가할 수 있습니다:\n\npeople.insert(1, \"height\", [172, 181, 185])\npeople\n\n\n\n\n\n  \n    \n      \n      weight\n      height\n      hobby\n      age\n      over 30\n      pets\n    \n  \n  \n    \n      alice\n      68\n      172\n      Biking\n      37\n      True\n      NaN\n    \n    \n      bob\n      83\n      181\n      Dancing\n      38\n      True\n      0.0\n    \n    \n      charles\n      112\n      185\n      NaN\n      30\n      False\n      5.0"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#새로운-열-할당하기",
    "href": "Datamining/pandas/pandas.html#새로운-열-할당하기",
    "title": "Pandas",
    "section": "새로운 열 할당하기",
    "text": "새로운 열 할당하기\nassign() 메서드를 호출하여 새로운 열을 만들 수도 있습니다. 이는 새로운 DataFrame 객체를 반환하며 원본 객체는 변경되지 않습니다:\n\npeople.assign(\n    body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n    has_pets = people[\"pets\"] > 0\n)\n\n\n\n\n\n  \n    \n      \n      weight\n      height\n      hobby\n      age\n      over 30\n      pets\n      body_mass_index\n      has_pets\n    \n  \n  \n    \n      alice\n      68\n      172\n      Biking\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      83\n      181\n      Dancing\n      38\n      True\n      0.0\n      25.335002\n      False\n    \n    \n      charles\n      112\n      185\n      NaN\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n\npeople[\"body_mass_index\"] = people[\"weight\"] / (people[\"height\"] / 100) ** 2\n\npeople\n\n\n\n\n\n  \n    \n      \n      weight\n      height\n      hobby\n      age\n      over 30\n      pets\n      body_mass_index\n    \n  \n  \n    \n      alice\n      68\n      172\n      Biking\n      37\n      True\n      NaN\n      22.985398\n    \n    \n      bob\n      83\n      181\n      Dancing\n      38\n      True\n      0.0\n      25.335002\n    \n    \n      charles\n      112\n      185\n      NaN\n      30\n      False\n      5.0\n      32.724617\n    \n  \n\n\n\n\n\ndel people[\"body_mass_index\"]\n\n\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n    \n  \n\n\n\n\n할당문 안에서 만든 열은 접근할 수 없습니다:\n\ntry:\n    people.assign(\n        body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n        overweight = people[\"body_mass_index\"] > 25\n    )\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 'body_mass_index'\n\n\n해결책은 두 개의 연속된 할당문으로 나누는 것입니다:\n\nd6 = people.assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\nd6.assign(overweight = d6[\"body_mass_index\"] > 25)\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n임시 변수 d6를 만들면 불편합니다. assign() 메서드를 연결하고 싶겠지만 people 객체가 첫 번째 할당문에서 실제로 수정되지 않기 때문에 작동하지 않습니다:\n\ntry:\n    (people\n         .assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\n         .assign(overweight = people[\"body_mass_index\"] > 25)\n    )\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 'body_mass_index'\n\n\n하지만 걱정하지 마세요. 간단한 방법이 있습니다. assign() 메서드에 함수(전형적으로 lambda 함수)를 전달하면 DataFrame을 매개변수로 이 함수를 호출할 것입니다:\n\n(people\n     .assign(body_mass_index = lambda df: df[\"weight\"] / (df[\"height\"] / 100) ** 2)\n     .assign(overweight = lambda df: df[\"body_mass_index\"] > 25)\n)\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n문제가 해결되었군요!\n\npeople[\"body_mass_index\"] = people[\"weight\"] / (people[\"height\"] / 100) ** 2\npeople[\"overweight\"] = people[\"body_mass_index\"]>25\n\n\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#표현식-평가",
    "href": "Datamining/pandas/pandas.html#표현식-평가",
    "title": "Pandas",
    "section": "표현식 평가",
    "text": "표현식 평가\n판다스가 제공하는 뛰어난 기능 하나는 표현식 평가입니다. 이는 numexpr 라이브러리에 의존하기 때문에 설치가 되어 있어야 합니다.\n\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n\n\"weight / (height/100) ** 2 > 25\"\n\n'weight / (height/100) ** 2 > 25'\n\n\n\npeople.eval(\"weight / (height/100) ** 2 > 25\")\n\nalice      False\nbob         True\ncharles     True\ndtype: bool\n\n\n할당 표현식도 지원됩니다. inplace=True로 지정하면 수정된 복사본을 만들지 않고 바로 DataFrame을 변경합니다:\n\npeople.eval(\"body_mass_index = weight / (height/100) ** 2\", inplace=True)\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n'@'를 접두어로 사용하여 지역 변수나 전역 변수를 참조할 수 있습니다:\n\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\noverweight_threshold = 30\npeople.eval(\"overweight = body_mass_index > @overweight_threshold\", inplace=True)\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#dataframe-쿼리하기",
    "href": "Datamining/pandas/pandas.html#dataframe-쿼리하기",
    "title": "Pandas",
    "section": "DataFrame 쿼리하기",
    "text": "DataFrame 쿼리하기\nquery() 메서드를 사용하면 쿼리 표현식에 기반하여 DataFrame을 필터링할 수 있습니다:\n\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\npeople.query(\"age > 30 and pets == 0\")\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\npeople[(people[\"age\"]>30) & (people[\"pets\"] == 0)]\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\nmask = (people[\"age\"]>30) & (people[\"pets\"] == 0)\n\n\npeople[mask]\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#dataframe-정렬",
    "href": "Datamining/pandas/pandas.html#dataframe-정렬",
    "title": "Pandas",
    "section": "DataFrame 정렬",
    "text": "DataFrame 정렬\nsort_index 메서드를 호출하여 DataFrame을 정렬할 수 있습니다. 기본적으로 인덱스 레이블을 기준으로 오름차순으로 행을 정렬합니다. 여기에서는 내림차순으로 정렬해 보죠:\n\npeople.sort_index(ascending=False)\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n  \n\n\n\n\nsort_index는 DataFrame의 정렬된 복사본을 반환합니다. people을 직접 수정하려면 inplace 매개변수를 True로 지정합니다. 또한 axis=1로 지정하여 열 대신 행을 정렬할 수 있습니다:\n\npeople.sort_index(axis=1, inplace=True)\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n레이블이 아니라 값을 기준으로 DataFrame을 정렬하려면 sort_values에 정렬하려는 열을 지정합니다:\n\npeople.sort_values(by=\"age\", inplace=True)\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#dataframe-그래프-그리기",
    "href": "Datamining/pandas/pandas.html#dataframe-그래프-그리기",
    "title": "Pandas",
    "section": "DataFrame 그래프 그리기",
    "text": "DataFrame 그래프 그리기\nSeries와 마찬가지로 판다스는 DataFrame 기반으로 멋진 그래프를 손쉽게 그릴 수 있습니다.\n예를 들어 plot 메서드를 호출하여 DataFrame의 데이터에서 선 그래프를 쉽게 그릴 수 있습니다:\n\npeople.plot(kind = \"line\", x = \"body_mass_index\", y = [\"height\", \"weight\"])\nplt.show()\n\n\n\n\n맷플롯립의 함수가 지원하는 다른 매개변수를 사용할 수 있습니다. 예를 들어, 산점도를 그릴 때 맷플롯립의 scatter() 함수의 s 매개변수를 사용해 크기를 지정할 수 있습니다:\n\npeople.plot(kind = \"scatter\", x = \"height\", y = \"weight\", s=[40, 120, 200])\nplt.show()\n\n\n\n\n선택할 수 있는 옵션이 많습니다. 판다스 문서의 시각화 페이지에서 마음에 드는 그래프를 찾아 예제 코드를 살펴 보세요.\n\nHistogram\n\n\ndf4 = pd.DataFrame(\n    {\n        \"a\": np.random.randn(1000) + 1,\n        \"b\": np.random.randn(1000),\n        \"c\": np.random.randn(1000) - 1,\n    },\n    columns=[\"a\", \"b\", \"c\"],\n)\n\nplt.figure();\n\ndf4.plot.hist(alpha=0.5);\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n\ndf4\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1.024702\n      0.205680\n      -1.151098\n    \n    \n      1\n      0.932532\n      -0.406178\n      -0.108419\n    \n    \n      2\n      -0.616295\n      -1.700059\n      -1.133808\n    \n    \n      3\n      1.481105\n      -0.520759\n      0.685474\n    \n    \n      4\n      0.736898\n      -0.533195\n      -1.705509\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      -0.220861\n      -0.182931\n      -1.275840\n    \n    \n      996\n      1.069163\n      -1.368469\n      -0.772002\n    \n    \n      997\n      1.378971\n      -0.554198\n      -2.278343\n    \n    \n      998\n      0.661284\n      -0.835081\n      -0.700224\n    \n    \n      999\n      0.664459\n      -0.083600\n      0.502208\n    \n  \n\n1000 rows × 3 columns\n\n\n\n\ndf4.plot(kind=\"hist\",alpha=0.5, x=\"a\")\nplt.show()\n\n\n\n\n\ndf4['a'].plot.hist()\nplt.show()\n\n\n\n\n\nBoxplot\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n    \n  \n  \n    \n      0\n      0.887491\n      0.028126\n      0.003576\n      0.013055\n      0.509898\n    \n    \n      1\n      0.653231\n      0.003621\n      0.969910\n      0.173393\n      0.409486\n    \n    \n      2\n      0.803033\n      0.382834\n      0.195527\n      0.616920\n      0.581911\n    \n    \n      3\n      0.524122\n      0.926863\n      0.170608\n      0.300242\n      0.930059\n    \n    \n      4\n      0.968483\n      0.187320\n      0.839602\n      0.149723\n      0.650208\n    \n    \n      5\n      0.110352\n      0.393050\n      0.719806\n      0.859684\n      0.501955\n    \n    \n      6\n      0.866960\n      0.221862\n      0.892753\n      0.990645\n      0.736521\n    \n    \n      7\n      0.801126\n      0.614989\n      0.057752\n      0.183695\n      0.569820\n    \n    \n      8\n      0.013725\n      0.439573\n      0.021304\n      0.192832\n      0.270145\n    \n    \n      9\n      0.696394\n      0.974029\n      0.351002\n      0.409430\n      0.581877\n    \n  \n\n\n\n\n\ndf = pd.DataFrame(np.random.rand(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\ndf.plot.box();\n\n\n\n\n\ndf = pd.DataFrame(np.random.rand(10, 2), columns=[\"Col1\", \"Col2\"])\n\ndf[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\ndf\n\n\n\n\n\n  \n    \n      \n      Col1\n      Col2\n      X\n    \n  \n  \n    \n      0\n      0.078610\n      0.759337\n      A\n    \n    \n      1\n      0.443074\n      0.020230\n      A\n    \n    \n      2\n      0.858927\n      0.881363\n      A\n    \n    \n      3\n      0.912618\n      0.354576\n      A\n    \n    \n      4\n      0.784800\n      0.178885\n      A\n    \n    \n      5\n      0.552493\n      0.020503\n      B\n    \n    \n      6\n      0.895306\n      0.838616\n      B\n    \n    \n      7\n      0.901667\n      0.012848\n      B\n    \n    \n      8\n      0.814965\n      0.330909\n      B\n    \n    \n      9\n      0.211971\n      0.653164\n      B\n    \n  \n\n\n\n\n\nplt.figure();\n\nbp = df.boxplot(by=\"X\")\n\n<Figure size 432x288 with 0 Axes>"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#dataframe-연산",
    "href": "Datamining/pandas/pandas.html#dataframe-연산",
    "title": "Pandas",
    "section": "DataFrame 연산",
    "text": "DataFrame 연산\nDataFrame이 넘파이 배열을 흉내내려는 것은 아니지만 몇 가지 비슷한 점이 있습니다. 예제 DataFrame을 만들어 보죠:\n\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\nDataFrame에 넘파이 수학 함수를 적용하면 모든 값에 이 함수가 적용됩니다:\n\nnp.sqrt(grades)\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      2.828427\n      2.828427\n      3.000000\n    \n    \n      bob\n      3.162278\n      3.000000\n      3.000000\n    \n    \n      charles\n      2.000000\n      2.828427\n      1.414214\n    \n    \n      darwin\n      3.000000\n      3.162278\n      3.162278\n    \n  \n\n\n\n\n비슷하게 DataFrame에 하나의 값을 더하면 DataFrame의 모든 원소에 이 값이 더해집니다. 이를 브로드캐스팅이라고 합니다:\n\ngrades + 1\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      9\n      9\n      10\n    \n    \n      bob\n      11\n      10\n      10\n    \n    \n      charles\n      5\n      9\n      3\n    \n    \n      darwin\n      10\n      11\n      11\n    \n  \n\n\n\n\n물론 산술 연산(*,/,**…)과 조건 연산(>, ==…)을 포함해 모든 이항 연산에도 마찬가지 입니다:\n\ngrades >= 5\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      True\n      True\n      True\n    \n    \n      bob\n      True\n      True\n      True\n    \n    \n      charles\n      False\n      True\n      False\n    \n    \n      darwin\n      True\n      True\n      True\n    \n  \n\n\n\n\nDataFrame의 max, sum, mean 같은 집계 연산은 각 열에 적용되어 Series 객체가 반환됩니다:\n\ngrades.mean()\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\nall 메서드도 집계 연산입니다: 모든 값이 True인지 아닌지 확인합니다. 모든 학생의 점수가 5 이상인 월을 찾아 보죠:\n\n(grades > 5).all()\n\nsep    False\noct     True\nnov    False\ndtype: bool\n\n\nMost of these functions take an optional axis parameter which lets you specify along which axis of the DataFrame you want the operation executed. The default is axis=0, meaning that the operation is executed vertically (on each column). You can set axis=1 to execute the operation horizontally (on each row). For example, let’s find out which students had all grades greater than 5:\n\ngrades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\n\n(grades > 5).all(axis = 1)\n\nalice       True\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nany 메서드는 하나라도 참이면 True를 반환합니다. 한 번이라도 10점을 받은 사람을 찾아 보죠:\n\n(grades == 10).any(axis = 1)\n\nalice      False\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nDataFrame에 Series 객체를 더하면 (또는 다른 이항 연산을 수행하면) 판다스는 DataFrame에 있는 모든 행에 이 연산을 브로드캐스팅합니다. 이는 Series 객체가 DataFrame의 행의 개수와 크기가 같을 때만 동작합니다. 예를 들어 DataFrame의 mean(Series 객체)을 빼보죠:\n\ngrades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\n\ngrades.mean()\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\n\ngrades - grades.mean()  # grades - [7.75, 8.75, 7.50] 와 동일\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      0.25\n      -0.75\n      1.5\n    \n    \n      bob\n      2.25\n      0.25\n      1.5\n    \n    \n      charles\n      -3.75\n      -0.75\n      -5.5\n    \n    \n      darwin\n      1.25\n      1.25\n      2.5\n    \n  \n\n\n\n\n모든 9월 성적에서 7.75를 빼고, 10월 성적에서 8.75를 빼고, 11월 성적에서 7.50을 뺍니다. 이는 다음 DataFrame을 빼는 것과 같습니다:\n\npd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns)\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      7.75\n      8.75\n      7.5\n    \n    \n      bob\n      7.75\n      8.75\n      7.5\n    \n    \n      charles\n      7.75\n      8.75\n      7.5\n    \n    \n      darwin\n      7.75\n      8.75\n      7.5\n    \n  \n\n\n\n\n모든 성적의 전체 평균을 빼고 싶다면 다음과 같은 방법을 사용합니다:\n\ngrades.values.mean()\n\nNameError: name 'grades' is not defined\n\n\n\ngrades - grades.values.mean() # 모든 점수에서 전체 평균(8.00)을 뺍니다\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      0.0\n      0.0\n      1.0\n    \n    \n      bob\n      2.0\n      1.0\n      1.0\n    \n    \n      charles\n      -4.0\n      0.0\n      -6.0\n    \n    \n      darwin\n      1.0\n      2.0\n      2.0"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#자동-정렬-1",
    "href": "Datamining/pandas/pandas.html#자동-정렬-1",
    "title": "Pandas",
    "section": "자동 정렬",
    "text": "자동 정렬\nSeries와 비슷하게 여러 개의 DataFrame에 대한 연산을 수행하면 판다스는 자동으로 행 인덱스 레이블로 정렬하지만 열 이름으로도 정렬할 수 있습니다. 10월부터 12월까지 보너스 포인트를 담은 DataFrame을 만들어 보겠습니다:\n\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\n\nbonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]])\nbonus_points = pd.DataFrame(bonus_array, columns=[\"oct\", \"nov\", \"dec\"], index=[\"bob\",\"colin\", \"darwin\", \"charles\"])\nbonus_points\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      NaN\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\n\ngrades + bonus_points\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      bob\n      NaN\n      NaN\n      9.0\n      NaN\n    \n    \n      charles\n      NaN\n      5.0\n      11.0\n      NaN\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      NaN\n      11.0\n      10.0\n      NaN\n    \n  \n\n\n\n\n덧셈 연산이 수행되었지만 너무 많은 원소가 NaN이 되었습니다. DataFrame을 정렬할 때 일부 열과 행이 한 쪽에만 있기 때문입니다. 다른 쪽에는 누란되었다고 간주합니다(NaN). NaN에 어떤 수를 더하면 NaN이 됩니다."
  },
  {
    "objectID": "Datamining/pandas/pandas.html#누락된-데이터-다루기",
    "href": "Datamining/pandas/pandas.html#누락된-데이터-다루기",
    "title": "Pandas",
    "section": "누락된 데이터 다루기",
    "text": "누락된 데이터 다루기\n실제 데이터에서 누락된 데이터를 다루는 경우는 자주 발생합니다. 판다스는 누락된 데이터를 다룰 수 있는 몇 가지 방법을 제공합니다.\n위 데이터에 있는 문제를 해결해 보죠. 예를 들어, 누락된 데이터는 NaN이 아니라 0이 되어야 한다고 결정할 수 있습니다. fillna() 메서드를 사용해 모든 NaN 값을 어떤 값으로 바꿀 수 있습니다:\n\n(grades + bonus_points).fillna(0)\n\nNameError: name 'grades' is not defined\n\n\n9월의 점수를 0으로 만드는 것은 공정하지 않습니다. 누락된 점수는 그대로 두고, 누락된 보너스 포인트는 0으로 바꿀 수 있습니다:\n\nbonus_points\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      NaN\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\n\nfixed_bonus_points = bonus_points.fillna(0) # NA 값 0으로 바꾸기\nfixed_bonus_points.insert(loc=0, column=\"sep\", value=0) # 누락된 컬럼 만들기\nfixed_bonus_points.loc[\"alice\"] = 0 # 누락된 행 만들기\nfixed_bonus_points\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0\n      0.0\n      0.0\n      2.0\n    \n    \n      colin\n      0\n      0.0\n      1.0\n      0.0\n    \n    \n      darwin\n      0\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      0\n      3.0\n      3.0\n      0.0\n    \n    \n      alice\n      0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n\ngrades + fixed_bonus_points\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      NaN\n      9.0\n      8.0\n      8.0\n    \n    \n      bob\n      NaN\n      9.0\n      9.0\n      10.0\n    \n    \n      charles\n      NaN\n      5.0\n      11.0\n      4.0\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      NaN\n      11.0\n      10.0\n      9.0\n    \n  \n\n\n\n\n훨씬 낫네요: 일부 데이터를 꾸며냈지만 덜 불공정합니다.\n누락된 값을 다루는 또 다른 방법은 보간입니다. bonus_points DataFrame을 다시 보죠:\n\nbonus_points\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      NaN\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\ninterpolate 메서드를 사용해 보죠. 기본적으로 수직 방향(axis=0)으로 보간합니다. 따라서 수평으로(axis=1)으로 보간하도록 지정합니다.\n\nbonus_points.interpolate(axis=1)\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      1.0\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\nbob의 보너스 포인트는 10월에 0이고 12월에 2입니다. 11월을 보간하면 평균 보너스 포인트 1을 얻습니다. colin의 보너스 포인트는 11월에 1이지만 9월에 포인트는 얼마인지 모릅니다. 따라서 보간할 수 없고 10월의 포인트는 그대로 누락된 값으로 남아 있습니다. 이를 해결하려면 보간하기 전에 9월의 보너스 포인트를 0으로 설정해야 합니다.\n\nbetter_bonus_points = bonus_points.copy()\nbetter_bonus_points.insert(0, \"sep\", 0)\nbetter_bonus_points.loc[\"alice\"] = 0\nbetter_bonus_points = better_bonus_points.interpolate(axis=1)\nbetter_bonus_points\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      0.0\n      1.0\n      2.0\n    \n    \n      colin\n      0.0\n      0.5\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      0.0\n      3.0\n      3.0\n      0.0\n    \n    \n      alice\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n좋습니다. 이제 모든 보너스 포인트가 합리적으로 보간되었습니다. 최종 점수를 확인해 보죠:\n\ngrades + better_bonus_points\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      NaN\n      9.0\n      8.0\n      8.0\n    \n    \n      bob\n      NaN\n      10.0\n      9.0\n      10.0\n    \n    \n      charles\n      NaN\n      5.0\n      11.0\n      4.0\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      NaN\n      11.0\n      10.0\n      9.0\n    \n  \n\n\n\n\n9월 열이 오른쪽에 추가되었는데 좀 이상합니다. 이는 더하려는 DataFrame이 정확히 같은 열을 가지고 있지 않기 때문입니다(grade DataFrame에는 \"dec\" 열이 없습니다). 따라서 판다스는 알파벳 순서로 최종 열을 정렬합니다. 이를 해결하려면 덧셈을 하기 전에 누락된 열을 추가하면 됩니다:\n\ngrades[\"dec\"] = np.nan\nfinal_grades = grades + better_bonus_points\nfinal_grades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n      NaN\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n      NaN\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n      NaN\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0\n      NaN\n    \n  \n\n\n\n\n12월과 colin에 대해 할 수 있는 것이 많지 않습니다. 보너스 포인트를 만드는 것이 나쁘지만 점수를 합리적으로 올릴 수는 없습니다(어떤 선생님들은 그럴 수 있지만). dropna() 메서드를 사용해 모두 NaN인 행을 삭제합니다:\n\nfinal_grades_clean = final_grades.dropna(how=\"all\")\nfinal_grades_clean\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n      NaN\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n      NaN\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n      NaN\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0\n      NaN\n    \n  \n\n\n\n\n그다음 axis 매개변수를 1로 지정하여 모두 NaN인 열을 삭제합니다:\n\nfinal_grades_clean = final_grades_clean.dropna(axis=1, how=\"all\")\nfinal_grades_clean\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#groupby로-집계하기",
    "href": "Datamining/pandas/pandas.html#groupby로-집계하기",
    "title": "Pandas",
    "section": "groupby로 집계하기",
    "text": "groupby로 집계하기\nSQL과 비슷하게 판다스는 데이터를 그룹핑하고 각 그룹에 대해 연산을 수행할 수 있습니다.\n먼저 그루핑을 위해 각 사람의 데이터를 추가로 만들겠습니다. NaN 값을 어떻게 다루는지 보기 위해 final_grades DataFrame을 다시 사용하겠습니다:\n\nfinal_grades[\"hobby\"] = [\"Biking\", \"Dancing\", np.nan, \"Dancing\", \"Biking\"]\nfinal_grades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n      hobby\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n      NaN\n      Biking\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n      NaN\n      Dancing\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n      NaN\n      NaN\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n      Dancing\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0\n      NaN\n      Biking\n    \n  \n\n\n\n\nhobby로 이 DataFrame을 그룹핑해 보죠:\n\ngrouped_grades = final_grades.groupby(\"hobby\")\ngrouped_grades\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x00000273EBA146D0>\n\n\n이제 hobby마다 평균 점수를 계산할 수 있습니다:\n\ngrouped_grades.mean()\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n    \n      hobby\n      \n      \n      \n      \n    \n  \n  \n    \n      Biking\n      8.5\n      9.0\n      10.0\n      NaN\n    \n    \n      Dancing\n      10.0\n      9.0\n      10.0\n      NaN\n    \n  \n\n\n\n\n\nfinal_grades.groupby(\"hobby\").mean()\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n    \n      hobby\n      \n      \n      \n      \n    \n  \n  \n    \n      Biking\n      8.5\n      9.0\n      10.0\n      NaN\n    \n    \n      Dancing\n      10.0\n      9.0\n      10.0\n      NaN\n    \n  \n\n\n\n\n아주 쉽네요! 평균을 계산할 때 NaN 값은 그냥 무시됩니다."
  },
  {
    "objectID": "Datamining/pandas/pandas.html#피봇-테이블",
    "href": "Datamining/pandas/pandas.html#피봇-테이블",
    "title": "Pandas",
    "section": "피봇 테이블",
    "text": "피봇 테이블\n판다스는 스프레드시트와 비슷하 피봇 테이블을 지원하여 데이터를 빠르게 요약할 수 있습니다. 어떻게 동작하는 알아 보기 위해 간단한 DataFrame을 만들어 보죠:\n\nbonus_points.stack().reset_index()\n\nNameError: name 'bonus_points' is not defined\n\n\n\nmore_grades = final_grades_clean.stack().reset_index()\nmore_grades.columns = [\"name\", \"month\", \"grade\"]\nmore_grades[\"bonus\"] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0]\nmore_grades\n\n\n\n\n\n  \n    \n      \n      name\n      month\n      grade\n      bonus\n    \n  \n  \n    \n      0\n      alice\n      sep\n      8.0\n      NaN\n    \n    \n      1\n      alice\n      oct\n      8.0\n      NaN\n    \n    \n      2\n      alice\n      nov\n      9.0\n      NaN\n    \n    \n      3\n      bob\n      sep\n      10.0\n      0.0\n    \n    \n      4\n      bob\n      oct\n      9.0\n      NaN\n    \n    \n      5\n      bob\n      nov\n      10.0\n      2.0\n    \n    \n      6\n      charles\n      sep\n      4.0\n      3.0\n    \n    \n      7\n      charles\n      oct\n      11.0\n      3.0\n    \n    \n      8\n      charles\n      nov\n      5.0\n      0.0\n    \n    \n      9\n      darwin\n      sep\n      9.0\n      0.0\n    \n    \n      10\n      darwin\n      oct\n      10.0\n      1.0\n    \n    \n      11\n      darwin\n      nov\n      11.0\n      0.0\n    \n  \n\n\n\n\n이제 이 DataFrame에 대해 pd.pivot_table() 함수를 호출하고 name 열로 그룹핑합니다. 기본적으로 pivot_table()은 수치 열의 평균을 계산합니다:\n\npd.pivot_table(more_grades, index=\"name\")\n\nNameError: name 'more_grades' is not defined\n\n\n집계 함수를 aggfunc 매개변수로 바꿀 수 있습니다. 또한 집계 대상의 열을 리스트로 지정할 수 있습니다:\n\npd.pivot_table(more_grades, index=\"name\", values=[\"grade\",\"bonus\"], aggfunc=np.max)\n\n\n\n\n\n  \n    \n      \n      bonus\n      grade\n    \n    \n      name\n      \n      \n    \n  \n  \n    \n      alice\n      NaN\n      9.0\n    \n    \n      bob\n      2.0\n      10.0\n    \n    \n      charles\n      3.0\n      11.0\n    \n    \n      darwin\n      1.0\n      11.0\n    \n  \n\n\n\n\ncolumns 매개변수를 지정하여 수평으로 집계할 수 있고 margins=True로 설정해 각 행과 열에 대해 전체 합을 계산할 수 있습니다:\n\npd.pivot_table(more_grades, index=\"name\", values=\"grade\", columns=\"month\", margins=True)\n\n\n\n\n\n  \n    \n      month\n      nov\n      oct\n      sep\n      All\n    \n    \n      name\n      \n      \n      \n      \n    \n  \n  \n    \n      alice\n      9.00\n      8.0\n      8.00\n      8.333333\n    \n    \n      bob\n      10.00\n      9.0\n      10.00\n      9.666667\n    \n    \n      charles\n      5.00\n      11.0\n      4.00\n      6.666667\n    \n    \n      darwin\n      11.00\n      10.0\n      9.00\n      10.000000\n    \n    \n      All\n      8.75\n      9.5\n      7.75\n      8.666667\n    \n  \n\n\n\n\n마지막으로 여러 개의 인덱스나 열 이름을 지정하면 판다스가 다중 레벨 인덱스를 만듭니다:\n\npd.pivot_table(more_grades, index=(\"name\", \"month\"), margins=True)\n\n\n\n\n\n  \n    \n      \n      \n      bonus\n      grade\n    \n    \n      name\n      month\n      \n      \n    \n  \n  \n    \n      alice\n      nov\n      NaN\n      9.00\n    \n    \n      oct\n      NaN\n      8.00\n    \n    \n      sep\n      NaN\n      8.00\n    \n    \n      bob\n      nov\n      2.000\n      10.00\n    \n    \n      oct\n      NaN\n      9.00\n    \n    \n      sep\n      0.000\n      10.00\n    \n    \n      charles\n      nov\n      0.000\n      5.00\n    \n    \n      oct\n      3.000\n      11.00\n    \n    \n      sep\n      3.000\n      4.00\n    \n    \n      darwin\n      nov\n      0.000\n      11.00\n    \n    \n      oct\n      1.000\n      10.00\n    \n    \n      sep\n      0.000\n      9.00\n    \n    \n      All\n      \n      1.125\n      8.75"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#함수",
    "href": "Datamining/pandas/pandas.html#함수",
    "title": "Pandas",
    "section": "함수",
    "text": "함수\n큰 DataFrame을 다룰 때 내용을 간단히 요약하는 것이 도움이 됩니다. 판다스는 이를 위한 몇 가지 함수를 제공합니다. 먼저 수치 값, 누락된 값, 텍스트 값이 섞인 큰 DataFrame을 만들어 보죠. 주피터 노트북은 이 DataFrame의 일부만 보여줍니다:\n\nmuch_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26))\nlarge_df = pd.DataFrame(much_data, columns=list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\nlarge_df[large_df % 16 == 0] = np.nan\nlarge_df.insert(3,\"some_text\", \"Blabla\")\nlarge_df\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      some_text\n      D\n      E\n      F\n      G\n      H\n      I\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      0\n      NaN\n      11.0\n      44.0\n      Blabla\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n      ...\n      11.0\n      NaN\n      11.0\n      44.0\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n    \n    \n      1\n      11.0\n      22.0\n      55.0\n      Blabla\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n      ...\n      22.0\n      11.0\n      22.0\n      55.0\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n    \n    \n      2\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      3\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n    \n      4\n      44.0\n      55.0\n      88.0\n      Blabla\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n      ...\n      55.0\n      44.0\n      55.0\n      88.0\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9995\n      NaN\n      NaN\n      33.0\n      Blabla\n      88.0\n      165.0\n      77.0\n      11.0\n      154.0\n      132.0\n      ...\n      NaN\n      NaN\n      NaN\n      33.0\n      88.0\n      165.0\n      77.0\n      11.0\n      154.0\n      132.0\n    \n    \n      9996\n      NaN\n      11.0\n      44.0\n      Blabla\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n      ...\n      11.0\n      NaN\n      11.0\n      44.0\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n    \n    \n      9997\n      11.0\n      22.0\n      55.0\n      Blabla\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n      ...\n      22.0\n      11.0\n      22.0\n      55.0\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n    \n    \n      9998\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      9999\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n  \n\n10000 rows × 27 columns\n\n\n\nhead() 메서드는 처음 5개 행을 반환합니다:\n\nlarge_df.head(n=10)\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      some_text\n      D\n      E\n      F\n      G\n      H\n      I\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      0\n      NaN\n      11.0\n      44.0\n      Blabla\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n      ...\n      11.0\n      NaN\n      11.0\n      44.0\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n    \n    \n      1\n      11.0\n      22.0\n      55.0\n      Blabla\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n      ...\n      22.0\n      11.0\n      22.0\n      55.0\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n    \n    \n      2\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      3\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n    \n      4\n      44.0\n      55.0\n      88.0\n      Blabla\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n      ...\n      55.0\n      44.0\n      55.0\n      88.0\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n    \n    \n      5\n      55.0\n      66.0\n      99.0\n      Blabla\n      154.0\n      44.0\n      143.0\n      77.0\n      33.0\n      11.0\n      ...\n      66.0\n      55.0\n      66.0\n      99.0\n      154.0\n      44.0\n      143.0\n      77.0\n      33.0\n      11.0\n    \n    \n      6\n      66.0\n      77.0\n      110.0\n      Blabla\n      165.0\n      55.0\n      154.0\n      88.0\n      44.0\n      22.0\n      ...\n      77.0\n      66.0\n      77.0\n      110.0\n      165.0\n      55.0\n      154.0\n      88.0\n      44.0\n      22.0\n    \n    \n      7\n      77.0\n      88.0\n      121.0\n      Blabla\n      NaN\n      66.0\n      165.0\n      99.0\n      55.0\n      33.0\n      ...\n      88.0\n      77.0\n      88.0\n      121.0\n      NaN\n      66.0\n      165.0\n      99.0\n      55.0\n      33.0\n    \n    \n      8\n      88.0\n      99.0\n      132.0\n      Blabla\n      NaN\n      77.0\n      NaN\n      110.0\n      66.0\n      44.0\n      ...\n      99.0\n      88.0\n      99.0\n      132.0\n      NaN\n      77.0\n      NaN\n      110.0\n      66.0\n      44.0\n    \n    \n      9\n      99.0\n      110.0\n      143.0\n      Blabla\n      11.0\n      88.0\n      NaN\n      121.0\n      77.0\n      55.0\n      ...\n      110.0\n      99.0\n      110.0\n      143.0\n      11.0\n      88.0\n      NaN\n      121.0\n      77.0\n      55.0\n    \n  \n\n10 rows × 27 columns\n\n\n\n마지막 5개 행을 반환하는 tail() 함수도 있습니다. 원하는 행 개수를 전달할 수도 있습니다:\n\nlarge_df.tail(n=2)\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      some_text\n      D\n      E\n      F\n      G\n      H\n      I\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      9998\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      9999\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n  \n\n2 rows × 27 columns\n\n\n\ninfo() 메서드는 각 열의 내용을 요약하여 출력합니다:\n\nlarge_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 27 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   A          8823 non-null   float64\n 1   B          8824 non-null   float64\n 2   C          8824 non-null   float64\n 3   some_text  10000 non-null  object \n 4   D          8824 non-null   float64\n 5   E          8822 non-null   float64\n 6   F          8824 non-null   float64\n 7   G          8824 non-null   float64\n 8   H          8822 non-null   float64\n 9   I          8823 non-null   float64\n 10  J          8823 non-null   float64\n 11  K          8822 non-null   float64\n 12  L          8824 non-null   float64\n 13  M          8824 non-null   float64\n 14  N          8822 non-null   float64\n 15  O          8824 non-null   float64\n 16  P          8824 non-null   float64\n 17  Q          8824 non-null   float64\n 18  R          8823 non-null   float64\n 19  S          8824 non-null   float64\n 20  T          8824 non-null   float64\n 21  U          8824 non-null   float64\n 22  V          8822 non-null   float64\n 23  W          8824 non-null   float64\n 24  X          8824 non-null   float64\n 25  Y          8822 non-null   float64\n 26  Z          8823 non-null   float64\ndtypes: float64(26), object(1)\nmemory usage: 2.1+ MB\n\n\n마지막으로 describe() 메서드는 각 열에 대한 주요 집계 연산을 수행한 결과를 보여줍니다:\nFinally, the describe() method gives a nice overview of the main aggregated values over each column: * count: null(NaN)이 아닌 값의 개수 * mean: null이 아닌 값의 평균 * std: null이 아닌 값의 표준 편차 * min: null이 아닌 값의 최솟값 * 25%, 50%, 75%: null이 아닌 값의 25번째, 50번째, 75번째 백분위수 * max: null이 아닌 값의 최댓값\n\nlarge_df.describe()\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n      F\n      G\n      H\n      I\n      J\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      count\n      8823.000000\n      8824.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8823.000000\n      8823.000000\n      ...\n      8824.000000\n      8823.000000\n      8824.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8823.000000\n    \n    \n      mean\n      87.977559\n      87.972575\n      87.987534\n      88.012466\n      87.983791\n      88.007480\n      87.977561\n      88.000000\n      88.022441\n      88.022441\n      ...\n      87.972575\n      87.977559\n      87.972575\n      87.987534\n      88.012466\n      87.983791\n      88.007480\n      87.977561\n      88.000000\n      88.022441\n    \n    \n      std\n      47.535911\n      47.535523\n      47.521679\n      47.521679\n      47.535001\n      47.519371\n      47.529755\n      47.536879\n      47.535911\n      47.535911\n      ...\n      47.535523\n      47.535911\n      47.535523\n      47.521679\n      47.521679\n      47.535001\n      47.519371\n      47.529755\n      47.536879\n      47.535911\n    \n    \n      min\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      ...\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n    \n    \n      25%\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      ...\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n    \n    \n      50%\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      ...\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n    \n    \n      75%\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      ...\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n    \n    \n      max\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      ...\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n    \n  \n\n8 rows × 26 columns"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#저장",
    "href": "Datamining/pandas/pandas.html#저장",
    "title": "Pandas",
    "section": "저장",
    "text": "저장\nCSV, HTML, JSON로 저장해 보죠:\n\nmy_df.to_csv(\"my_df.csv\")\nmy_df.to_html(\"my_df.html\")\nmy_df.to_json(\"my_df.json\")\n\n저장된 내용을 확인해 보죠:\n\nfor filename in (\"my_df.csv\", \"my_df.html\", \"my_df.json\"):\n    print(\"#\", filename)\n    with open(filename, \"rt\") as f:\n        print(f.read())\n        print()\n\n# my_df.csv\n,hobby,weight,birthyear,children\nalice,Biking,68.5,1985,\nbob,Dancing,83.1,1984,3.0\n\n\n# my_df.html\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hobby</th>\n      <th>weight</th>\n      <th>birthyear</th>\n      <th>children</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>alice</th>\n      <td>Biking</td>\n      <td>68.5</td>\n      <td>1985</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>bob</th>\n      <td>Dancing</td>\n      <td>83.1</td>\n      <td>1984</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n\n# my_df.json\n{\"hobby\":{\"alice\":\"Biking\",\"bob\":\"Dancing\"},\"weight\":{\"alice\":68.5,\"bob\":83.1},\"birthyear\":{\"alice\":1985,\"bob\":1984},\"children\":{\"alice\":null,\"bob\":3.0}}\n\n\n\n인덱스는 (이름 없이) CSV 파일의 첫 번째 열에 저장되었습니다. HTML에서는 <th> 태그와 JSON에서는 키로 저장되었습니다.\n다른 포맷으로 저장하는 것도 비슷합니다. 하지만 일부 포맷은 추가적인 라이브러리 설치가 필요합니다. 예를 들어, 엑셀로 저장하려면 openpyxl 라이브러리가 필요합니다:\n\ntry:\n    my_df.to_excel(\"my_df.xlsx\", sheet_name='People')\nexcept ImportError as e:\n    print(e)\n\nNo module named 'openpyxl'"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#로딩",
    "href": "Datamining/pandas/pandas.html#로딩",
    "title": "Pandas",
    "section": "로딩",
    "text": "로딩\nCSV 파일을 DataFrame으로 로드해 보죠:\n\nmy_df_loaded = pd.read_csv(\"my_df.csv\", index_col=0)\nmy_df_loaded\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      birthyear\n      children\n    \n  \n  \n    \n      alice\n      Biking\n      68.5\n      1985\n      NaN\n    \n    \n      bob\n      Dancing\n      83.1\n      1984\n      3.0\n    \n  \n\n\n\n\n예상할 수 있듯이 read_json, read_html, read_excel 함수도 있습니다. 인터넷에서 데이터를 바로 읽을 수도 있습니다. 예를 들어 깃허브에서 1,000개의 U.S. 도시를 로드해 보죠:\n\nus_cities = None\ntry:\n    csv_url = \"https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv\"\n    us_cities = pd.read_csv(csv_url, index_col=0)\n    us_cities = us_cities.head()\nexcept IOError as e:\n    print(e)\nus_cities\n\n\n\n\n\n  \n    \n      \n      State\n      Population\n      lat\n      lon\n    \n    \n      City\n      \n      \n      \n      \n    \n  \n  \n    \n      Marysville\n      Washington\n      63269\n      48.051764\n      -122.177082\n    \n    \n      Perris\n      California\n      72326\n      33.782519\n      -117.228648\n    \n    \n      Cleveland\n      Ohio\n      390113\n      41.499320\n      -81.694361\n    \n    \n      Worcester\n      Massachusetts\n      182544\n      42.262593\n      -71.802293\n    \n    \n      Columbia\n      South Carolina\n      133358\n      34.000710\n      -81.034814\n    \n  \n\n\n\n\n이외에도 많은 옵션이 있습니다. 특히 datetime 포맷에 관련된 옵션이 많습니다. 더 자세한 내용은 온라인 문서를 참고하세요."
  },
  {
    "objectID": "Datamining/pandas/pandas.html#sql-조인",
    "href": "Datamining/pandas/pandas.html#sql-조인",
    "title": "Pandas",
    "section": "SQL 조인",
    "text": "SQL 조인\n판다스의 강력한 기능 중 하나는 DataFrame에 대해 SQL 같은 조인(join)을 수행할 수 있는 것입니다. 여러 종류의 조인이 지원됩니다. 이너 조인(inner join), 레프트/라이트 아우터 조인(left/right outer join), 풀 조인(full join)입니다. 이에 대해 알아 보기 위해 간단한 DataFrame을 만들어 보죠:\n\ncity_loc = pd.DataFrame(\n    [\n        [\"CA\", \"San Francisco\", 37.781334, -122.416728],\n        [\"NY\", \"New York\", 40.705649, -74.008344],\n        [\"FL\", \"Miami\", 25.791100, -80.320733],\n        [\"OH\", \"Cleveland\", 41.473508, -81.739791],\n        [\"UT\", \"Salt Lake City\", 40.755851, -111.896657]\n    ], columns=[\"state\", \"city\", \"lat\", \"lng\"])\ncity_loc\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n    \n  \n\n\n\n\n\ncity_pop = pd.DataFrame(\n    [\n        [808976, \"San Francisco\", \"California\"],\n        [8363710, \"New York\", \"New-York\"],\n        [413201, \"Miami\", \"Florida\"],\n        [2242193, \"Houston\", \"Texas\"]\n    ], index=[3,4,5,6], columns=[\"population\", \"city\", \"state\"])\ncity_pop\n\n\n\n\n\n  \n    \n      \n      population\n      city\n      state\n    \n  \n  \n    \n      3\n      808976\n      San Francisco\n      California\n    \n    \n      4\n      8363710\n      New York\n      New-York\n    \n    \n      5\n      413201\n      Miami\n      Florida\n    \n    \n      6\n      2242193\n      Houston\n      Texas\n    \n  \n\n\n\n\n이제 merge() 함수를 사용해 이 DataFrame을 조인해 보죠:\n\npd.merge(left=city_loc, right=city_pop, on=\"city\")\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201\n      Florida\n    \n  \n\n\n\n\n두 DataFrame은 state란 이름의 열을 가지고 있으므로 state_x와 state_y로 이름이 바뀌었습니다.\n또한 Cleveland, Salt Lake City, Houston은 두 DataFrame에 모두 존재하지 않기 때문에 삭제되었습니다. SQL의 INNER JOIN과 동일합니다. 도시를 삭제하지 않고 NaN으로 채우는 FULL OUTER JOIN을 원하면 how=\"outer\"로 지정합니다:\n\nall_cities = pd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"outer\")\nall_cities\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976.0\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710.0\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201.0\n      Florida\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n      NaN\n    \n    \n      5\n      NaN\n      Houston\n      NaN\n      NaN\n      2242193.0\n      Texas\n    \n  \n\n\n\n\n물론 LEFT OUTER JOIN은 how=\"left\"로 지정할 수 있습니다. 왼쪽의 DataFrame에 있는 도시만 남습니다. 비슷하게 how=\"right\"는 오른쪽 DataFrame에 있는 도시만 결과에 남습니다. 예를 들면:\n\npd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"right\")\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201\n      Florida\n    \n    \n      3\n      NaN\n      Houston\n      NaN\n      NaN\n      2242193\n      Texas\n    \n  \n\n\n\n\n조인할 키가 DataFrame 인덱스라면 left_index=True나 right_index=True로 지정해야 합니다. 키 열의 이름이 다르면 left_on과 right_on을 사용합니다. 예를 들어:\n\ncity_pop2 = city_pop.copy()\ncity_pop2.columns = [\"population\", \"name\", \"state\"]\ncity_pop2\n\n\n\n\n\n  \n    \n      \n      population\n      name\n      state\n    \n  \n  \n    \n      3\n      808976\n      San Francisco\n      California\n    \n    \n      4\n      8363710\n      New York\n      New-York\n    \n    \n      5\n      413201\n      Miami\n      Florida\n    \n    \n      6\n      2242193\n      Houston\n      Texas\n    \n  \n\n\n\n\n\npd.merge(left=city_loc, right=city_pop2, left_on=\"city\", right_on=\"name\")\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      name\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976\n      San Francisco\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710\n      New York\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201\n      Miami\n      Florida"
  },
  {
    "objectID": "Datamining/pandas/pandas.html#연결",
    "href": "Datamining/pandas/pandas.html#연결",
    "title": "Pandas",
    "section": "연결",
    "text": "연결\nDataFrame을 조인하는 대신 그냥 연결할 수도 있습니다. concat() 함수가 하는 일입니다:\n\ncity_loc\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n    \n  \n\n\n\n\n\ncity_pop\n\n\n\n\n\n  \n    \n      \n      population\n      city\n      state\n    \n  \n  \n    \n      3\n      808976\n      San Francisco\n      California\n    \n    \n      4\n      8363710\n      New York\n      New-York\n    \n    \n      5\n      413201\n      Miami\n      Florida\n    \n    \n      6\n      2242193\n      Houston\n      Texas\n    \n  \n\n\n\n\n\nresult_concat = pd.concat([city_loc, city_pop])\nresult_concat\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      4\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      5\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      6\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n이 연산은 (행을 따라) 수직적으로 데이터를 연결하고 (열을 따라) 수평으로 연결하지 않습니다. 이 예에서 동일한 인덱스를 가진 행이 있습니다(예를 들면 3). 판다스는 이를 우아하게 처리합니다:\n\nresult_concat.loc[3]\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n  \n\n\n\n\n또는 인덱스를 무시하도록 설정할 수 있습니다:\n\npd.concat([city_loc, city_pop], ignore_index=True)\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      5\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      6\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      7\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      8\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n한 DataFrame에 열이 없을 때 NaN이 채워져 있는 것처럼 동작합니다. join=\"inner\"로 설정하면 양쪽의 DataFrame에 존재하는 열만 반환됩니다:\n\npd.concat([city_loc, city_pop], join=\"inner\")\n\n\n\n\n\n  \n    \n      \n      state\n      city\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n    \n    \n      1\n      NY\n      New York\n    \n    \n      2\n      FL\n      Miami\n    \n    \n      3\n      OH\n      Cleveland\n    \n    \n      4\n      UT\n      Salt Lake City\n    \n    \n      3\n      California\n      San Francisco\n    \n    \n      4\n      New-York\n      New York\n    \n    \n      5\n      Florida\n      Miami\n    \n    \n      6\n      Texas\n      Houston\n    \n  \n\n\n\n\naxis=1로 설정하면 DataFrame을 수직이 아니라 수평으로 연결할 수 있습니다:\n\npd.concat([city_loc, city_pop], axis=1)\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n      city\n      state\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      808976.0\n      San Francisco\n      California\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      8363710.0\n      New York\n      New-York\n    \n    \n      5\n      NaN\n      NaN\n      NaN\n      NaN\n      413201.0\n      Miami\n      Florida\n    \n    \n      6\n      NaN\n      NaN\n      NaN\n      NaN\n      2242193.0\n      Houston\n      Texas\n    \n  \n\n\n\n\n이 경우 인덱스가 잘 정렬되지 않기 때문에 의미가 없습니다(예를 들어 Cleveland와 San Francisco의 인덱스 레이블이 3이기 때문에 동일한 행에 놓여 있습니다). 이 DataFrame을 연결하기 전에 도시로 인덱스를 재설정해 보죠:\n\npd.concat([city_loc.set_index(\"city\"), city_pop.set_index(\"city\")], axis=1)\n\n\n\n\n\n  \n    \n      \n      state\n      lat\n      lng\n      population\n      state\n    \n    \n      city\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      San Francisco\n      CA\n      37.781334\n      -122.416728\n      808976.0\n      California\n    \n    \n      New York\n      NY\n      40.705649\n      -74.008344\n      8363710.0\n      New-York\n    \n    \n      Miami\n      FL\n      25.791100\n      -80.320733\n      413201.0\n      Florida\n    \n    \n      Cleveland\n      OH\n      41.473508\n      -81.739791\n      NaN\n      NaN\n    \n    \n      Salt Lake City\n      UT\n      40.755851\n      -111.896657\n      NaN\n      NaN\n    \n    \n      Houston\n      NaN\n      NaN\n      NaN\n      2242193.0\n      Texas\n    \n  \n\n\n\n\nFULL OUTER JOIN을 수행한 것과 비슷합니다. 하지만 state 열이 state_x와 state_y로 바뀌지 않았고 city 열이 인덱스가 되었습니다.\nappend() 메서드는 DataFrame을 수직으로 연결하는 단축 메서드입니다:\n\ncity_loc.append(city_pop)\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      4\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      5\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      6\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n\npd.concat([city_loc,city_pop])\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      4\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      5\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      6\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n판다스의 다른 메서드와 마찬가지로 append() 메서드는 실제 city_loc을 수정하지 않습니다. 복사본을 만들어 수정한 다음 반환합니다."
  },
  {
    "objectID": "Datamining/seaborn/seaborn & matplotlib.html",
    "href": "Datamining/seaborn/seaborn & matplotlib.html",
    "title": "seabron",
    "section": "",
    "text": "1.1 Load data\n\n예제로 사용할 펭귄 데이터를 불러옵니다.\nseaborn에 내장되어 있습니다.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female\n    \n  \n\n\n\n\n\n\n1.2 Figure and Axes\n\nmatplotlib으로 도화지figure를 깔고 축공간axes를 만듭니다.\n1 x 2 축공간을 구성합니다.\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))  #8인치 4인치 크기를 가진 그림 생성\n                                                # figsize : 그림의 크기를 지정하는 ㄳ\nfig.tight_layout()\n\n\n\n\n\n\n1.3 plot with matplotlib\n\nmatplotlib 기능을 이용해서 산점도를 그립니다.\nx축은 부리 길이 bill length\ny축은 부리 위 아래 두께 bill depth\n색상은 종species로 합니다.\nAdelie, Chinstrap, Gentoo이 있습니다.\n두 축공간 중 왼쪽에만 그립니다. 컬러를 다르게 주기 위해 f-string 포맷을 사용했습니다. f-string 포맷에 대한 설명은 https://blockdmask.tistory.com/429를 참고하세요\n\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\nfor i, s in enumerate(species_u):  #enumerate() : 자료형(list,tuple,string)을 인덱스 값을 포함하게 /바꿈 0,1,2,3으로 숫자 부여\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],  #[0]: 왼쪽에 그리기\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)   #c{i}  : 문자열로 나타내기  텍스트 만들어주어 텍스트 색에 따라 만들어지는것임\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plt.show()\nfig.tight_layout()\n\n\n\n\n조금 더 간단히 그리는 방법 matplotlib는 기본적으로 Categorical 변수를 color로 바로 사용하지 못함\n\npenguins[\"species_codes\"] = pd.Categorical(penguins[\"species\"]).codes  #확인 해보기\n\n\n# We transform text categorical variables into numerical variables\npenguins[\"species_codes\"] = pd.Categorical(penguins[\"species\"]).codes #pd.categorical : 범주형 데이터 처리 함수\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\naxes[0].scatter(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", c=\"species_codes\", alpha=0.3)   #하나만 표시 하고 싶으면 axes[0]을 axes로 바꾸기\n\n<matplotlib.collections.PathCollection at 0x164375d7fd0>\n\n\n\n\n\n\n\n1.4 Plot with seaborn\n\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n\n# plot 1 : seaborn\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()\n\n\n\n\n\n단 세 줄로 거의 동일한 그림이 나왔습니다.\nscatter plot의 점 크기만 살짝 작습니다.\nlabel의 투명도만 살짝 다릅니다.\nseaborn 명령 scatterplot()을 그대로 사용했습니다. x축과 y축 label도 바꾸었습니다. ax=axes[1] 인자에서 볼 수 있듯, 존재하는 axes에 그림만 얹었습니다. matplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통합니다. # 1.5 matplotlib + seaborn & seaborn + matplotlib\nmatplotlib과 seaborn이 자유롭게 섞일 수 있습니다.\nmatplotlib 산점도 위에 seaborn 추세선을 얹을 수 있고,\nseaborn 산점도 위에 matplotlib 중심점을 얹을 수 있습니다.\n파이썬 코드는 다음과 같습니다.\n\n\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n\nspecies_u = penguins[\"species\"].unique()\n\n# plot 0 : matplotlib + seaborn\nfor i, s in enumerate(species_u):\n    # matplotlib 산점도\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],  #axes[]를 잘 맞추면 그림을 잘 그릴수 있음\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                   c=f\"C{i}\", label=s, alpha=0.3\n                  )\n                  \n    # seaborn 추세선  #regplot이 추세선 그리는 함수\n    sns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=penguins.loc[penguins[\"species\"]==s], \n                scatter=False, ax=axes[0])\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n# plot 1 : seaborn + matplotlib\n# seaborn 산점도\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfor i, s in enumerate(species_u):\n    # matplotlib 중심점 #scatter이 산점도 그리는 함수\n    axes[1].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),  #각각을 평균을 구해라\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   c=f\"C{i}\", alpha=1, marker=\"x\", s=100\n                  )\n\nfig.tight_layout()\n\n\n\n\n\n\n1.6 seaborn + seaborn + matplotlib\n\n안 될 이유가 없습니다.\nseaborn scatterplot + seaborn kdeplot + matplotlib text입니다\n\n\nfig, ax = plt.subplots(figsize=(6,5))\n\n# plot 0: scatter plot\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"k\", data=penguins, alpha=0.3, ax=ax, legend=False)\n\n# plot 1: kde plot #곡선 그리는 함수\nsns.kdeplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.5, ax=ax, legend=False)\n\n# text:\nspecies_u = penguins[\"species\"].unique()\nfor i, s in enumerate(species_u):\n    ax.text(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n            penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n            s = s, fontdict={\"fontsize\":14, \"fontweight\":\"bold\",\"color\":\"k\"}\n            )\n\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout() #서브 플롯 사이의 간격이나 축을 자동으로 조정하는 함수\n\n\n\n\n\n\nQize\nbill length를 10단위로 나눈 후, bill length에 따른 Bill depth의 boxplot을 그려라\n\npenguins[\"bill_length_10\"] = penguins[\"bill_length_mm\"]//10 * 10\npenguins[\"bill_length_10\"]\n\n0      30.0\n1      30.0\n2      40.0\n3       NaN\n4      30.0\n       ... \n339     NaN\n340    40.0\n341    50.0\n342    40.0\n343    40.0\nName: bill_length_10, Length: 344, dtype: float64\n\n\n\n#연속형을 범주형으로 \npenguins[\"bill_length_10\"] = (penguins[\"bill_length_mm\"]//10 )* 10\n\nfig, axes = plt.subplots(figsize=(8,4))\n\nsns.boxplot(x=\"bill_length_10\", y=\"bill_depth_mm\", data=penguins)\nsns.stripplot(x=\"bill_length_10\", y=\"bill_depth_mm\", color=\"black\", data=penguins,size = 4)\n\nplt.show()\n\n#fig.tight_layout()\n\n\n\n\n\n박스 플랏을 각각 그리기\n\n\npenguins[\"bill_length_group\"] = pd.cut(penguins[\"bill_length_mm\"],\n                                      bins=[0,40,50,60],\n                                        labels = [\"0-40\",\"40-50\",\"50-60\"])     \n\nsns.boxplot(x=\"bill_length_group\", y=\"bill_depth_mm\", data=penguins)\nsns.stripplot(x=\"bill_length_group\", y=\"bill_depth_mm\", color=\"black\", data=penguins,size = 4)\n\nsns.set_style(\"whitegrid\")\nsns.despine()\n\nplt.show()\n\n\n\n\n\nsns.scatterplot(x=\"bill_length_mm\",y=\"bill_depth_mm\",hue=\"species\",data=penguins,alpha = 0.3)\n\nplt.show()\n\n\n\n\n\n그래프 나눠 그리기 : FacetGrid\n\n\ng=sns.FacetGrid(penguins,col=\"species\",col_wrap = 3,hue = \"sex\")  \ng.map(sns.scatterplot,\"bill_length_mm\",\"bill_depth_mm\") \n\nsns.set_style(\"whitegrid\")\nsns.despine()\n\nplt.show()\n\nc:\\python\\lib\\site-packages\\seaborn\\axisgrid.py:676: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\nc:\\python\\lib\\site-packages\\seaborn\\axisgrid.py:676: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\nc:\\python\\lib\\site-packages\\seaborn\\axisgrid.py:676: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\nc:\\python\\lib\\site-packages\\seaborn\\axisgrid.py:676: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\nc:\\python\\lib\\site-packages\\seaborn\\axisgrid.py:676: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\nc:\\python\\lib\\site-packages\\seaborn\\axisgrid.py:676: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n\n\n\n\ng=sns.FacetGrid(penguins,col=\"species\",col_wrap = 3)\ng.map(sns.boxplot,\"bill_length_group\",\"bill_depth_mm\",order=[\"0-40\",\"40-50\",\"50-60\"])\ng.map(sns.stripplot,\"bill_length_group\",\"bill_depth_mm\",color=\"black\",size = 4,order=[\"0-40\",\"40-50\",\"50-60\"])\n\nsns.set_style(\"whitegrid\")\nsns.despine()\n\nplt.show()\n\nc:\\python\\lib\\site-packages\\seaborn\\axisgrid.py:676: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\nc:\\python\\lib\\site-packages\\seaborn\\axisgrid.py:676: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\nc:\\python\\lib\\site-packages\\seaborn\\axisgrid.py:676: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\nc:\\python\\lib\\site-packages\\seaborn\\axisgrid.py:676: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\nc:\\python\\lib\\site-packages\\seaborn\\axisgrid.py:676: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\nc:\\python\\lib\\site-packages\\seaborn\\axisgrid.py:676: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n\n\n\n\n위 그래프를 scatterplot으로 그려보기\n\n\ng=sns.FacetGrid(penguins,col=\"species\",col_wrap = 4,hue=\"species\")  #hue로 색 지정 가능\ng.map(sns.scatterplot,\"bill_length_mm\",\"bill_depth_mm\")\n\nsns.set_style(\"whitegrid\")\nsns.despine()\n\nplt.show()\n\nNameError: name 'sns' is not defined"
  },
  {
    "objectID": "Datamining/Tools_matplotlib/Tools_matplotlib.html",
    "href": "Datamining/Tools_matplotlib/Tools_matplotlib.html",
    "title": "Tools_matplotlib",
    "section": "",
    "text": "원문: by Aurélien Geron (Link)\nTranslated by Chansung PARK (Link)\n\nObject Oriented API Addition by Jehyun LEE (Link)\n\nTools - matplotlib\n이 노트북은 matplotlib 라이브러리를 사용하여 아름다운 그래프를 그리는 방법을 보여줍니다.\n\n이제현 주 : * 원 코드가 pyplot 기반으로 작성되었기에 object oriented API를 추가하였습니다. * pyplot은 pandas 같은 라이브러리와 함께 사용하며 그래프를 빠르게 그려보기 좋습니다. 그러나 코드의 가독성과 섬세한 제어는 object oriented API(객체지향 인터페이스)방식이 더 유리하게 느껴집니다. * pyplot과 object oriented API의 차이에 대해 상세히 알고 싶으시면 이 글을 참고하십시오"
  },
  {
    "objectID": "Datamining.html",
    "href": "Datamining.html",
    "title": "Numpy & Pandas & Loops",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\ngeopandas 설치법\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\nFor loop 속도 개선하기\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\n데이터 마이닝 정리\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkimdayeon\n\n\n\n\n\n\n  \n\n\n\n\nNumpy\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\nPandas\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\nseabron\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\nTools_matplotlib\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\nUber\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Exercise/1. Your First Map exercise.html",
    "href": "Exercise/1. Your First Map exercise.html",
    "title": "1. Your First Map Exercise",
    "section": "",
    "text": "Introduction\nKiva.org is an online crowdfunding platform extending financial services to poor people around the world. Kiva lenders have provided over $1 billion dollars in loans to over 2 million people.\n\n\n\nKiva reaches some of the most remote places in the world through their global network of “Field Partners”. These partners are local organizations working in communities to vet borrowers, provide services, and administer loans.\nIn this exercise, you’ll investigate Kiva loans in the Philippines. Can you identify regions that might be outside of Kiva’s current network, in order to identify opportunities for recruiting new Field Partners?\nTo get started, run the code cell below to set up our feedback system.\n\n비영리단체가 운영을 확장할 수 있는 필리핀의 원격 지역을 식별하시오.\n\n\nimport geopandas as gpd\n\n\n# Read in the data\nworld_loans = gpd.read_file(\"D:/archive (1)/kiva_loans/kiva_loans/kiva_loans.shp\")\n\n\nworld_loans.head()\n\n\n\n\n\n  \n    \n      \n      Partner ID\n      Field Part\n      sector\n      Loan Theme\n      country\n      amount\n      geometry\n    \n  \n  \n    \n      0\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Higher Education\n      Cambodia\n      450\n      POINT (102.89751 13.66726)\n    \n    \n      1\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Vulnerable Populations\n      Cambodia\n      20275\n      POINT (102.98962 13.02870)\n    \n    \n      2\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Higher Education\n      Cambodia\n      9150\n      POINT (102.98962 13.02870)\n    \n    \n      3\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Vulnerable Populations\n      Cambodia\n      604950\n      POINT (105.31312 12.09829)\n    \n    \n      4\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Sanitation\n      Cambodia\n      275\n      POINT (105.31312 12.09829)\n    \n  \n\n\n\n\n\n\nplot the data\n\n# This dataset is provided in GeoPandas\nworld_filepath = gpd.datasets.get_path('naturalearth_lowres')\nworld = gpd.read_file(world_filepath)\nworld.head()\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      0\n      889953.0\n      Oceania\n      Fiji\n      FJI\n      5496\n      MULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n    \n    \n      1\n      58005463.0\n      Africa\n      Tanzania\n      TZA\n      63177\n      POLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n    \n    \n      2\n      603253.0\n      Africa\n      W. Sahara\n      ESH\n      907\n      POLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n    \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n    \n      4\n      328239523.0\n      North America\n      United States of America\n      USA\n      21433226\n      MULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n    \n  \n\n\n\n\nworld 와 world_loans 지도로 표현하여 대출 위치 시각화\n\nax = world.plot(figsize=(10,10), color='none',edgecolor='gainsboro', zorder=3)\n\nworld_loans.plot(color=\"green\",ax=ax,markersize=2)\n\n<Axes: >\n\n\n\n\n\n\n필리핀 지역만 선택\n\n\nPHL_loans = world_loans.loc[world_loans.country == \"Philippines\"].copy()\nPHL_loans.head()\n\n\n\n\n\n  \n    \n      \n      Partner ID\n      Field Part\n      sector\n      Loan Theme\n      country\n      amount\n      geometry\n    \n  \n  \n    \n      2859\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      400\n      POINT (121.73961 17.64228)\n    \n    \n      2860\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      400\n      POINT (121.74169 17.63235)\n    \n    \n      2861\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      400\n      POINT (121.46667 16.60000)\n    \n    \n      2862\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      6050\n      POINT (121.73333 17.83333)\n    \n    \n      2863\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      625\n      POINT (121.51800 16.72368)\n    \n  \n\n\n\n\n필리핀의 모든 섬에 대한 경계 표시\n\n# Load a KML file containing island boundaries\ngpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\nPHL = gpd.read_file(\"D:/kimdablog/archive (1)/Philippines_AL258.kml\", driver='KML')\nPHL.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      geometry\n    \n  \n  \n    \n      0\n      Autonomous Region in Muslim Mindanao\n      \n      MULTIPOLYGON (((119.46690 4.58718, 119.46653 4...\n    \n    \n      1\n      Bicol Region\n      \n      MULTIPOLYGON (((124.04577 11.57862, 124.04594 ...\n    \n    \n      2\n      Cagayan Valley\n      \n      MULTIPOLYGON (((122.51581 17.04436, 122.51568 ...\n    \n    \n      3\n      Calabarzon\n      \n      MULTIPOLYGON (((120.49202 14.05403, 120.49201 ...\n    \n    \n      4\n      Caraga\n      \n      MULTIPOLYGON (((126.45401 8.24400, 126.45407 8...\n    \n  \n\n\n\n\n\nax = PHL.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)\n\nPHL_loans.plot(color=\"green\",ax=ax,markersize=2)\n\n<Axes: >"
  },
  {
    "objectID": "Exercise/2. Coordinate Reference Systems.html",
    "href": "Exercise/2. Coordinate Reference Systems.html",
    "title": "2. Coordinate Reference Systems",
    "section": "",
    "text": "Introduction\nYou are a bird conservation expert and want to understand migration patterns of purple martins. In your research, you discover that these birds typically spend the summer breeding season in the eastern United States, and then migrate to South America for the winter. But since this bird is under threat of endangerment, you’d like to take a closer look at the locations that these birds are more likely to visit.\n\n\n\nThere are several protected areas in South America, which operate under special regulations to ensure that species that migrate (or live) there have the best opportunity to thrive. You’d like to know if purple martins tend to visit these areas. To answer this question, you’ll use some recently collected data that tracks the year-round location of eleven different birds.\nBefore you get started, run the code cell below to set everything up.\n\nimport pandas as pd\nimport geopandas as gpd\n\nfrom shapely.geometry import LineString\n\n#from learntools.core import binder\n#binder.bind(globals())\n#from learntools.geospatial.ex2 import *\n\n\n\nExercises\n\n1) Load the data.\nRun the next code cell (without changes) to load the GPS data into a pandas DataFrame birds_df.\n\n# Load the data and print the first 5 rows\nbirds_df = pd.read_csv(\"D:/archive (1)/purple_martin.csv\", parse_dates=['timestamp'])\nprint(\"There are {} different birds in the dataset.\".format(birds_df[\"tag-local-identifier\"].nunique()))\nbirds_df.head()\n\nThere are 11 different birds in the dataset.\n\n\n\n\n\n\n  \n    \n      \n      timestamp\n      location-long\n      location-lat\n      tag-local-identifier\n    \n  \n  \n    \n      0\n      2014-08-15 05:56:00\n      -88.146014\n      17.513049\n      30448\n    \n    \n      1\n      2014-09-01 05:59:00\n      -85.243501\n      13.095782\n      30448\n    \n    \n      2\n      2014-10-30 23:58:00\n      -62.906089\n      -7.852436\n      30448\n    \n    \n      3\n      2014-11-15 04:59:00\n      -61.776826\n      -11.723898\n      30448\n    \n    \n      4\n      2014-11-30 09:59:00\n      -61.241538\n      -11.612237\n      30448\n    \n  \n\n\n\n\n\n# Your code here: Create the GeoDataFrame\nbirds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df[\"location-long\"], birds_df[\"location-lat\"]))\n\n# Your code here: Set the CRS to {'init': 'epsg:4326'}\nbirds.crs = {'init': 'epsg:4326'}\n\n# Check your answer\nbirds.head()\n\nc:\\Users\\jiyeo\\anaconda3\\envs\\dayeon\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      timestamp\n      location-long\n      location-lat\n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      2014-08-15 05:56:00\n      -88.146014\n      17.513049\n      30448\n      POINT (-88.14601 17.51305)\n    \n    \n      1\n      2014-09-01 05:59:00\n      -85.243501\n      13.095782\n      30448\n      POINT (-85.24350 13.09578)\n    \n    \n      2\n      2014-10-30 23:58:00\n      -62.906089\n      -7.852436\n      30448\n      POINT (-62.90609 -7.85244)\n    \n    \n      3\n      2014-11-15 04:59:00\n      -61.776826\n      -11.723898\n      30448\n      POINT (-61.77683 -11.72390)\n    \n    \n      4\n      2014-11-30 09:59:00\n      -61.241538\n      -11.612237\n      30448\n      POINT (-61.24154 -11.61224)\n    \n  \n\n\n\n\n\n\n\n데이터 시각화 하기\n\n# Load a GeoDataFrame with country boundaries in North/South America, print the first 5 rows\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\namericas = world.loc[world['continent'].isin(['North America', 'South America'])]\namericas.head()\n\nC:\\Users\\jiyeo\\AppData\\Local\\Temp\\ipykernel_7052\\2146633488.py:2: FutureWarning: The geopandas.dataset module is deprecated and will be removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.\n  world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n    \n      4\n      328239523.0\n      North America\n      United States of America\n      USA\n      21433226\n      MULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n    \n    \n      9\n      44938712.0\n      South America\n      Argentina\n      ARG\n      445445\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.25000...\n    \n    \n      10\n      18952038.0\n      South America\n      Chile\n      CHL\n      282318\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.63335...\n    \n    \n      16\n      11263077.0\n      North America\n      Haiti\n      HTI\n      14332\n      POLYGON ((-71.71236 19.71446, -71.62487 19.169...\n    \n  \n\n\n\n\n\n# Create a map\nax = americas.plot(figsize=(10, 10), color='whitesmoke', linestyle=':', edgecolor='black')\nbirds.plot(ax = ax, markersize=5)\n\n<Axes: >\n\n\n\n\n\n\n\n각 새의 여행 시작점과 끝점 알아내기 (part 1)\n\nWhere does each bird start and end its journey? (Part 1)?\n\n\n# GeoDataFrame showing path for each bird\n#각 새의 경로를 표시함 \npath_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: LineString(x)).reset_index()\npath_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry)\npath_gdf.crs = {'init' :'epsg:4326'}\npath_gdf.head()\n\nc:\\Users\\jiyeo\\anaconda3\\envs\\dayeon\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      30048\n      LINESTRING (-90.12992 20.73242, -56.29394 -10....\n    \n    \n      1\n      30054\n      LINESTRING (-93.60861 46.50563, -63.28897 -5.7...\n    \n    \n      2\n      30198\n      LINESTRING (-80.31036 25.92545, -88.14608 17.5...\n    \n    \n      3\n      30263\n      LINESTRING (-76.78146 42.99209, -62.90648 -7.8...\n    \n    \n      4\n      30275\n      LINESTRING (-76.78213 42.99207, -60.26131 -15....\n    \n  \n\n\n\n\n\n# GeoDataFrame showing starting point for each bird\n# 각 새의 시작점 표시\nstart_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[0]).reset_index()\nstart_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry)\nstart_gdf.crs = {'init' :'epsg:4326'}\n\n# Show first five rows of GeoDataFrame\nstart_gdf.head()\n\nc:\\Users\\jiyeo\\anaconda3\\envs\\dayeon\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      30048\n      POINT (-90.12992 20.73242)\n    \n    \n      1\n      30054\n      POINT (-93.60861 46.50563)\n    \n    \n      2\n      30198\n      POINT (-80.31036 25.92545)\n    \n    \n      3\n      30263\n      POINT (-76.78146 42.99209)\n    \n    \n      4\n      30275\n      POINT (-76.78213 42.99207)\n    \n  \n\n\n\n\n\n\n새의 끝점을 data frame으로 만드는 셀 생성\n형식은 두개의 열 (“tag-local-identifier” 및 “geometry”)이 있어 start_gdf(시작점)과 형식이 동일해야함 /geometry 열에 point 객체가 포함되야함\nend_gdf(끝점)의 crs는 init : crs: 4326으로 설정합니다.\n\nend_gdf = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[-1]).reset_index()\nend_gdf = gpd.GeoDataFrame(end_gdf, geometry=end_gdf.geometry)\nend_gdf.crs = {'init' :'epsg:4326'}\n\n# Show first five rows of GeoDataFrame\nstart_gdf.head()\n\nc:\\Users\\jiyeo\\anaconda3\\envs\\dayeon\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      30048\n      POINT (-90.12992 20.73242)\n    \n    \n      1\n      30054\n      POINT (-93.60861 46.50563)\n    \n    \n      2\n      30198\n      POINT (-80.31036 25.92545)\n    \n    \n      3\n      30263\n      POINT (-76.78146 42.99209)\n    \n    \n      4\n      30275\n      POINT (-76.78213 42.99207)\n    \n  \n\n\n\n\n\n\n각 새는 어디에서 여행을 시작하고 끝납니까?\n\nWhere does each bird start and end its journey? (Part 2)?\n\n\npath_gdf,start_gdf & end_gdf 를 사용하여 모든 새의 경로를 시각화 해라 미국 GeoDataFame을 사용할수 있음\n\n\nax = americas.plot(figsize=(10, 10), color='whitesmoke', linestyle=':', edgecolor='black')\npath_gdf.plot(ax = ax, markersize=5,color = 'lightblue')\nstart_gdf.plot(ax = ax, markersize=5,color = 'red')\nend_gdf.plot(ax = ax, markersize=5,color=\"darkgreen\")\n\n<Axes: >\n\n\n\n\n\n\n\n남미의 보호지역은 어딘가? (part 1)\n새들은 남미 어딘가에 도착하는데 그 새들은 보호지역으로 가고있나요?\n\n남아메리카의 모든 모호지역 위치를 (GeoDataFrame)protected_areas를 생성해라\n\n\n# Path of the shapefile to load\nprotected_areas= gpd.read_file(\"D:/archive (1)/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp\")\n\nprotected_areas.head()\n\n\n\n\n\n  \n    \n      \n      WDPAID\n      WDPA_PID\n      PA_DEF\n      NAME\n      ORIG_NAME\n      DESIG\n      DESIG_ENG\n      DESIG_TYPE\n      IUCN_CAT\n      INT_CRIT\n      ...\n      GOV_TYPE\n      OWN_TYPE\n      MANG_AUTH\n      MANG_PLAN\n      VERIF\n      METADATAID\n      SUB_LOC\n      PARENT_ISO\n      ISO3\n      geometry\n    \n  \n  \n    \n      0\n      14067.0\n      14067\n      1\n      Het Spaans Lagoen\n      Het Spaans Lagoen\n      Ramsar Site, Wetland of International Importance\n      Ramsar Site, Wetland of International Importance\n      International\n      Not Reported\n      Not Reported\n      ...\n      Not Reported\n      Not Reported\n      Not Reported\n      Management plan is not implemented and not ava...\n      State Verified\n      1856\n      Not Reported\n      NLD\n      ABW\n      POLYGON ((-69.97523 12.47379, -69.97523 12.473...\n    \n    \n      1\n      14003.0\n      14003\n      1\n      Bubali Pond Bird Sanctuary\n      Bubali Pond Bird Sanctuary\n      Bird Sanctuary\n      Bird Sanctuary\n      National\n      Not Reported\n      Not Applicable\n      ...\n      Not Reported\n      Not Reported\n      Not Reported\n      Not Reported\n      State Verified\n      1899\n      Not Reported\n      NLD\n      ABW\n      POLYGON ((-70.04734 12.56329, -70.04615 12.563...\n    \n    \n      2\n      555624439.0\n      555624439\n      1\n      Arikok National Park\n      Arikok National Park\n      National Park\n      National Park\n      National\n      Not Reported\n      Not Applicable\n      ...\n      Non-profit organisations\n      Non-profit organisations\n      Fundacion Parke Nacional Arikok\n      Not Reported\n      State Verified\n      1899\n      Not Reported\n      NLD\n      ABW\n      MULTIPOLYGON (((-69.96302 12.48384, -69.96295 ...\n    \n    \n      3\n      303894.0\n      303894\n      1\n      Madidi\n      Madidi\n      Area Natural de Manejo Integrado\n      Natural Integrated Management Area\n      National\n      Not Reported\n      Not Applicable\n      ...\n      Federal or national ministry or agency\n      Not Reported\n      Not Reported\n      Not Reported\n      State Verified\n      1860\n      BO-L\n      BOL\n      BOL\n      POLYGON ((-68.59060 -14.43388, -68.59062 -14.4...\n    \n    \n      4\n      303893.0\n      303893\n      1\n      Apolobamba\n      Apolobamba\n      Area Natural de Manejo Integado Nacional\n      National Natural Integrated Management Area\n      National\n      Not Reported\n      Not Applicable\n      ...\n      Federal or national ministry or agency\n      Not Reported\n      Not Reported\n      Not Reported\n      State Verified\n      1860\n      BO-L\n      BOL\n      BOL\n      POLYGON ((-69.20949 -14.73334, -69.20130 -14.7...\n    \n  \n\n5 rows × 29 columns\n\n\n\n\n\n남비의 보호지역은 ? (part 2)\n\n남아메리카의 보호지역 위치 표시하기\n(GeoDataFame)protected_areas 사용해 plot그리기\n일부 보호 지역은 육지에 있고 다른 지역은 해수에 있음을 알수 있다.\n\n\nsouth_america = americas.loc[americas['continent']=='South America']\n\n\nax = south_america.plot(figsize=(10, 10), color='whitesmoke', linestyle=':', edgecolor='black')\nprotected_areas.plot(ax = ax, markersize=5,color = 'lightblue')\n\n<Axes: >\n\n\n\n\n\n\n\n남아메리카의 몇 퍼센트가 보호됩니까?\n\n\n남미의 모든 보호 토지(해양 지역 제외) 총면적 계산하기\n\n\n“REP_AREA” 및 “REP_M_AREA” 열사용\n\nP_Area = sum(protected_areas['REP_AREA']-protected_areas['REP_M_AREA'])\nprint(\"South America has {} square kilometers of protected areas.\".format(P_Area))\n\nsouth_america.head()\n\nSouth America has 5396761.9116883585 square kilometers of protected areas.\n\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      9\n      44938712.0\n      South America\n      Argentina\n      ARG\n      445445\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.25000...\n    \n    \n      10\n      18952038.0\n      South America\n      Chile\n      CHL\n      282318\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.63335...\n    \n    \n      20\n      3398.0\n      South America\n      Falkland Is.\n      FLK\n      282\n      POLYGON ((-61.20000 -51.85000, -60.00000 -51.2...\n    \n    \n      28\n      3461734.0\n      South America\n      Uruguay\n      URY\n      56045\n      POLYGON ((-57.62513 -30.21629, -56.97603 -30.1...\n    \n    \n      29\n      211049527.0\n      South America\n      Brazil\n      BRA\n      1839758\n      POLYGON ((-53.37366 -33.76838, -53.65054 -33.2...\n    \n  \n\n\n\n\n각 폴리곤의 면적 속성 (crs로 epsg 3035사용) 국가의 면적 계산 후 결과 합치기\n” 답을 평방 킬로미터 단위로 변경”\n\ntotalArea = sum(south_america.geometry.to_crs(epsg=3035).area) / 10**6\n\n\n# 보호되는 남미의 비율 계산하기\npercentage_protected = P_Area/totalArea\nprint('Approximately {}% of South America is protected.'.format(round(percentage_protected*100, 2)))\n\nApproximately 30.39% of South America is protected.\n\n\n\n\n8. 남아메리카의 새들은 어디에 있나?\n-> 보호구역에 새들이 있나요?\n\n남미에 있는 새의 위치를 plot하세요\n(토지구성요소가 없는) 순수해양지역인 “MARINE”열을 사용할수 있습니다. (protected_areas[protected_areas[‘MARINE’]!=‘2’] 의 행만 표시) ’protected_areas` 의 모든행\n\n\nbirds.head()\n\n\n\n\n\n  \n    \n      \n      timestamp\n      location-long\n      location-lat\n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      2014-08-15 05:56:00\n      -88.146014\n      17.513049\n      30448\n      POINT (-88.14601 17.51305)\n    \n    \n      1\n      2014-09-01 05:59:00\n      -85.243501\n      13.095782\n      30448\n      POINT (-85.24350 13.09578)\n    \n    \n      2\n      2014-10-30 23:58:00\n      -62.906089\n      -7.852436\n      30448\n      POINT (-62.90609 -7.85244)\n    \n    \n      3\n      2014-11-15 04:59:00\n      -61.776826\n      -11.723898\n      30448\n      POINT (-61.77683 -11.72390)\n    \n    \n      4\n      2014-11-30 09:59:00\n      -61.241538\n      -11.612237\n      30448\n      POINT (-61.24154 -11.61224)\n    \n  \n\n\n\n\n\nax = south_america.plot(figsize=(10, 10), color='whitesmoke', linestyle=':', edgecolor='black')\n\n\nprotected_areas[protected_areas['MARINE']!='2'].plot(ax = ax, markersize=5,color = 'lightblue')\nbirds[birds.geometry.y < 0].plot(ax = ax, markersize=5,color=\"darkgreen\")\n\n<Axes: >"
  },
  {
    "objectID": "Exercise.html",
    "href": "Exercise.html",
    "title": "Exercise",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n:::{.list .quarto-listing-default}\n\n\n  \n\n\n\n\n\nYour First Map Exercise\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\n\nCoordinate Reference Systems\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\n\nInteractive Maps\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\n\nManipulating Geospatial Data\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\n\nProximity Analysis\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n:::\n\n\n\n\n\n\nNo matching items\n\n:::\n:::"
  },
  {
    "objectID": "공간정보/2장/index.html",
    "href": "공간정보/2장/index.html",
    "title": "2장",
    "section": "",
    "text": "< 1. 지도 겹쳐 그리기/ 중심 원 / 특정 나라 확장\n필요 패키지 로드\nlibrary(dplyr) library(sf) library(raster) library(spData) library(spDataLarge)\n\n지도 겹쳐 그리기\nst_union : 하나의 좌표로 합침 reset = FALSE : 지도 위 다른 요소를 추가할 수 있음(모든 플롯 유지) add = TRUE : 지도 위 다른 지도 겹처 그리기\n\nlibrary(dplyr)\n\nWarning: 패키지 'dplyr'는 R 버전 4.2.3에서 작성되었습니다\n\n\n\n다음의 패키지를 부착합니다: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(sf)\n\nWarning: 패키지 'sf'는 R 버전 4.2.3에서 작성되었습니다\n\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(raster)\n\n필요한 패키지를 로딩중입니다: sp\n\n\n\n다음의 패키지를 부착합니다: 'raster'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(spData)\n\nWarning: 패키지 'spData'는 R 버전 4.2.3에서 작성되었습니다\n\nlibrary(spDataLarge)\n\nworld_asia = world[world$continent == \"Asia\", ]\nasia = st_union(world_asia) #아시아 국가 합치기\n\n#아시아만 빨간색으로 표시\nplot(world[\"pop\"], reset = FALSE) #reset = FLASE이면 지도 요소를 더 추가할 수 있는 모드로 플롯을 유지\nplot(asia, add = TRUE, col = \"red\")\n\n\n\n\n\n\n중심 원 씌우기\nst_centroid() ; 폴리곤의 중심점을 계산하는 함수 of_largest = TRUE : 전체 폴리곤의 중앙 cex : 원 크기\n\nplot(world[\"continent\"], reset = FALSE)\ncex = sqrt(world$pop) / 10000 #pop변수에 제곱근을 취하고 1000으로 나누어서 지도 시각화를 위해 크기를 맞춤\nworld_cents = st_centroid(world, of_largest = TRUE) #다각형(국가별) 중앙점 계산\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nplot(st_geometry(world_cents), add = TRUE, cex = cex) #인구크기에 따라 대륙별 중앙점에 원그려넣기\n\n\n\n\n\n\n특정 나라 중심 확장\nexpandBB : 각 방향으로 경계 확장 lwd: 선굵기 world_assia[0] : 아시아에 대한 표시 0은 색 표시하겠다\n\nindia = world[world$name_long == \"India\", ]\nplot(st_geometry(india), expandBB = c(0, 0.2, 0.1, 1), col = \"gray\", lwd = 3)\nplot(world_asia[0], add = TRUE)\n\n\n\n\n\n\nsf class : 위치 + 속성\nst_sf() : 위치와 속성 데이터를 통합하는 함수\n\nlnd_point = st_point(c(0.1, 51.5))  #런던 좌표\nlnd_geom = st_sfc(lnd_point, crs = 4326) #좌표1,2  # crs = 4326 : 좌표계 정보 추가\nlnd_attrib = data.frame(                          \n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2017-06-21\")\n) #속성정보, dat.frame으로 만듬\nlnd_sf = st_sf(lnd_attrib, geometry = lnd_geom)  #st_sf : 위치,속성 정보를 합침\n\n\n\nRaster data\nres : 해상도 vals : 값 입력 ex> vals=1:64 > 1~64까지 입력해라\n\n\nRasteLayer class 래스터 객체중에서 가장 간단한 형태의 클래스, 한개의 층으로 구성되어있음\n\n\n\nmy_raster = raster(nrows=8,ncols = 8,res = 0.5,xmn=-2.0,xmx=2.0,ymn = -2.0,ymx=2.0,vals = 1:64)\nmy_raster\n\nclass      : RasterLayer \ndimensions : 8, 8, 64  (nrow, ncol, ncell)\nresolution : 0.5, 0.5  (x, y)\nextent     : -2, 2, -2, 2  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : layer \nvalues     : 1, 64  (min, max)\n\nplot(my_raster,main=\"my raster (64 cells = 8 rows * 8 cols)\")\n\n\n\n\n\n\nRasterBrick class\n\n\n\nmulti_raster_file = system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nr_brick = brick(multi_raster_file)\n\nr_brick\n\nclass      : RasterBrick \ndimensions : 1428, 1128, 1610784, 4  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=12 +datum=WGS84 +units=m +no_defs \nsource     : landsat.tif \nnames      : landsat_1, landsat_2, landsat_3, landsat_4 \nmin values :      7550,      6404,      5678,      5252 \nmax values :     19071,     22051,     25780,     31961 \n\nnlayers(r_brick)\n\n[1] 4\n\nplot(r_brick) #plotting RasterBrick object with 4 layers"
  },
  {
    "objectID": "공간정보.html",
    "href": "공간정보.html",
    "title": "공간정보",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n2장\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\n3장\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "Data",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n:::{.list .quarto-listing-default}\n\n\n  \n\n\n\n\n\nYour First Map\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\n\nCoordinate Reference Systems\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\n\nCoordinate Reference Systems\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\n\nManipulating Geospatial Data\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\n5.Proximity Analysis\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\n  \n\n\n\n\n\npydeck-example\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n:::\n\n\n\n\n\n\nNo matching items\n\n:::\n:::"
  },
  {
    "objectID": "Data/1. Your First Map.html",
    "href": "Data/1. Your First Map.html",
    "title": "1. Your First Map",
    "section": "",
    "text": "#파일 위치 확인\nimport os\nos.getcwd()\n\n'd:\\\\kimdayeon\\\\Data'\n\n\n\nIntroduction\nIn this micro-course, you’ll learn about different methods to wrangle and visualize geospatial data, or data with a geographic location\n\n\n\nimage.png\n\n\n\n\nReading data\nThe first step is to read in some geospatial data! To do this, we’ll use the GeoPandas library.\n\nimport geopandas as gpd\n\n\n# Read in the data\nfull_data = gpd.read_file(\"D:/archive (1)/DEC_lands/DEC_lands/DEC_lands.shp\")\n\n# 데이터 확인\nfull_data.head()\n\n\n\n\n\n  \n    \n      \n      OBJECTID\n      CATEGORY\n      UNIT\n      FACILITY\n      CLASS\n      UMP\n      DESCRIPTIO\n      REGION\n      COUNTY\n      URL\n      SOURCE\n      UPDATE_\n      OFFICE\n      ACRES\n      LANDS_UID\n      GREENCERT\n      SHAPE_AREA\n      SHAPE_LEN\n      geometry\n    \n  \n  \n    \n      0\n      1\n      FOR PRES DET PAR\n      CFP\n      HANCOCK FP DETACHED PARCEL\n      WILD FOREST\n      None\n      DELAWARE COUNTY DETACHED PARCEL\n      4\n      DELAWARE\n      http://www.dec.ny.gov/\n      DELAWARE RPP\n      5/12\n      STAMFORD\n      738.620192\n      103\n      N\n      2.990365e+06\n      7927.662385\n      POLYGON ((486093.245 4635308.586, 486787.235 4...\n    \n    \n      1\n      2\n      FOR PRES DET PAR\n      CFP\n      HANCOCK FP DETACHED PARCEL\n      WILD FOREST\n      None\n      DELAWARE COUNTY DETACHED PARCEL\n      4\n      DELAWARE\n      http://www.dec.ny.gov/\n      DELAWARE RPP\n      5/12\n      STAMFORD\n      282.553140\n      1218\n      N\n      1.143940e+06\n      4776.375600\n      POLYGON ((491931.514 4637416.256, 491305.424 4...\n    \n    \n      2\n      3\n      FOR PRES DET PAR\n      CFP\n      HANCOCK FP DETACHED PARCEL\n      WILD FOREST\n      None\n      DELAWARE COUNTY DETACHED PARCEL\n      4\n      DELAWARE\n      http://www.dec.ny.gov/\n      DELAWARE RPP\n      5/12\n      STAMFORD\n      234.291262\n      1780\n      N\n      9.485476e+05\n      5783.070364\n      POLYGON ((486000.287 4635834.453, 485007.550 4...\n    \n    \n      3\n      4\n      FOR PRES DET PAR\n      CFP\n      GREENE COUNTY FP DETACHED PARCEL\n      WILD FOREST\n      None\n      None\n      4\n      GREENE\n      http://www.dec.ny.gov/\n      GREENE RPP\n      5/12\n      STAMFORD\n      450.106464\n      2060\n      N\n      1.822293e+06\n      7021.644833\n      POLYGON ((541716.775 4675243.268, 541217.579 4...\n    \n    \n      4\n      6\n      FOREST PRESERVE\n      AFP\n      SARANAC LAKES WILD FOREST\n      WILD FOREST\n      SARANAC LAKES\n      None\n      5\n      ESSEX\n      http://www.dec.ny.gov/lands/22593.html\n      DECRP, ESSEX RPP\n      12/96\n      RAY BROOK\n      69.702387\n      1517\n      N\n      2.821959e+05\n      2663.909932\n      POLYGON ((583896.043 4909643.187, 583891.200 4...\n    \n  \n\n\n\n\n\n\nPrerequisites\n\ntype(full_data) #공간 정보를 담고 있기에 GeoDataFram\n\ngeopandas.geodataframe.GeoDataFrame\n\n\nloc활용으로 3개만 가지고 오기\n\ndata = full_data.loc[:, [\"CLASS\", \"COUNTY\", \"geometry\"]].copy()\n\n\n# How many lands of each type are there?\ndata.CLASS.value_counts()\n\nWILD FOREST                   965\nINTENSIVE USE                 108\nPRIMITIVE                      60\nWILDERNESS                     52\nADMINISTRATIVE                 17\nUNCLASSIFIED                    7\nHISTORIC                        5\nPRIMITIVE BICYCLE CORRIDOR      4\nCANOE AREA                      1\nName: CLASS, dtype: int64\n\n\n\n# Select lands that fall under the \"WILD FOREST\" or \"WILDERNESS\" category\nwild_lands = data.loc[data.CLASS.isin(['WILD FOREST', 'WILDERNESS'])].copy()\nwild_lands.head()  #class.isin 풀어서 가지고 오기\n\n\n\n\n\n  \n    \n      \n      CLASS\n      COUNTY\n      geometry\n    \n  \n  \n    \n      0\n      WILD FOREST\n      DELAWARE\n      POLYGON ((486093.245 4635308.586, 486787.235 4...\n    \n    \n      1\n      WILD FOREST\n      DELAWARE\n      POLYGON ((491931.514 4637416.256, 491305.424 4...\n    \n    \n      2\n      WILD FOREST\n      DELAWARE\n      POLYGON ((486000.287 4635834.453, 485007.550 4...\n    \n    \n      3\n      WILD FOREST\n      GREENE\n      POLYGON ((541716.775 4675243.268, 541217.579 4...\n    \n    \n      4\n      WILD FOREST\n      ESSEX\n      POLYGON ((583896.043 4909643.187, 583891.200 4...\n    \n  \n\n\n\n\n\n\nCreate your first map!\n\nwild_lands.plot()\n\n<Axes: >\n\n\n\n\n\n\n# View the first five entries in the \"geometry\" column\nwild_lands.geometry.head()\n\n0    POLYGON ((486093.245 4635308.586, 486787.235 4...\n1    POLYGON ((491931.514 4637416.256, 491305.424 4...\n2    POLYGON ((486000.287 4635834.453, 485007.550 4...\n3    POLYGON ((541716.775 4675243.268, 541217.579 4...\n4    POLYGON ((583896.043 4909643.187, 583891.200 4...\nName: geometry, dtype: geometry\n\n\nWhile this column can contain a variety of different datatypes, each entry will typically be a Point, LineString, or Polygon.\n\n\n\nimage.png\n\n\n다른 데이터 가지고 오기\n\n# Campsites in New York state (Point)\nPOI_data = gpd.read_file(\"D:/archive (1)/DEC_pointsinterest/DEC_pointsinterest/Decptsofinterest.shp\")\ncampsites = POI_data.loc[POI_data.ASSET=='PRIMITIVE CAMPSITE'].copy()\n\n# Foot trails in New York state (LineString)\nroads_trails = gpd.read_file(\"D:/archive (1)/DEC_roadstrails/DEC_roadstrails/Decroadstrails.shp\")\ntrails = roads_trails.loc[roads_trails.ASSET=='FOOT TRAIL'].copy()\n\n# County boundaries in New York state (Polygon)\ncounties = gpd.read_file(\"D:/archive (1)/NY_county_boundaries/NY_county_boundaries/NY_county_boundaries.shp\")\n\n\npoint/ LineString/ Polygone인지 확인 가능\n\n\nPOI_data\n\n\n\n\n\n  \n    \n      \n      OBJECTID\n      UNIT\n      FACILITY\n      NAME\n      ASSET\n      DESCRIP\n      REGION\n      OFFICE\n      UPDATED\n      ASSET_UID\n      ACCESSIBLE\n      geometry\n    \n  \n  \n    \n      0\n      91706\n      CFP\n      DELAWARE WILD FOREST\n      ADA PRIMITIVE CAMPSITE\n      PRIMITIVE CAMPSITE\n      None\n      4\n      STAMFORD\n      2016-09-26\n      16519\n      N\n      POINT (505138.696 4649388.247)\n    \n    \n      1\n      108646\n      SCHUYLER 02\n      SUGAR HILL STATE FOREST\n      MAPLE LANE CAMPSITE #3\n      PRIMITIVE CAMPSITE\n      None\n      8\n      BATH\n      2018-02-23\n      18124\n      N\n      POINT (333481.874 4692737.800)\n    \n    \n      2\n      19933\n      AFP\n      MOOSE RIVER PLAINS WILD FOREST\n      MOOSE RIVER PLAINS CAMPSITE 103\n      PRIMITIVE CAMPSITE\n      LIVE\n      5\n      NORTHVILLE\n      2010-06-16\n      1018\n      N\n      POINT (525210.784 4833837.295)\n    \n    \n      3\n      3945\n      ALLEGANY 12\n      LOST NATION STATE FOREST\n      NEWLAND FR PARKING\n      UNPAVED PARKING LOT\n      -99\n      9\n      WEST ALMOND\n      None\n      8623\n      N\n      POINT (231909.125 4712818.500)\n    \n    \n      4\n      1037\n      CATTARAUGUS 08\n      ROCK CITY STATE FOREST\n      LRC PICNIC PAVILION #1\n      PICNIC PAVILION\n      None\n      9\n      ALLEGANY\n      None\n      14561\n      N\n      POINT (193656.734 4679632.500)\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4312\n      6662\n      AFP\n      SARANAC LAKES WILD FOREST\n      RAQUETTE RIVER SITE 3\n      PRIMITIVE CAMPSITE\n      None\n      5\n      RAY BROOK\n      None\n      13671\n      N\n      POINT (549184.220 4898291.000)\n    \n    \n      4313\n      1497\n      AFP\n      BLACK RIVER WILD FOREST\n      MCKEEVER WEST PARKING\n      UNPAVED PARKING LOT\n      None\n      6\n      HERKIMER\n      2012-03-13\n      9733\n      N\n      POINT (492699.219 4828844.500)\n    \n    \n      4314\n      39711\n      AFP\n      SARANAC LAKES WILD FOREST\n      SARANAC ISLANDS CAMPGROUND SITE 65\n      PRIMITIVE CAMPSITE\n      None\n      5\n      RAY BROOK\n      2012-01-09\n      5374\n      N\n      POINT (559949.450 4901744.055)\n    \n    \n      4315\n      6412\n      AFP\n      DEBAR MTN. WILD FOREST\n      DEBAR MOUNTIAN LEAN-TO\n      LEAN-TO\n      None\n      5\n      RAY BROOK\n      None\n      13272\n      N\n      POINT (561221.432 4938687.082)\n    \n    \n      4316\n      19898\n      AFP\n      MOOSE RIVER PLAINS CAMPING AREA\n      MOOSE RIVER PLAINS CAMPSITE 41\n      PRIMITIVE CAMPSITE\n      LIVE\n      5\n      NORTHVILLE\n      2010-06-16\n      852\n      N\n      POINT (527492.411 4836534.303)\n    \n  \n\n4317 rows × 12 columns\n\n\n\n\n# Define a base map with county boundaries\n#x축 /사이즈. 색지정\nax = counties.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)\n\n# Add wild lands, campsites, and foot trails to the base map\nwild_lands.plot(color='lightgreen', ax=ax)\ncampsites.plot(color='maroon', markersize=2, ax=ax)\ntrails.plot(color='black', markersize=1, ax=ax)\n\n<Axes: >"
  },
  {
    "objectID": "Datamining/geopandas/0_install_geopandas.html",
    "href": "Datamining/geopandas/0_install_geopandas.html",
    "title": "geopandas 설치법",
    "section": "",
    "text": "https://domdom.tistory.com/599"
  },
  {
    "objectID": "Datamining/geopandas/0_install_geopandas.html#기본-환경",
    "href": "Datamining/geopandas/0_install_geopandas.html#기본-환경",
    "title": "geopandas 설치법",
    "section": "기본 환경",
    "text": "기본 환경\nAnaconda - Python: 3.11.10 Mac\n\nimport sys\nsys.executable\n\n'/Users/jihoyeo/miniforge3/envs/base2/bin/python'\n\n\n\nsys.version\n\n'3.11.0 | packaged by conda-forge | (main, Jan 14 2023, 12:26:40) [Clang 14.0.6 ]'"
  },
  {
    "objectID": "Datamining/geopandas/0_install_geopandas.html#패키지-다운로드",
    "href": "Datamining/geopandas/0_install_geopandas.html#패키지-다운로드",
    "title": "geopandas 설치법",
    "section": "패키지 다운로드",
    "text": "패키지 다운로드\n더 아래의 pip 설치 구문에 있는 패키지별 버전을 참조하여 아래의 사이트에서 각 패키지 설치 파일(.whl)을 다운로드 받습니다.\nhttps://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely\nhttps://www.lfd.uci.edu/~gohlke/pythonlibs/#gdal\nhttps://www.lfd.uci.edu/~gohlke/pythonlibs/#fiona\nhttps://pypi.org/project/geopandas/#files"
  },
  {
    "objectID": "Datamining/geopandas/0_install_geopandas.html#설치-윈도우",
    "href": "Datamining/geopandas/0_install_geopandas.html#설치-윈도우",
    "title": "geopandas 설치법",
    "section": "설치 (윈도우)",
    "text": "설치 (윈도우)\n아나콘다 프롬프트를 관리자 권한으로 실행합니다.\nVSCode로 진행하시는 분은 VSCode 실행을 관리자권한으로 해야 합니다.\n현재 경로를 확인하고 이를 감안하여 적정 경로에 설치 파일을 이동해놓습니다.\n저는 강의자료가 있는 곳에 geopandas를 폴더를 만들고 이 폴더 안에 설치 파일들을 옮겨놓음\n아래의 설치/업데이트 구문을 한줄씩 실행합니다.\n먼저, pip와 numpy를 업그레이드 합니다.\n!python -m pip install – upgrade pip !pip install – upgrade numpy\n그 다음 아래 패키지를 순차적으로 설치합니다\n!pip install pyproj !pip install geopandas/Shapely-1.8.2-cp38-cp38-win_amd64.whl !pip install geopandas/GDAL-3.4.3-cp38-cp38-win_amd64.whl !pip install geopandas/Fiona-1.8.21-cp38-cp38-win_amd64.whl !pip install geopandas/geopandas-0.10.2-py2.py3-none-any.whl"
  },
  {
    "objectID": "Datamining/geopandas/0_install_geopandas.html#설치-맥북",
    "href": "Datamining/geopandas/0_install_geopandas.html#설치-맥북",
    "title": "geopandas 설치법",
    "section": "설치 (맥북)",
    "text": "설치 (맥북)\n!pip install pyproj\n!pip install geopy\n!pip install geopandas\n실행이 잘 되는지 확인\n\nimport geopandas as gpd"
  },
  {
    "objectID": "Datamining/loops/loop_speed.html",
    "href": "Datamining/loops/loop_speed.html",
    "title": "For loop 속도 개선하기",
    "section": "",
    "text": "강의자료 출처 - https://blog.fearcat.in/a?ID=00900-6997c6fb-2680-4531-af1d-73eeccce74ef - https://aldente0630.github.io/data-science/2018/08/05/a-beginners-guide-to-optimizing-pandas-code-for-speed.html\n\nimport pandas as pd\nimport numpy as np\nfrom math import *\n\n\n\n익스피디아 개발자 사이트에서 제공한 뉴욕 주 내 모든 호텔 좌표가 들어있는 데이터셋\n\ndf = pd.read_csv('new_york_hotels.csv', encoding='cp1252')\n\n\ndf.shape\n\n(1631, 11)\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ean_hotel_id\n      name\n      address1\n      city\n      state_province\n      postal_code\n      latitude\n      longitude\n      star_rating\n      high_rate\n      low_rate\n    \n  \n  \n    \n      0\n      269955\n      Hilton Garden Inn Albany/SUNY Area\n      1389 Washington Ave\n      Albany\n      NY\n      12206\n      42.68751\n      -73.81643\n      3.0\n      154.0272\n      124.0216\n    \n    \n      1\n      113431\n      Courtyard by Marriott Albany Thruway\n      1455 Washington Avenue\n      Albany\n      NY\n      12206\n      42.68971\n      -73.82021\n      3.0\n      179.0100\n      134.0000\n    \n    \n      2\n      108151\n      Radisson Hotel Albany\n      205 Wolf Rd\n      Albany\n      NY\n      12205\n      42.72410\n      -73.79822\n      3.0\n      134.1700\n      84.1600\n    \n    \n      3\n      254756\n      Hilton Garden Inn Albany Medical Center\n      62 New Scotland Ave\n      Albany\n      NY\n      12208\n      42.65157\n      -73.77638\n      3.0\n      308.2807\n      228.4597\n    \n    \n      4\n      198232\n      CrestHill Suites SUNY University Albany\n      1415 Washington Avenue\n      Albany\n      NY\n      12206\n      42.68873\n      -73.81854\n      3.0\n      169.3900\n      89.3900\n    \n  \n\n\n\n\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      ean_hotel_id\n      latitude\n      longitude\n      star_rating\n      high_rate\n      low_rate\n    \n  \n  \n    \n      count\n      1631.000000\n      1631.000000\n      1631.000000\n      1630.000000\n      1631.000000\n      1631.000000\n    \n    \n      mean\n      302845.515021\n      41.851026\n      -75.015019\n      2.894785\n      273.268624\n      169.408866\n    \n    \n      std\n      163497.215910\n      1.131960\n      1.774482\n      0.777486\n      504.191880\n      205.914287\n    \n    \n      min\n      6295.000000\n      40.583990\n      -79.742010\n      1.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      163765.500000\n      40.755540\n      -76.142530\n      2.500000\n      121.400000\n      97.318000\n    \n    \n      50%\n      252457.000000\n      41.558420\n      -73.988710\n      3.000000\n      170.000000\n      134.370000\n    \n    \n      75%\n      437138.000000\n      42.949455\n      -73.905340\n      3.500000\n      279.930000\n      195.260000\n    \n    \n      max\n      685047.000000\n      44.967850\n      -71.933340\n      5.000000\n      10888.500000\n      5990.250000"
  },
  {
    "objectID": "Datamining/loops/loop_speed.html#haversine-definition",
    "href": "Datamining/loops/loop_speed.html#haversine-definition",
    "title": "For loop 속도 개선하기",
    "section": "Haversine definition",
    "text": "Haversine definition\n두 위치 사이의 거리를 계산하는 함수 - https://stricky.tistory.com/284\n\ndef haversine(lat1, lon1, lat2, lon2):\n    miles_constant = 3959\n    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1 \n    dlon = lon2 - lon1 \n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n    c = 2 * np.arcsin(np.sqrt(a)) \n    mi = miles_constant * c\n    return mi"
  },
  {
    "objectID": "Datamining/loops/loop_speed.html#task",
    "href": "Datamining/loops/loop_speed.html#task",
    "title": "For loop 속도 개선하기",
    "section": "Task",
    "text": "Task\n어떤 위치, (40.671, -73.985)에서 df에 존재하는 모든 호텔까지의 거리를 구해봅시다"
  },
  {
    "objectID": "Datamining/loops/loop_speed.html#looping-haversine",
    "href": "Datamining/loops/loop_speed.html#looping-haversine",
    "title": "For loop 속도 개선하기",
    "section": "Looping Haversine",
    "text": "Looping Haversine\n\ndef haversine_looping(df):\n    distance_list = [] # 빈 리스트를 생성\n    for i in range(0, len(df)):   #dataframe에서 값을 하나씩 뽑아서 \n        d = haversine(40.671, -73.985, df.iloc[i]['latitude'], df.iloc[i]['longitude'])\n        distance_list.append(d)\n    return distance_list\n\n%%timeit은 Jupyter Notebook에서 사용되는 매직 명령어 중 하나로, 코드 실행 시간을 측정하는 도구입니다.\n%%timeit 매직 명령어를 사용하면 해당 셀의 코드를 여러 번 실행하여 실행 시간을 평균적으로 계산합니다. 이를 통해 코드의 실행 성능을 쉽게 측정하고 비교할 수 있습니다.\n\n%%timeit\n\n# Haversine 반복 함수 실행하기\ndf['distance'] = haversine_looping(df)\n\n446 ms ± 121 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\ndf['distance'].describe()\n\ncount    1631.000000\nmean      111.318922\nstd       107.476086\nmin         0.163480\n25%         6.305530\n50%        71.070425\n75%       199.395866\nmax       314.936306\nName: distance, dtype: float64"
  },
  {
    "objectID": "Datamining/loops/loop_speed.html#iterrows-haversine",
    "href": "Datamining/loops/loop_speed.html#iterrows-haversine",
    "title": "For loop 속도 개선하기",
    "section": "Iterrows Haversine",
    "text": "Iterrows Haversine\n반복문을 돌려야 할 때 iterrows() 메서드를 사용하는 건 행을 반복하기 위한 더 좋은 방법이다. iterrows()는 데이터 프레임의 행을 반복하며 행 자체를 포함하는 객체에 덧붙여 각 행의 색인을 반환하는 제너레이터다. iterrows()는 판다스 데이터 프레임과 함께 작동하게끔 최적화되어 있으며 표준 함수 대부분을 실행하는 데 가장 효율적인 방법은 아니지만(나중에 자세히 설명) 단순 반복보다는 상당히 개선되었다. 예제의 경우 iterrows()는 행을 수동으로 반복하는 것보다 거의 똑같은 문제를 약 4배 빠르게 해결한다.\n\n# Haversine applied on rows via iteration  #그냥 반복 보다는 빠르다 \nhaversine_series = []\nfor index, row in df.iloc[0:10].iterrows():  #iterrows:index랑 row랑 같이 뽑아줌  #row는 데이터의 행이 하나씩 뽑아짐\n    print(row['latitude'])    #index : ex) 0123....\n\n42.68751\n42.68971\n42.7241\n42.65157\n42.68873\n42.72874\n42.68031\n42.65334\n42.72111\n42.67807\n\n\n\n%%timeit\n# Haversine applied on rows via iteration\nhaversine_series = []\nfor index, row in df.iterrows():\n    haversine_series.append(haversine(40.671, -73.985, row['latitude'], row['longitude']))\ndf['distance'] = haversine_series\n\n224 ms ± 40.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nitertuples와 iterrows는 모두 Pandas 데이터프레임의 행을 순회(iterate)하는 메서드입니다. 그러나 itertuples는 iterrows보다 더욱 빠른 속도를 보이므로, 대체로 itertuples를 사용하는 것이 좋습니다.\n이유는 iterrows는 각 행(row)을 Series 객체로 반환하는 반면, itertuples는 각 행을 NamedTuple로 반환합니다. NamedTuple은 각 속성(attribute)에 이름이 지정되어 있기 때문에, Series보다 빠르게 데이터에 접근할 수 있습니다. 따라서 대용량의 데이터프레임을 다룰 때는 itertuples를 사용하는 것이 더욱 효율적입니다.\n\n%%timeit\nhaversine_series = []\nfor idx, lat, lon in df[['latitude','longitude']].itertuples():  #itertples가 일반 루프문 보다는 상당히 빠르다\n    haversine_series.append(haversine(40.671, -73.985, lat, lon))\n    \ndf['distance'] = haversine_series\n\n27.5 ms ± 6.51 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\ndf2\n\nNameError: name 'df2' is not defined\n\n\n\n#예제   # 한 행씩 뽑는것\nimport pandas as pd\n\ndf2 = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'c']})\n\nfor row in df2.itertuples():\n    print(row)\n\nPandas(Index=0, A=1, B='a')\nPandas(Index=1, A=2, B='b')\nPandas(Index=2, A=3, B='c')"
  },
  {
    "objectID": "Datamining/loops/loop_speed.html#apply-haversine-on-rows",
    "href": "Datamining/loops/loop_speed.html#apply-haversine-on-rows",
    "title": "For loop 속도 개선하기",
    "section": "Apply Haversine on rows",
    "text": "Apply Haversine on rows\niterrows()보다 더 좋은 옵션은 데이터 프레임의 특정 축(행 또는 열을 의미)을 따라 함수를 적용하는 apply() 메서드를 사용하는 것이다. apply()는 본질적으로 행을 반복하지만 Cython에서 이터레이터를 사용하는 것 같이 내부 최적화를 다양하게 활용하므로 iterrows()보다 훨씬 효율적이다.\n익명의 람다 함수를 사용하여 Haversine 함수를 각 행에 적용하며 각 행의 특정 셀을 함수 입력값으로 지정할 수 있다. 람다 함수는 판다스가 행(축 = 1)과 열(축 = 0) 중 어디에 함수를 적용할지 정할 수 있게 축 매개 변수를 마지막에 포함한다.\n\nTiming “apply”\n\n%%timeit \n\ndf['distance'] =\\\ndf.apply(lambda row: haversine(40.671, -73.985, \\\n    row['latitude'], row['longitude']), axis=1)   #apply: 사용하면 각각의 행이 실행  \\: 줄바꿈이라 무시# axis: 축 0번째는 열 기준 1은 행기준\n\n119 ms ± 24.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "Datamining/loops/loop_speed.html#vectorized-implementation-of-haversine-applied-on-pandas-series",
    "href": "Datamining/loops/loop_speed.html#vectorized-implementation-of-haversine-applied-on-pandas-series",
    "title": "For loop 속도 개선하기",
    "section": "Vectorized implementation of Haversine applied on Pandas series",
    "text": "Vectorized implementation of Haversine applied on Pandas series\n\nTiming vectorized implementation\n함수 수행의 반복량 줄이는 방법을 이해하기 위해 판다스의 기본 단위, 데이터 프레임과 시리즈가 모두 배열 기반임을 알아두자. 기본 단위의 내부 구조는 개별 값(스칼라라고 함)마다 순차적으로 작동하는 대신 전체 배열 위로 작동하도록 설계된 내장 판다스 함수를 위해 변환된다. 벡터화는 전체 배열 위로 작업을 실행하는 프로세스다.\n판다스는 수학 연산에서 집계 및 문자열 함수(사용 가능한 함수의 광범위한 목록은 판다스 문서에서 확인해라)에 이르기까지 다양한 벡터화 함수를 포함하고 있다. 내장 함수는 판다스 시리즈와 데이터 프레임에서 작동하게끔 최적화되어있다. 결과적으로 벡터화 판다스 함수를 사용하는 건 비슷한 목적을 위해 손수 반복시키는 방법보다 거의 항상 바람직하다.\n지금까지는 Haversine 함수에 스칼라를 전달했다. 그러나 Haversine 함수 내에서 사용하는 모든 함수를 배열 위로 작동시킬 수 있다. 이렇게 하면 거리 함수를 매우 간단하게 벡터화할 수 있다. 스칼라 값으로 각 위도, 경도를 전달하는 대신 전체 시리즈(열)를 전달한다. 이를 통해 판다스는 벡터화 함수에 적용 가능한 모든 최적화 옵션을 활용할 수 있고 특히 전체 배열에 대한 모든 계산을 동시에 수행하게 된다.\n\n%%timeit\n# Vectorized implementation of Haversine applied on Pandas series\ndf['distance'] = haversine(40.671, -73.985,\\\n                                   df['latitude'], df['longitude'])\n\n2.77 ms ± 556 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nhaversine(40.671, -73.985,\\\n           df['latitude'], df['longitude'])\n\n0       139.607190\n1       139.746898\n2       142.191050\n3       137.275546\n4       139.684583\n           ...    \n1626     21.551000\n1627     20.583072\n1628     18.914309\n1629     20.785405\n1630     21.588998\nLength: 1631, dtype: float64\n\n\n함수 벡터화를 통해 apply() 메서드 대비 50배 이상 개선시켰고 iterrows() 대비 100배 이상 개선시켰다. 입력 유형 변경하는 것 외에 아무것도 하지 않아도 됐다!"
  },
  {
    "objectID": "Datamining/loops/loop_speed.html#vectorized-implementation-of-haversine-applied-on-numpy-arrays",
    "href": "Datamining/loops/loop_speed.html#vectorized-implementation-of-haversine-applied-on-numpy-arrays",
    "title": "For loop 속도 개선하기",
    "section": "Vectorized implementation of Haversine applied on NumPy arrays",
    "text": "Vectorized implementation of Haversine applied on NumPy arrays\n이 지점에서 그만두어도 괜찮다. 판다스 시리즈를 사용해 벡터화하면 상시 계산을 위한 최적화 요구 사항의 거의 대부분을 만족시킬 수 있다. 그러나 속도가 최우선이라면 넘파이 파이썬 라이브러리 형식에 도움을 요청해볼 수 있다.\n넘파이 라이브러리는 “과학 계산을 위한 파이썬 기본 패키지”를 표방하며 내부가 최적화된, 사전 컴파일된 C 코드로 작업을 수행한다. 판다스와 마찬가지로 넘파이는 배열 객체(ndarrays라고 함) 상에서 작동한다. 그러나 색인, 데이터 유형 확인 등과 같이 판다스 시리즈 작업으로 인한 오버헤드가 많이 발생하지 않는다. 결과적으로 넘파이 배열에 대한 작업은 판다스 시리즈에 대한 작업보다 훨씬 빠르다.\n판다스 시리즈가 제공하는 추가 기능이 중요하지 않을 때 넘파이 배열을 판다스 시리즈 대신 사용할 수 있다. 예를 들어 Haversine 함수의 벡터화 구현은 실제로 위도 또는 경도 시리즈의 색인을 사용하지 않으므로 사용할 수 있는 색인이 없어도 함수가 중단되지 않는다. 이에 비해 색인으로 값을 참조해야 하는 데이터 프레임의 조인 같은 작업을 수행한다면 판다스 개체를 계속 사용하는 편이 낫다.\n위도와 경도 배열을 시리즈의 values 메서드를 단순 사용해서 판다스 시리즈에서 넘파이 배열로 변환한다. 시리즈의 벡터화와 마찬가지로 넘파이 배열을 함수에 직접 전달하면 판다스가 전체 벡터에 함수를 적용시킨다.\n\nTiming vectorized implementation\n\n# Vectorized implementation of Haversine applied on NumPy arrays\n%timeit df['distance'] = haversine(40.671, -73.985,\\\n                         df['latitude'].values, df['longitude'].values)  #판다스를 넘파이로 바꿔 계산 하면 조금 더 빠름\n\n276 µs ± 35.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\n\n%%timeit\n# Convert pandas arrays to NumPy ndarrays\nnp_lat = df['latitude'].values\nnp_lon = df['longitude'].values\n\n12.9 µs ± 2.5 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)"
  },
  {
    "objectID": "Datamining/loops/loop_speed.html#summary",
    "href": "Datamining/loops/loop_speed.html#summary",
    "title": "For loop 속도 개선하기",
    "section": "Summary",
    "text": "Summary\n판다스 코드 최적화에 관해 몇 가지 기본적인 결론을 내릴 수 있다.\n1 . 반복을 피해라. 사용 사례 대부분의 경우 반복은 느리고 불필요하다.\n2 . 반복해야 하는 경우 반복 함수가 아닌 apply()를 사용해라.\n3 . 보통은 벡터화가 스칼라 연산보다 낫다. 대부분의 판다스 작업은 벡터화시킬 수 있다.\n4 . 넘파이 배열에서의 벡터 연산은 판다스 시리즈에서 수행하는 것보다 효율적이다.\n\n실습\n아래의 조건을 만족하는 호텔의 List를 출력해 봅시다.\n\n현재 나는 (“latitude”, “longitude”) = (40.671, -73.985) 위치에 있고, 숙박할 호텔을 찾고 있습니다.\n직선거리 기준으로 200m 안쪽에 있었으면 좋겠습니다.\nstar_rating이 4 이상인 호텔을 찾고 있습니다.\n\n해당 조건을 만족하는 호텔들을 출력해봅시다\n\ndf\n\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n1626    True\n1627    True\n1628    True\n1629    True\n1630    True\nName: distance, Length: 1631, dtype: bool\n\n\n\ndf.describe()\n\ncount     1631\nunique       1\ntop       True\nfreq      1631\nName: distance, dtype: object\n\n\n\ndef haversine_looping(df):\n    distance_list = [] # 빈 리스트를 생성\n    for i in range(0, len(df)):   #dataframe에서 값을 하나씩 뽑아서 \n        d = haversine(40.671, -73.985, df.iloc[i]['latitude'], df.iloc[i]['longitude'])\n        distance_list.append(d)\n    return distance_list\n\n\ndf['distance'] = haversine_looping(df) \n\n\ndf[(df['distance'] < 200) & (df['star_rating'] >= 4)]\n\n\ndf[(df['distance'] < 200) & (df['star_rating'] >= 4)][['name',\"address1\"]]\n\n\n\n\n\n\n  \n    \n      \n      name\n      address1\n    \n  \n  \n    \n      114\n      Topping Rose House\n      One Bridgehampton - Sag Harbor Turnpike\n    \n    \n      129\n      Sheraton Brooklyn New York Hotel\n      228 Duffield Street\n    \n    \n      134\n      McCarren Hotel & Pool\n      160 N 12th St\n    \n    \n      142\n      The Box House Hotel\n      77 Box Street\n    \n    \n      154\n      New York Marriott at the Brooklyn Bridge\n      333 Adams St\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      1592\n      Viana Hotel & Spa, BW Premier Collection\n      3998 Brush Hollow Rd\n    \n    \n      1600\n      The Ritz-Carlton New York, Westchester\n      3 Renaissance Square\n    \n    \n      1601\n      Furnished Quarters Bank Street Commons\n      15/25 Bank Street\n    \n    \n      1602\n      Global Luxury Apartments in White Plains\n      15 Bank Street\n    \n    \n      1603\n      Global Luxury Suites at White Plains\n      27 Barker Avenue\n    \n  \n\n245 rows × 2 columns\n\n\n\n\nlen(df[(df['distance'] < 200) & (df['star_rating'] >= 4)])\n\n245"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "kimdayeon",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "Data/pydeck-example.html",
    "href": "Data/pydeck-example.html",
    "title": "4. pydeck-example",
    "section": "",
    "text": "<실행시키기위한 패키지 설치 및 실행>\n#1. pip install pydeck\n#2. !jupyter nbextension install –sys-prefix –symlink –overwrite –py pydeck #. !jupyter nbextension enable –sys-prefix –py pydeck\n#3. pip install pydeck[jupyter]\n#4. MAPBOX_API_KEY=“pk.eyJ1Ijoic3BlYXI1MzA2IiwiYSI6ImNremN5Z2FrOTI0ZGgycm45Mzh3dDV6OWQifQ.kXGWHPRjnVAEHgVgLzXn2g”\n\n공식 홈페이지 예시\n\nimport pandas as pd\nimport pydeck\n\nUK_ACCIDENTS_DATA = 'https://raw.githubusercontent.com/uber-common/deck.gl-data/master/examples/3d-heatmap/heatmap-data.csv'\n\npd.read_csv(UK_ACCIDENTS_DATA).head()\n\n\n\n\n\n  \n    \n      \n      lng\n      lat\n    \n  \n  \n    \n      0\n      -0.198465\n      51.505538\n    \n    \n      1\n      -0.178838\n      51.491836\n    \n    \n      2\n      -0.205590\n      51.514910\n    \n    \n      3\n      -0.208327\n      51.514952\n    \n    \n      4\n      -0.206022\n      51.496572\n    \n  \n\n\n\n\n\nUK_ACCIDENTS_DATA\n\n'https://raw.githubusercontent.com/uber-common/deck.gl-data/master/examples/3d-heatmap/heatmap-data.csv'\n\n\n\nlayer = pydeck.Layer(\n    'HexagonLayer',\n    UK_ACCIDENTS_DATA,  #데이터 셋\n    get_position='[lng,lat]', #경도 위도를 불러오는 매개변수\n    auto_highlight=True,  #3D 형태로 표시\n    elevation_scale=50, #히트맵 높이 조정\n    pickable=True,\n    elevation_range=[0, 3000],\n    extruded=True,                 \n    coverage=1)\n\n# Set the viewport location  #어디서 지도를 볼건지 확인\nview_state = pydeck.ViewState(  \n    longitude=-1.415,  #중심위치 설정\n    latitude=52.2323,   #중심위치 설정\n    zoom=6,\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,   #지도 시점조정\n    bearing=-27.36)  #지도 시점 조정\n\n# Combined all of it and render a viewport\nr = pydeck.Deck(layers=[layer], initial_view_state=view_state)  #initial_view_State로 인하여 초기 뷰 상태 전달\nr.show()  #지도 보이기 \n# r.to_html('demo.html')  #html로 저장할 수 있음\n\n\n\n\n\nlayer.elevation_range = [0, 500]\n\nr.update()\n\n\nimport pydeck as pdk\n\n\npdk.Deck\n\npydeck.bindings.deck.Deck\n\n\n\n\nScatter Plots\n\nimport pandas as pd\nfrom pydeck import (\n    data_utils,\n    Deck,\n    Layer\n)\n\n# First, let's use Pandas to download our data\nURL = 'https://raw.githubusercontent.com/ajduberstein/data_sets/master/beijing_subway_station.csv'\ndf = pd.read_csv(URL)\ndf.head()\n#위경도 확인 할수 있음 color: RGB값 [R,G,B, 투명도]\n\n\n\n\n\n  \n    \n      \n      lat\n      lng\n      osm_id\n      station_name\n      chinese_name\n      opening_date\n      color\n      line_name\n    \n  \n  \n    \n      0\n      39.940249\n      116.456359\n      1351272524\n      Agricultural Exhibition Center\n      农业展览馆\n      2008-07-19\n      [0, 146, 188, 255]\n      Line 10\n    \n    \n      1\n      39.955570\n      116.388507\n      5057476994\n      Andelibeijie\n      安德里北街\n      2015-12-26\n      [0, 155, 119, 255]\n      Line 8 (North section)\n    \n    \n      2\n      39.947729\n      116.402067\n      339088654\n      Andingmen\n      安定门\n      1984-09-20\n      [0, 75, 135, 255]\n      Line 2\n    \n    \n      3\n      40.011026\n      116.263981\n      1362259113\n      Anheqiao North\n      安河桥北\n      2009-09-28\n      [0, 140, 149, 255]\n      Line 4\n    \n    \n      4\n      39.967112\n      116.388398\n      5305505996\n      Anhuaqiao\n      安华桥\n      2012-12-30\n      [0, 155, 119, 255]\n      Line 8 (North section)\n    \n  \n\n\n\n\n\nfrom ast import literal_eval  #텍스트를 리스트 형식으로 바꾸기\n# We have to re-code position to be one field in a list, so we'll do that here:\n# The CSV encodes the [R, G, B, A] color values listed in it as a string\ndf['color'] = df.apply(lambda x: literal_eval(x['color']), axis=1)\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      lat\n      lng\n      osm_id\n      station_name\n      chinese_name\n      opening_date\n      color\n      line_name\n    \n  \n  \n    \n      0\n      39.940249\n      116.456359\n      1351272524\n      Agricultural Exhibition Center\n      农业展览馆\n      2008-07-19\n      [0, 146, 188, 255]\n      Line 10\n    \n    \n      1\n      39.955570\n      116.388507\n      5057476994\n      Andelibeijie\n      安德里北街\n      2015-12-26\n      [0, 155, 119, 255]\n      Line 8 (North section)\n    \n    \n      2\n      39.947729\n      116.402067\n      339088654\n      Andingmen\n      安定门\n      1984-09-20\n      [0, 75, 135, 255]\n      Line 2\n    \n    \n      3\n      40.011026\n      116.263981\n      1362259113\n      Anheqiao North\n      安河桥北\n      2009-09-28\n      [0, 140, 149, 255]\n      Line 4\n    \n    \n      4\n      39.967112\n      116.388398\n      5305505996\n      Anhuaqiao\n      安华桥\n      2012-12-30\n      [0, 155, 119, 255]\n      Line 8 (North section)\n    \n  \n\n\n\n\n\n# Use pydeck's data_utils module to fit a viewport to the central 90% of the data\n#위경도 좌표 표시\nviewport = data_utils.compute_view(points=df[['lng', 'lat']], view_proportion=0.9)\nauto_zoom_map = Deck(layers=None, initial_view_state=viewport)\nauto_zoom_map.show()\nauto_zoom_map\n\n# auto_zoom_map.to_html('demo.html')\n\n\n        \n    \n\n\n\nfrom IPython.core.display import display\nimport ipywidgets\n\nyear = 2019\n\nscatterplot = Layer(\n    'ScatterplotLayer',\n    df,\n    id='scatterplot-layer',\n    get_radius=500,\n    get_fill_color='color',\n    get_position='[lng, lat]')\nr = Deck(layers=[scatterplot], initial_view_state=viewport)\n\n# Create an HTML header to display the year\ndisplay_el = ipywidgets.HTML('<h1>{}</h1>'.format(year))\ndisplay(display_el)\n# Show the current visualization\nr.show()\n# r.to_html('demo.html')\n\nC:\\Users\\jiyeo\\AppData\\Local\\Temp\\ipykernel_1504\\1358459236.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n  from IPython.core.display import display\n\n\n\n\n\n\n\n\n\nimport time\nfor y in range(1971, 2020):\n    scatterplot.data = df[df['opening_date'] <= str(y)]\n    year = y\n    # Reset the header to display the year\n    display_el.value = '<h1>{}</h1>'.format(year)\n    r.update()\n    time.sleep(0.1)\n\n\n\nUsing pydeck to manipulate data\n\nimport pydeck as pdk\n\nDATA_URL = 'https://api.data.gov.sg/v1/transport/taxi-availability'\nCOLOR_RANGE = [\n  [255, 255, 178, 25],\n  [254, 217, 118, 85],\n  [254, 178, 76, 127],\n  [253, 141, 60, 170],\n  [240, 59, 32, 212],\n  [189, 0, 38, 255]\n]\n\n\nimport pandas as pd\nimport requests\n\n#싱가포르의 택시 위치 좌표\n\njson = requests.get(DATA_URL).json()  #requests를 통해 데이터 요청\ndf = pd.DataFrame(json[\"features\"][0][\"geometry\"][\"coordinates\"]) #데이터를 데이터프레임으로 변경\ndf.columns = ['lng', 'lat']\n\nviewport = pdk.data_utils.compute_view(df[['lng', 'lat']])  #데이터좌표 범위 계산하여 초기 뷰포인트 설정 생성\nlayer = pdk.Layer(\n    'ScreenGridLayer',  #그리드 형태로 데이터를 시각화\n    df,\n    cell_size_pixels=20, #픽셀 크기\n    color_range=COLOR_RANGE, #그리드 색상 범위\n    get_position='[lng, lat]',  #데이터 좌표 정보를 가져올 수 있는 방법\n    pickable=True, #레이어 선택 가능 하게 하는 옵션\n    auto_highlight=True) #해당 레이어의 요소가 자동으로 하이라이트 되게하는 옵션\nr = pdk.Deck(layers=[layer], initial_view_state=viewport)\n\n\n\nr.show()\n\n\n\n\n\npd.DataFrame([r.deck_widget.selected_data])\n\n\n\n\n\n  \n    \n      \n    \n  \n  \n    \n      0\n    \n  \n\n\n\n\n\n\nPlotting massive data sets.ipynb\n\nimport pandas as pd\n#point 데이터\nall_lidar = pd.concat([\n    pd.read_csv('https://raw.githubusercontent.com/ajduberstein/kitti_subset/master/kitti_1.csv'),\n    pd.read_csv('https://raw.githubusercontent.com/ajduberstein/kitti_subset/master/kitti_2.csv'),\n    pd.read_csv('https://raw.githubusercontent.com/ajduberstein/kitti_subset/master/kitti_3.csv'),\n    pd.read_csv('https://raw.githubusercontent.com/ajduberstein/kitti_subset/master/kitti_4.csv'),\n])\n\n# Filter to one frame of data\nlidar = all_lidar[all_lidar['source'] == 136]\nlidar.loc[: , ['x', 'y']] = lidar[['x', 'y']] / 10000  \n\n\nall_lidar.head()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      z\n      source\n    \n  \n  \n    \n      0\n      32.836\n      0.056\n      1.319\n      122\n    \n    \n      1\n      32.588\n      0.107\n      1.311\n      122\n    \n    \n      2\n      32.403\n      0.208\n      1.305\n      122\n    \n    \n      3\n      60.085\n      1.163\n      2.241\n      122\n    \n    \n      4\n      76.862\n      20.430\n      2.898\n      122\n    \n  \n\n\n\n\n\nimport pydeck as pdk\n\n\npoint_cloud = pdk.Layer(\n    'PointCloudLayer',\n    lidar[['x', 'y', 'z']],\n    get_position='[x, y, z * 10]',\n    get_normal=[0, 0, 1],\n    get_color=[255, 0, 100, 200],\n    pickable=True,  \n    auto_highlight=True,\n    point_size=1)\n\n\nview_state = pdk.data_utils.compute_view(lidar[['x', 'y']], 0.9)\nview_state.max_pitch = 360\nview_state.pitch = 80\nview_state.bearing = 120\n\nr = pdk.Deck(\n    point_cloud,\n    initial_view_state=view_state,\n    map_style='')\nr.show()\n\n\n\n\n\nimport time\nfrom collections import deque\n\n# Choose a handful of frames to loop through\nframe_buffer = deque([42, 56, 81, 95])\nprint('Press the stop icon to exit')\nwhile True:\n    current_frame = frame_buffer[0]\n    lidar = all_lidar[all_lidar['source'] == current_frame]\n    r.layers[0].get_position = '[x , y , z * 10]'\n    r.layers[0].data = lidar.to_dict(orient='records')\n    frame_buffer.rotate()\n    r.update()\n    time.sleep(0.5)\n\nPress the stop icon to exit\n\n\n\n\nInteracting with other Jupyter widgets.ipynb\n\nLIGHTS_URL = 'https://raw.githubusercontent.com/ajduberstein/lights_at_night/master/chengdu_lights_at_night.csv'\ndf = pd.read_csv(LIGHTS_URL)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      lng\n      lat\n      brightness\n    \n  \n  \n    \n      0\n      1993\n      104.575\n      31.808\n      4\n    \n    \n      1\n      1993\n      104.583\n      31.808\n      4\n    \n    \n      2\n      1993\n      104.592\n      31.808\n      4\n    \n    \n      3\n      1993\n      104.600\n      31.808\n      4\n    \n    \n      4\n      1993\n      104.675\n      31.808\n      4\n    \n  \n\n\n\n\n\ndf['color'] = df['brightness'].apply(lambda val: [255, val * 4,  255, 255])\ndf.sample(10) ##255가 흰색\n\n\n\n\n\n  \n    \n      \n      year\n      lng\n      lat\n      brightness\n      color\n    \n  \n  \n    \n      222200\n      2011\n      104.325\n      31.417\n      8\n      [255, 32, 255, 255]\n    \n    \n      86964\n      2001\n      104.483\n      31.675\n      4\n      [255, 16, 255, 255]\n    \n    \n      162996\n      2013\n      103.800\n      30.700\n      51\n      [255, 204, 255, 255]\n    \n    \n      316286\n      1999\n      105.767\n      29.608\n      4\n      [255, 16, 255, 255]\n    \n    \n      263285\n      2005\n      104.783\n      31.442\n      25\n      [255, 100, 255, 255]\n    \n    \n      44450\n      1995\n      103.908\n      30.825\n      8\n      [255, 32, 255, 255]\n    \n    \n      237232\n      2011\n      103.858\n      30.625\n      25\n      [255, 100, 255, 255]\n    \n    \n      122409\n      2003\n      105.042\n      31.133\n      3\n      [255, 12, 255, 255]\n    \n    \n      138668\n      2003\n      104.575\n      30.208\n      3\n      [255, 12, 255, 255]\n    \n    \n      143339\n      2003\n      103.192\n      29.858\n      3\n      [255, 12, 255, 255]\n    \n  \n\n\n\n\n\nplottable = df[df['year'] == 1993].to_dict(orient='records')\n\nview_state = pdk.ViewState(\n    latitude=31.0,\n    longitude=104.5,\n    zoom=8,\n    max_zoom=8,\n    min_zoom=8)\nscatterplot = pdk.Layer(\n    'HeatmapLayer',\n    data=plottable,\n    get_position='[lng, lat]',\n    get_weight='brightness', #가중치를 변수 사용해서 두기\n    opacity=0.5,\n    pickable=False,\n    get_radius=800)\nr = pdk.Deck(\n    layers=[scatterplot],\n    initial_view_state=view_state,\n    views=[pdk.View(type='MapView', controller=None)])\nr.show()\n\n\n\n\n\nimport ipywidgets as widgets\nfrom IPython.display import display\n#연도에 따라 어떻게 변하는지 확인 하기 위해 slider 사용\nslider = widgets.IntSlider(1992, min=1993, max=2013, step=2)\ndef on_change(v):\n    results = df[df['year'] == slider.value].to_dict(orient='records')\n    scatterplot.data = results\n    r.update()\n    \nslider.observe(on_change, names='value')\ndisplay(slider)\n\n\n\n\n\n#지도를 뛰웠을 때 막대를 찍으면 어떤 데이터인지 알려주는 것\ntooltip = {\n   \"html\": \"<b>Elevation Value:</b> {elevationValue}\",\n   \"style\": {\n        \"backgroundColor\": \"steelblue\",\n        \"color\": \"white\"\n   }\n}\n\n\n\nTooltip\n\nimport pydeck as pdk\n\nlayer = pdk.Layer(\n    'HexagonLayer',\n    UK_ACCIDENTS_DATA,\n    get_position='[lng, lat]',\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 3000],\n    extruded=True,\n    coverage=1)\n\n# Set the viewport location\nview_state = pdk.ViewState(\n    longitude=-1.415,\n    latitude=52.2323,\n    zoom=6,\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,\n    bearing=-27.36)\n\n# Combined all of it and render a viewport\nr = pdk.Deck(\n    layers=[layer],\n    initial_view_state=view_state,\n    tooltip={\n        'html': '<b>Elevation Value:</b> {elevationValue}',\n        'style': {\n            'color': 'white'  #text 색 정한것\n        }\n    }\n)\nr.show()\n\n\n\n\n\n그냥 텍스트로 하기\n\n\nimport pydeck as pdk\n\nlayer = pdk.Layer(\n    'HexagonLayer',\n    UK_ACCIDENTS_DATA,\n    get_position='[lng, lat]',\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 3000],\n    extruded=True,\n    coverage=1)\n\n# Set the viewport location\nview_state = pdk.ViewState(\n    longitude=-1.415,\n    latitude=52.2323,\n    zoom=6,\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,\n    bearing=-27.36)\n\n# Combined all of it and render a viewport\nr = pdk.Deck(\n    layers=[layer],\n    initial_view_state=view_state,\n    tooltip = {\n    \"text\": \"Elevation: {elevationValue}\"  #툽팁 말고 text -> 가독성이 떨어짐 \n    }   #원하는 text 사용해서 할수 있음\n)\nr.show()\n\n\n\n\n\n\nTooltip을 그냥 True값만 주기\n\n\nimport pydeck as pdk\n\nlayer = pdk.Layer(\n    'HexagonLayer',\n    UK_ACCIDENTS_DATA,\n    get_position='[lng, lat]',\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 3000],\n    extruded=True,\n    coverage=1)\n\n# Set the viewport location\nview_state = pdk.ViewState(\n    longitude=-1.415,\n    latitude=52.2323,\n    zoom=6,\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,\n    bearing=-27.36)\n\n# Combined all of it and render a viewport\nr = pdk.Deck(\n    layers=[layer],\n    initial_view_state=view_state,\n    tooltip=True\n)\nr.show()\n\n\n\n\n\nUK_ACCIDENTS_DATA = 'https://raw.githubusercontent.com/uber-common/deck.gl-data/master/examples/3d-heatmap/heatmap-data.csv'\n\nuk_data = pd.read_csv(UK_ACCIDENTS_DATA)\n\n\nuk_data.head()\n\n\n\n\n\n  \n    \n      \n      lng\n      lat\n    \n  \n  \n    \n      0\n      -0.198465\n      51.505538\n    \n    \n      1\n      -0.178838\n      51.491836\n    \n    \n      2\n      -0.205590\n      51.514910\n    \n    \n      3\n      -0.208327\n      51.514952\n    \n    \n      4\n      -0.206022\n      51.496572\n    \n  \n\n\n\n\n\n\n미국 택시 데이터 시각화\n\npip install pandas-gbq -U\n\nCollecting pandas-gbq\n  Downloading pandas_gbq-0.19.2-py2.py3-none-any.whl (25 kB)\nCollecting db-dtypes<2.0.0,>=1.0.4\n  Downloading db_dtypes-1.1.1-py2.py3-none-any.whl (14 kB)\nCollecting pyarrow>=3.0.0\n  Downloading pyarrow-12.0.0-cp38-cp38-win_amd64.whl (21.5 MB)\n     -------------------------------------- 21.5/21.5 MB 500.1 kB/s eta 0:00:00\nCollecting google-auth-oauthlib>=0.7.0\n  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nCollecting google-cloud-bigquery-storage<3.0.0dev,>=2.16.2\n  Downloading google_cloud_bigquery_storage-2.19.1-py2.py3-none-any.whl (190 kB)\n     ------------------------------------ 190.1/190.1 kB 501.0 kB/s eta 0:00:00\nRequirement already satisfied: pandas>=1.1.4 in c:\\python\\lib\\site-packages (from pandas-gbq) (1.5.2)\nCollecting google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5\n  Downloading google_cloud_bigquery-3.10.0-py2.py3-none-any.whl (218 kB)\n     ------------------------------------ 218.4/218.4 kB 459.4 kB/s eta 0:00:00\nCollecting pydata-google-auth>=1.5.0\n  Downloading pydata_google_auth-1.8.0-py2.py3-none-any.whl (14 kB)\nCollecting google-api-core<3.0.0dev,>=2.10.2\n  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n     ------------------------------------ 120.3/120.3 kB 542.4 kB/s eta 0:00:00\nRequirement already satisfied: setuptools in c:\\python\\lib\\site-packages (from pandas-gbq) (52.0.0.post20210125)\nRequirement already satisfied: numpy>=1.16.6 in c:\\python\\lib\\site-packages (from pandas-gbq) (1.23.5)\nCollecting google-auth>=2.13.0\n  Downloading google_auth-2.18.1-py2.py3-none-any.whl (178 kB)\n     ------------------------------------ 178.9/178.9 kB 431.2 kB/s eta 0:00:00\nRequirement already satisfied: packaging>=17.0 in c:\\python\\lib\\site-packages (from db-dtypes<2.0.0,>=1.0.4->pandas-gbq) (21.3)\nCollecting googleapis-common-protos<2.0dev,>=1.56.2\n  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n     ------------------------------------ 223.6/223.6 kB 471.0 kB/s eta 0:00:00\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\python\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (2.25.1)\nCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n  Downloading protobuf-4.23.1-cp38-cp38-win_amd64.whl (422 kB)\n     ------------------------------------ 422.5/422.5 kB 560.9 kB/s eta 0:00:00\nCollecting pyasn1-modules>=0.2.1\n  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n     ------------------------------------ 181.3/181.3 kB 781.1 kB/s eta 0:00:00\nRequirement already satisfied: urllib3<2.0 in c:\\python\\lib\\site-packages (from google-auth>=2.13.0->pandas-gbq) (1.26.4)\nCollecting cachetools<6.0,>=2.0.0\n  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\nRequirement already satisfied: six>=1.9.0 in c:\\python\\lib\\site-packages (from google-auth>=2.13.0->pandas-gbq) (1.15.0)\nCollecting rsa<5,>=3.1.4\n  Downloading rsa-4.9-py3-none-any.whl (34 kB)\nCollecting requests-oauthlib>=0.7.0\n  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in c:\\python\\lib\\site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (2.8.1)\nCollecting proto-plus<2.0.0dev,>=1.15.0\n  Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n     -------------------------------------- 47.9/47.9 kB 597.7 kB/s eta 0:00:00\nCollecting google-resumable-media<3.0dev,>=0.6.0\n  Downloading google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB)\n     -------------------------------------- 77.7/77.7 kB 865.4 kB/s eta 0:00:00\nCollecting grpcio<2.0dev,>=1.47.0\n  Downloading grpcio-1.54.2-cp38-cp38-win_amd64.whl (4.1 MB)\n     ---------------------------------------- 4.1/4.1 MB 677.5 kB/s eta 0:00:00\nCollecting google-cloud-core<3.0.0dev,>=1.6.0\n  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\nRequirement already satisfied: pytz>=2020.1 in c:\\python\\lib\\site-packages (from pandas>=1.1.4->pandas-gbq) (2021.1)\nCollecting grpcio-status<2.0dev,>=1.33.2\n  Downloading grpcio_status-1.54.2-py3-none-any.whl (5.1 kB)\nCollecting google-crc32c<2.0dev,>=1.0\n  Downloading google_crc32c-1.5.0-cp38-cp38-win_amd64.whl (27 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python\\lib\\site-packages (from packaging>=17.0->db-dtypes<2.0.0,>=1.0.4->pandas-gbq) (2.4.7)\nCollecting pyasn1<0.6.0,>=0.4.6\n  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n     -------------------------------------- 83.9/83.9 kB 471.9 kB/s eta 0:00:00\nRequirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (2022.9.24)\nRequirement already satisfied: idna<3,>=2.5 in c:\\python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in c:\\python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (4.0.0)\nCollecting oauthlib>=3.0.0\n  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n     ------------------------------------ 151.7/151.7 kB 282.9 kB/s eta 0:00:00\nInstalling collected packages: pyasn1, pyarrow, protobuf, oauthlib, grpcio, google-crc32c, cachetools, rsa, requests-oauthlib, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, grpcio-status, google-auth, db-dtypes, google-auth-oauthlib, google-api-core, pydata-google-auth, google-cloud-core, google-cloud-bigquery-storage, google-cloud-bigquery, pandas-gbq\nSuccessfully installed cachetools-5.3.0 db-dtypes-1.1.1 google-api-core-2.11.0 google-auth-2.18.1 google-auth-oauthlib-1.0.0 google-cloud-bigquery-3.10.0 google-cloud-bigquery-storage-2.19.1 google-cloud-core-2.3.2 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.59.0 grpcio-1.54.2 grpcio-status-1.54.2 oauthlib-3.2.2 pandas-gbq-0.19.2 proto-plus-1.22.2 protobuf-4.23.1 pyarrow-12.0.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pydata-google-auth-1.8.0 requests-oauthlib-1.3.1 rsa-4.9\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n[notice] A new release of pip available: 22.3.1 -> 23.1.2\n[notice] To update, run: python.exe -m pip install --upgrade pip\n\n\n\nimport pandas as pd\nimport pydata_google_auth\n\nSCOPES = [\n  'https://www.googleapis.com/auth/cloud-platform',\n  'https://www.googleapis.com/auth/drive',\n  'https://www.googleapis.com/auth/bigquery'\n]\n\ncredentials = pydata_google_auth.get_user_credentials(\nSCOPES, auth_local_webserver=True)\n\nModuleNotFoundError: No module named 'pydata_google_auth'\n\n\n\nquery = \"\"\"\nSELECT \n    *\nFROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015` \nWHERE EXTRACT(MONTH from pickup_datetime) = 1\nLIMIT 10000\n\"\"\"\n\n\n#%%time\ntaxi_df = pd.read_gbq(query=query, dialect='standard', project_id='persuasive-zoo-147513', credentials=credentials)\n\nWall time: 6.7 s\n\n\n\ntaxi_df\n\n\n\n\n\n  \n    \n      \n      vendor_id\n      pickup_datetime\n      dropoff_datetime\n      passenger_count\n      trip_distance\n      pickup_longitude\n      pickup_latitude\n      rate_code\n      store_and_fwd_flag\n      dropoff_longitude\n      dropoff_latitude\n      payment_type\n      fare_amount\n      extra\n      mta_tax\n      tip_amount\n      tolls_amount\n      imp_surcharge\n      total_amount\n    \n  \n  \n    \n      0\n      1\n      2015-01-02 16:26:22\n      2015-01-02 16:51:10\n      2\n      2.50\n      -73.993172\n      40.762901\n      <NA>\n      N\n      -73.962097\n      40.763584\n      1\n      15.7\n      1.0\n      0.5\n      3.50\n      0.0\n      0.0\n      21.00\n    \n    \n      1\n      1\n      2015-01-16 17:13:00\n      2015-01-16 17:16:10\n      1\n      0.40\n      -73.961601\n      40.771229\n      <NA>\n      N\n      -73.959419\n      40.775253\n      1\n      4.0\n      1.0\n      0.5\n      1.15\n      0.0\n      0.3\n      6.95\n    \n    \n      2\n      2\n      2015-01-24 04:25:01\n      2015-01-24 04:41:43\n      2\n      4.64\n      -74.000595\n      40.737167\n      <NA>\n      N\n      -73.995499\n      40.680763\n      1\n      16.0\n      0.5\n      0.5\n      19.50\n      0.0\n      0.3\n      36.80\n    \n    \n      3\n      2\n      2015-01-30 14:29:58\n      2015-01-30 15:27:13\n      1\n      18.39\n      -73.989914\n      40.729706\n      <NA>\n      N\n      -73.782310\n      40.644180\n      1\n      52.0\n      0.0\n      0.5\n      5.50\n      0.0\n      0.3\n      58.30\n    \n    \n      4\n      1\n      2015-01-14 21:24:13\n      2015-01-14 21:25:55\n      1\n      0.50\n      -73.954849\n      40.773220\n      <NA>\n      N\n      -73.959801\n      40.769432\n      1\n      3.5\n      0.5\n      0.5\n      0.96\n      0.0\n      0.3\n      5.76\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9995\n      1\n      2015-01-24 22:49:17\n      2015-01-24 22:56:20\n      2\n      0.70\n      -74.004578\n      40.724056\n      <NA>\n      N\n      -74.006958\n      40.732971\n      1\n      6.0\n      0.5\n      0.5\n      1.80\n      0.0\n      0.3\n      9.10\n    \n    \n      9996\n      2\n      2015-01-21 18:20:27\n      2015-01-21 18:30:16\n      6\n      1.14\n      -74.000038\n      40.748291\n      <NA>\n      N\n      -73.990608\n      40.738071\n      1\n      8.0\n      1.0\n      0.5\n      1.80\n      0.0\n      0.3\n      11.60\n    \n    \n      9997\n      2\n      2015-01-09 20:35:23\n      2015-01-09 20:43:55\n      1\n      2.00\n      -73.974876\n      40.748661\n      <NA>\n      N\n      -73.980530\n      40.768021\n      1\n      8.5\n      0.5\n      0.5\n      1.80\n      0.0\n      0.3\n      11.60\n    \n    \n      9998\n      2\n      2015-01-31 22:01:33\n      2015-01-31 22:11:22\n      1\n      1.42\n      -73.984718\n      40.728447\n      <NA>\n      N\n      -73.975380\n      40.745564\n      1\n      8.5\n      0.5\n      0.5\n      1.80\n      0.0\n      0.3\n      11.60\n    \n    \n      9999\n      2\n      2015-01-13 13:40:14\n      2015-01-13 13:51:28\n      5\n      1.57\n      -73.981232\n      40.747498\n      <NA>\n      N\n      -73.998116\n      40.733883\n      1\n      9.0\n      0.0\n      0.5\n      1.80\n      0.0\n      0.3\n      11.60\n    \n  \n\n10000 rows × 19 columns\n\n\n\n\n\nGridLayer\n\n10만개 데이터\n\n\narc_layer = pdk.Layer(\n    'GridLayer',\n    taxi_df,\n    get_position='[pickup_longitude, pickup_latitude]',\n    pickable=True, \n    auto_highlight=True,\n    tooltip=True\n)\n\nnyc_center = [-73.9808, 40.7648] \nview_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9)\n\nr = pdk.Deck(layers=[arc_layer], initial_view_state=view_state)\nr.show()\n\n\n\n\n\n\nArc Layer\n\nzip_code_query = \"\"\"\nWITH base_data AS \n(\n  SELECT \n    nyc_taxi.*, \n    pickup.zip_code as pickup_zip_code,\n    pickup.internal_point_lat as pickup_zip_code_lat,\n    pickup.internal_point_lon as pickup_zip_code_lon,\n    dropoff.zip_code as dropoff_zip_code,\n    dropoff.internal_point_lat as dropoff_zip_code_lat,\n    dropoff.internal_point_lon as dropoff_zip_code_lon\n  FROM (\n    SELECT *\n    FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`\n    WHERE \n        EXTRACT(MONTH from pickup_datetime) = 1\n        and pickup_latitude <= 90 and pickup_latitude >= -90\n        and dropoff_latitude <= 90 and dropoff_latitude >= -90\n    ) AS nyc_taxi\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY'\n    ) AS pickup \n  ON ST_CONTAINS(pickup.zip_code_geom, st_geogpoint(pickup_longitude, pickup_latitude))\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY' \n    ) AS dropoff\n  ON ST_CONTAINS(dropoff.zip_code_geom, st_geogpoint(dropoff_longitude, dropoff_latitude))\n  \n)\n\nSELECT \n  *\nFROM base_data \nlimit 10000\n\"\"\"\n\n\n%%time\ntaxi_df_by_zipcode = pd.read_gbq(query=zip_code_query, dialect='standard', project_id='persuasive-zoo-147513', credentials=credentials)\n\nWall time: 36.5 s\n\n\n\ntaxi_df_by_zipcode.head(3)\n\n\n\n\n\n  \n    \n      \n      vendor_id\n      pickup_datetime\n      dropoff_datetime\n      passenger_count\n      trip_distance\n      pickup_longitude\n      pickup_latitude\n      rate_code\n      store_and_fwd_flag\n      dropoff_longitude\n      ...\n      tip_amount\n      tolls_amount\n      imp_surcharge\n      total_amount\n      pickup_zip_code\n      pickup_zip_code_lat\n      pickup_zip_code_lon\n      dropoff_zip_code\n      dropoff_zip_code_lat\n      dropoff_zip_code_lon\n    \n  \n  \n    \n      0\n      2\n      2015-01-20 09:57:47\n      2015-01-20 10:14:24\n      1\n      2.54\n      -74.014793\n      40.714111\n      <NA>\n      N\n      -73.991997\n      ...\n      2.5\n      0.0\n      0.3\n      15.8\n      10280\n      40.709073\n      -74.016423\n      10003\n      40.731829\n      -73.989181\n    \n    \n      1\n      1\n      2015-01-15 04:50:46\n      2015-01-15 05:01:29\n      1\n      2.30\n      -74.015938\n      40.710976\n      <NA>\n      N\n      -73.996552\n      ...\n      0.0\n      0.0\n      0.3\n      11.3\n      10280\n      40.709073\n      -74.016423\n      10011\n      40.742043\n      -74.000620\n    \n    \n      2\n      2\n      2015-01-22 09:31:00\n      2015-01-22 09:50:42\n      1\n      4.13\n      -73.989738\n      40.701981\n      <NA>\n      N\n      -74.007828\n      ...\n      2.0\n      0.0\n      0.3\n      19.8\n      11201\n      40.693700\n      -73.989859\n      10011\n      40.742043\n      -74.000620\n    \n  \n\n3 rows × 25 columns\n\n\n\n\n\narc_layer = pdk.Layer(\n    'ArcLayer',\n    taxi_df_by_zipcode,\n    get_source_position='[pickup_zip_code_lon, pickup_zip_code_lat]',\n    get_target_position='[dropoff_zip_code_lon, dropoff_zip_code_lat]',\n    get_source_color='[255, 255, 120]', \n    get_target_color='[255, 0, 0]',\n    get_widht='elevationValue',\n    pickable=True, \n    auto_highlight=True,\n)\n\nnyc_center = [-73.9808, 40.7648] \nview_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9)\n\nr = pdk.Deck(layers=[arc_layer], initial_view_state=view_state)\nr.show()\n\n\n\n\n\n\nAggregate\n\nagg_query = \"\"\"\nWITH base_data AS \n(\n  SELECT \n    nyc_taxi.*, \n    pickup.zip_code as pickup_zip_code,\n    pickup.internal_point_lat as pickup_zip_code_lat,\n    pickup.internal_point_lon as pickup_zip_code_lon,\n    dropoff.zip_code as dropoff_zip_code,\n    dropoff.internal_point_lat as dropoff_zip_code_lat,\n    dropoff.internal_point_lon as dropoff_zip_code_lon\n  FROM (\n    SELECT *\n    FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`\n    WHERE \n        EXTRACT(MONTH from pickup_datetime) = 1\n        and pickup_latitude <= 90 and pickup_latitude >= -90\n        and dropoff_latitude <= 90 and dropoff_latitude >= -90\n    ) AS nyc_taxi\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY'\n    ) AS pickup \n  ON ST_CONTAINS(pickup.zip_code_geom, st_geogpoint(pickup_longitude, pickup_latitude))\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY' \n    ) AS dropoff\n  ON ST_CONTAINS(dropoff.zip_code_geom, st_geogpoint(dropoff_longitude, dropoff_latitude))\n  \n)\n\nSELECT \n  pickup_zip_code,\n  pickup_zip_code_lat,\n  pickup_zip_code_lon,\n  dropoff_zip_code,\n  dropoff_zip_code_lat,\n  dropoff_zip_code_lon,\n  COUNT(*) AS cnt\nFROM base_data \nGROUP BY 1,2,3,4,5,6\nlimit 10000\n\"\"\"\n\n\n%%time\nagg_df = pd.read_gbq(query=agg_query, dialect='standard', project_id='persuasive-zoo-147513', credentials=credentials)\n\nWall time: 15.9 s\n\n\n\nagg_df.head()\n\n\n\n\n\n  \n    \n      \n      pickup_zip_code\n      pickup_zip_code_lat\n      pickup_zip_code_lon\n      dropoff_zip_code\n      dropoff_zip_code_lat\n      dropoff_zip_code_lon\n      cnt\n    \n  \n  \n    \n      0\n      11693\n      40.590916\n      -73.809715\n      11414\n      40.657604\n      -73.844804\n      1\n    \n    \n      1\n      10040\n      40.858314\n      -73.930494\n      10040\n      40.858314\n      -73.930494\n      139\n    \n    \n      2\n      10473\n      40.818690\n      -73.858474\n      10030\n      40.818267\n      -73.942856\n      1\n    \n    \n      3\n      10451\n      40.820454\n      -73.925066\n      10031\n      40.825288\n      -73.950045\n      93\n    \n    \n      4\n      11209\n      40.621993\n      -74.030134\n      11228\n      40.616698\n      -74.013066\n      28\n    \n  \n\n\n\n\n\nagg_df = agg_df.sort_values('cnt', ascending=False)\n\n\nagg_df = agg_df[:100]\n\n\n\narc_layer = pdk.Layer(\n    'ArcLayer',\n    agg_df,\n    get_source_position='[pickup_zip_code_lon, pickup_zip_code_lat]',\n    get_target_position='[dropoff_zip_code_lon, dropoff_zip_code_lat]',\n    get_source_color='[255, 255, 120]', \n    get_target_color='[255, 0, 0]',\n    width_units='meters',\n    get_width=\"1+10*cnt/500\",\n    pickable=True, \n    auto_highlight=True,\n)\n\nnyc_center = [-73.9808, 40.7648] \nview_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9)\n\nr = pdk.Deck(layers=[arc_layer], initial_view_state=view_state,\n             tooltip={\n                 'html': '<b>count:</b> {cnt}',\n                 'style': {\n                     'color': 'white'\n                 }\n             }\n            )\nr.show()\n\n\n\n\n\n\n요일별 위젯\n\nagg_query2 = \"\"\"\nWITH base_data AS \n(\n  SELECT \n    nyc_taxi.*, \n    pickup.zip_code as pickup_zip_code,\n    pickup.internal_point_lat as pickup_zip_code_lat,\n    pickup.internal_point_lon as pickup_zip_code_lon,\n    dropoff.zip_code as dropoff_zip_code,\n    dropoff.internal_point_lat as dropoff_zip_code_lat,\n    dropoff.internal_point_lon as dropoff_zip_code_lon\n  FROM (\n    SELECT *\n    FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`\n    WHERE \n        EXTRACT(MONTH from pickup_datetime) = 1\n        and pickup_latitude <= 90 and pickup_latitude >= -90\n        and dropoff_latitude <= 90 and dropoff_latitude >= -90\n    LIMIT 100000\n    ) AS nyc_taxi\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY'\n    ) AS pickup \n  ON ST_CONTAINS(pickup.zip_code_geom, st_geogpoint(pickup_longitude, pickup_latitude))\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY' \n    ) AS dropoff\n  ON ST_CONTAINS(dropoff.zip_code_geom, st_geogpoint(dropoff_longitude, dropoff_latitude))\n  \n)\n\nSELECT \n  CAST(format_datetime('%u', pickup_datetime) AS INT64) -1 AS weekday,\n  pickup_zip_code,\n  pickup_zip_code_lat,\n  pickup_zip_code_lon,\n  dropoff_zip_code,\n  dropoff_zip_code_lat,\n  dropoff_zip_code_lon,\n  COUNT(*) AS cnt\nFROM base_data \nGROUP BY 1,2,3,4,5,6,7\n\"\"\"\n\n\n%%time\nagg_df2 = pd.read_gbq(query=agg_query2, dialect='standard', project_id='persuasive-zoo-147513', credentials=credentials)\n\nWall time: 7.41 s\n\n\n\nagg_df2.head()\n\n\n\n\n\n  \n    \n      \n      weekday\n      pickup_zip_code\n      pickup_zip_code_lat\n      pickup_zip_code_lon\n      dropoff_zip_code\n      dropoff_zip_code_lat\n      dropoff_zip_code_lon\n      cnt\n    \n  \n  \n    \n      0\n      4\n      11214\n      40.599148\n      -73.996090\n      10035\n      40.795458\n      -73.929570\n      1\n    \n    \n      1\n      6\n      10171\n      40.755899\n      -73.973858\n      11430\n      40.646809\n      -73.786169\n      2\n    \n    \n      2\n      5\n      10461\n      40.847394\n      -73.840583\n      10475\n      40.874375\n      -73.823656\n      1\n    \n    \n      3\n      0\n      10172\n      40.755273\n      -73.974315\n      10065\n      40.764628\n      -73.963144\n      1\n    \n    \n      4\n      6\n      10162\n      40.769308\n      -73.949924\n      11430\n      40.646809\n      -73.786169\n      1\n    \n  \n\n\n\n\n\ndefault_data = agg_df2[agg_df2['weekday'] == 0].to_dict(orient='records')\n\n\narc_layer = pdk.Layer(\n    'ArcLayer',\n    default_data,\n    get_source_position='[pickup_zip_code_lon, pickup_zip_code_lat]',\n    get_target_position='[dropoff_zip_code_lon, dropoff_zip_code_lat]',\n    get_source_color='[255, 255, 120]', \n    get_target_color='[255, 0, 0]',\n    width_units='meters',\n    get_width=\"1+10*cnt/500\",\n    pickable=True, \n    auto_highlight=True,\n)\n\nnyc_center = [-73.9808, 40.7648] \nview_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9)\n\nr = pdk.Deck(layers=[arc_layer], initial_view_state=view_state,\n             tooltip={\n                 'html': '<b>count:</b> {cnt}',\n                 'style': {\n                     'color': 'white'\n                 }\n             }\n            )\nr.show()\n\n\n\n\n\n# Widget 슬라이더 생성\nimport ipywidgets as widgets\nfrom IPython.display import display\nslider = widgets.IntSlider(0, min=0, max=6, step=1)\n\n# Widget에서 사용할 함수 정의 \ndef on_change(v):\n    results = agg_df2[agg_df2['weekday'] == slider.value].to_dict(orient='records')\n    arc_layer.data = results\n    r.update()\n\n# Deck과 슬라이더 연결\nslider.observe(on_change, names='value')\ndisplay(slider)"
  },
  {
    "objectID": "Data/4. pydeck-example.html",
    "href": "Data/4. pydeck-example.html",
    "title": "4. pydeck-example",
    "section": "",
    "text": "<실행시키기위한 패키지 설치 및 실행>\n#1. pip install pydeck\n#2. !jupyter nbextension install –sys-prefix –symlink –overwrite –py pydeck #. !jupyter nbextension enable –sys-prefix –py pydeck\n#3. pip install pydeck[jupyter]\n#4. MAPBOX_API_KEY=“pk.eyJ1Ijoic3BlYXI1MzA2IiwiYSI6ImNremN5Z2FrOTI0ZGgycm45Mzh3dDV6OWQifQ.kXGWHPRjnVAEHgVgLzXn2g”\n\n공식 홈페이지 예시\n\nimport pandas as pd\nimport pydeck\n\nUK_ACCIDENTS_DATA = 'https://raw.githubusercontent.com/uber-common/deck.gl-data/master/examples/3d-heatmap/heatmap-data.csv'\n\npd.read_csv(UK_ACCIDENTS_DATA).head()\n\n\n\n\n\n  \n    \n      \n      lng\n      lat\n    \n  \n  \n    \n      0\n      -0.198465\n      51.505538\n    \n    \n      1\n      -0.178838\n      51.491836\n    \n    \n      2\n      -0.205590\n      51.514910\n    \n    \n      3\n      -0.208327\n      51.514952\n    \n    \n      4\n      -0.206022\n      51.496572\n    \n  \n\n\n\n\n\nUK_ACCIDENTS_DATA\n\n'https://raw.githubusercontent.com/uber-common/deck.gl-data/master/examples/3d-heatmap/heatmap-data.csv'\n\n\n\nlayer = pydeck.Layer(\n    'HexagonLayer',\n    UK_ACCIDENTS_DATA,\n    get_position='[lng,lat]',\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 3000],\n    extruded=True,                 \n    coverage=1)\n\n# Set the viewport location  #어디서 지도를 볼건지 확인\nview_state = pydeck.ViewState(  \n    longitude=-1.415,\n    latitude=52.2323,\n    zoom=6,\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,\n    bearing=-27.36)\n\n# Combined all of it and render a viewport\nr = pydeck.Deck(layers=[layer], initial_view_state=view_state)\nr.show()  #지도 보이기 \n# r.to_html('demo.html')  #html로 저장할 수 있음\n\n\n\n\n\nlayer.elevation_range = [0, 500]\n\nr.update()\n\n\nimport pydeck as pdk\n\n\npdk.Deck?\n\nInit signature:\npdk.Deck(\n    layers=None,\n    views=[{\n  \"@@type\": \"MapView\",\n  \"controller\": true\n}],\n    map_style='dark',\n    api_keys=None,\n    initial_view_state={\n  \"latitude\": 0,\n  \"longitude\": 0,\n  \"zoom\": 1\n},\n    width='100%',\n    height=500,\n    tooltip=True,\n    description=None,\n    effects=None,\n    map_provider='carto',\n    parameters=None,\n)\nDocstring:      <no docstring>\nInit docstring:\nThis is the renderer and configuration for a deck.gl visualization, similar to the\n`Deck <https://deck.gl/docs/api-reference/core/deck>`_ class from deck.gl.\nPass `Deck` a Mapbox API token to display a basemap; see the notes below.\n\nParameters\n----------\n\nlayers : pydeck.Layer or list of pydeck.Layer, default None\n    List of :class:`pydeck.bindings.layer.Layer` layers to render.\nviews : list of pydeck.View, default ``[pydeck.View(type=\"MapView\", controller=True)]``\n    List of :class:`pydeck.bindings.view.View` objects to render.\napi_keys : dict, default None\n    Dictionary of geospatial API service providers, where the keys are ``mapbox``, ``google_maps``, or ``carto``\n    and the values are the API key. Defaults to None if not set. Any of the environment variables\n    ``MAPBOX_API_KEY``, ``GOOGLE_MAPS_API_KEY``, and ``CARTO_API_KEY`` can be set instead of hardcoding the key here.\nmap_provider : str, default 'carto'\n    If multiple API keys are set (e.g., both Mapbox and Google Maps), inform pydeck which basemap provider to prefer.\n    Values can be ``carto``, ``mapbox`` or ``google_maps``\nmap_style : str or dict, default 'dark'\n    One of 'light', 'dark', 'road', 'satellite', 'dark_no_labels', and 'light_no_labels', a URI for a basemap\n    style, which varies by provider, or a dict that follows the Mapbox style `specification <https://docs.mapbox.com/mapbox-gl-js/style-spec/>`.\n    The default is Carto's Dark Matter map. For Mapbox examples, see  Mapbox's `gallery <https://www.mapbox.com/gallery/>`.\n    If not using a basemap, set ``map_provider=None``.\ninitial_view_state : pydeck.ViewState, default ``pydeck.ViewState(latitude=0, longitude=0, zoom=1)``\n    Initial camera angle relative to the map, defaults to a fully zoomed out 0, 0-centered map\n    To compute a viewport from data, see :func:`pydeck.data_utils.viewport_helpers.compute_view`\nheight : int, default 500\n    Height of Jupyter notebook cell, in pixels.\nwidth : int` or string, default '100%'\n    Width of visualization, in pixels (if a number) or as a CSS value string.\ntooltip : bool or dict of {str: str}, default True\n    If ``True``/``False``, toggles a default tooltip on visualization hover.\n    Layers must have ``pickable=True`` set in order to display a tooltip.\n    For more advanced usage, the user can pass a dict to configure more custom tooltip features.\n    Further documentation is `here <tooltip.html>`_.\n\n.. _Deck:\n    https://deck.gl/docs/api-reference/core/deck\n.. _gallery:\n    https://www.mapbox.com/gallery/\nFile:           c:\\users\\jiyeo\\anaconda3\\envs\\dayeon\\lib\\site-packages\\pydeck\\bindings\\deck.py\nType:           type\nSubclasses:     \n\n\n\n\nScatter Plots\n\nimport pandas as pd\nfrom pydeck import (\n    data_utils,\n    Deck,\n    Layer\n)\n\n# First, let's use Pandas to download our data\nURL = 'https://raw.githubusercontent.com/ajduberstein/data_sets/master/beijing_subway_station.csv'\ndf = pd.read_csv(URL)\ndf.head()\n#위경도 확인 할수 있음 color: RGB값 [R,G,B, 투명도]\n\n\n\n\n\n  \n    \n      \n      lat\n      lng\n      osm_id\n      station_name\n      chinese_name\n      opening_date\n      color\n      line_name\n    \n  \n  \n    \n      0\n      39.940249\n      116.456359\n      1351272524\n      Agricultural Exhibition Center\n      农业展览馆\n      2008-07-19\n      [0, 146, 188, 255]\n      Line 10\n    \n    \n      1\n      39.955570\n      116.388507\n      5057476994\n      Andelibeijie\n      安德里北街\n      2015-12-26\n      [0, 155, 119, 255]\n      Line 8 (North section)\n    \n    \n      2\n      39.947729\n      116.402067\n      339088654\n      Andingmen\n      安定门\n      1984-09-20\n      [0, 75, 135, 255]\n      Line 2\n    \n    \n      3\n      40.011026\n      116.263981\n      1362259113\n      Anheqiao North\n      安河桥北\n      2009-09-28\n      [0, 140, 149, 255]\n      Line 4\n    \n    \n      4\n      39.967112\n      116.388398\n      5305505996\n      Anhuaqiao\n      安华桥\n      2012-12-30\n      [0, 155, 119, 255]\n      Line 8 (North section)\n    \n  \n\n\n\n\n\nfrom ast import literal_eval  #텍스트를 리스트 형식으로 바꾸기\n# We have to re-code position to be one field in a list, so we'll do that here:\n# The CSV encodes the [R, G, B, A] color values listed in it as a string\ndf['color'] = df.apply(lambda x: literal_eval(x['color']), axis=1)\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      lat\n      lng\n      osm_id\n      station_name\n      chinese_name\n      opening_date\n      color\n      line_name\n    \n  \n  \n    \n      0\n      39.940249\n      116.456359\n      1351272524\n      Agricultural Exhibition Center\n      农业展览馆\n      2008-07-19\n      [0, 146, 188, 255]\n      Line 10\n    \n    \n      1\n      39.955570\n      116.388507\n      5057476994\n      Andelibeijie\n      安德里北街\n      2015-12-26\n      [0, 155, 119, 255]\n      Line 8 (North section)\n    \n    \n      2\n      39.947729\n      116.402067\n      339088654\n      Andingmen\n      安定门\n      1984-09-20\n      [0, 75, 135, 255]\n      Line 2\n    \n    \n      3\n      40.011026\n      116.263981\n      1362259113\n      Anheqiao North\n      安河桥北\n      2009-09-28\n      [0, 140, 149, 255]\n      Line 4\n    \n    \n      4\n      39.967112\n      116.388398\n      5305505996\n      Anhuaqiao\n      安华桥\n      2012-12-30\n      [0, 155, 119, 255]\n      Line 8 (North section)\n    \n  \n\n\n\n\n\n# Use pydeck's data_utils module to fit a viewport to the central 90% of the data\nviewport = data_utils.compute_view(points=df[['lng', 'lat']], view_proportion=0.9)\nauto_zoom_map = Deck(layers=None, initial_view_state=viewport)\nauto_zoom_map.show()\nauto_zoom_map\n\n# auto_zoom_map.to_html('demo.html')\n\n\n        \n    \n\n\n\nfrom IPython.core.display import display\nimport ipywidgets\n\nyear = 2019\n\nscatterplot = Layer(\n    'ScatterplotLayer',\n    df,\n    id='scatterplot-layer',\n    get_radius=500,\n    get_fill_color='color',\n    get_position='[lng, lat]')\nr = Deck(layers=[scatterplot], initial_view_state=viewport)\n\n# Create an HTML header to display the year\ndisplay_el = ipywidgets.HTML('<h1>{}</h1>'.format(year))\ndisplay(display_el)\n# Show the current visualization\nr.show()\n# r.to_html('demo.html')\n\n\n\n\n\n\n\n\nimport time\nfor y in range(1971, 2020):\n    scatterplot.data = df[df['opening_date'] <= str(y)]\n    year = y\n    # Reset the header to display the year\n    display_el.value = '<h1>{}</h1>'.format(year)\n    r.update()\n    time.sleep(0.1)\n\n\n\nUsing pydeck to manipulate data\n\nimport pydeck as pdk\n\nDATA_URL = 'https://api.data.gov.sg/v1/transport/taxi-availability'\nCOLOR_RANGE = [\n  [255, 255, 178, 25],\n  [254, 217, 118, 85],\n  [254, 178, 76, 127],\n  [253, 141, 60, 170],\n  [240, 59, 32, 212],\n  [189, 0, 38, 255]\n]\n\n\nimport pandas as pd\nimport requests\n\n#싱가포르의 택시 위치 좌표\n\njson = requests.get(DATA_URL).json()\ndf = pd.DataFrame(json[\"features\"][0][\"geometry\"][\"coordinates\"])\ndf.columns = ['lng', 'lat']\n\nviewport = pdk.data_utils.compute_view(df[['lng', 'lat']])\nlayer = pdk.Layer(\n    'ScreenGridLayer',\n    df,\n    cell_size_pixels=20,\n    color_range=COLOR_RANGE,\n    get_position='[lng, lat]',\n    pickable=True,\n    auto_highlight=True)\nr = pdk.Deck(layers=[layer], initial_view_state=viewport)\n\n\nr.show()\n\n\n\n\n\npd.DataFrame([r.deck_widget.selected_data])\n\n\n\n\n\n  \n    \n      \n    \n  \n  \n    \n      0\n    \n  \n\n\n\n\n\n\nPlotting massive data sets.ipynb\n\nimport pandas as pd\n#point 데이터\nall_lidar = pd.concat([\n    pd.read_csv('https://raw.githubusercontent.com/ajduberstein/kitti_subset/master/kitti_1.csv'),\n    pd.read_csv('https://raw.githubusercontent.com/ajduberstein/kitti_subset/master/kitti_2.csv'),\n    pd.read_csv('https://raw.githubusercontent.com/ajduberstein/kitti_subset/master/kitti_3.csv'),\n    pd.read_csv('https://raw.githubusercontent.com/ajduberstein/kitti_subset/master/kitti_4.csv'),\n])\n\n# Filter to one frame of data\nlidar = all_lidar[all_lidar['source'] == 136]\nlidar.loc[: , ['x', 'y']] = lidar[['x', 'y']] / 10000  \n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  lidar.loc[: , ['x', 'y']] = lidar[['x', 'y']] / 10000\n\n\n\nall_lidar.head()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      z\n      source\n    \n  \n  \n    \n      0\n      32.836\n      0.056\n      1.319\n      122\n    \n    \n      1\n      32.588\n      0.107\n      1.311\n      122\n    \n    \n      2\n      32.403\n      0.208\n      1.305\n      122\n    \n    \n      3\n      60.085\n      1.163\n      2.241\n      122\n    \n    \n      4\n      76.862\n      20.430\n      2.898\n      122\n    \n  \n\n\n\n\n\nimport pydeck as pdk\n\n\npoint_cloud = pdk.Layer(\n    'PointCloudLayer',\n    lidar[['x', 'y', 'z']],\n    get_position='[x, y, z * 10]',\n    get_normal=[0, 0, 1],\n    get_color=[255, 0, 100, 200],\n    pickable=True,  \n    auto_highlight=True,\n    point_size=1)\n\n\nview_state = pdk.data_utils.compute_view(lidar[['x', 'y']], 0.9)\nview_state.max_pitch = 360\nview_state.pitch = 80\nview_state.bearing = 120\n\nr = pdk.Deck(\n    point_cloud,\n    initial_view_state=view_state,\n    map_style='')\nr.show()\n\n\n\n\n\nimport time\nfrom collections import deque\n\n# Choose a handful of frames to loop through\nframe_buffer = deque([42, 56, 81, 95])\nprint('Press the stop icon to exit')\nwhile True:\n    current_frame = frame_buffer[0]\n    lidar = all_lidar[all_lidar['source'] == current_frame]\n    r.layers[0].get_position = '[x , y , z * 10]'\n    r.layers[0].data = lidar.to_dict(orient='records')\n    frame_buffer.rotate()\n    r.update()\n    time.sleep(0.5)\n\nPress the stop icon to exit\n\n\nNameError: name 'all_lidar' is not defined\n\n\n\n\nInteracting with other Jupyter widgets.ipynb\n\nLIGHTS_URL = 'https://raw.githubusercontent.com/ajduberstein/lights_at_night/master/chengdu_lights_at_night.csv'\ndf = pd.read_csv(LIGHTS_URL)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      lng\n      lat\n      brightness\n    \n  \n  \n    \n      0\n      1993\n      104.575\n      31.808\n      4\n    \n    \n      1\n      1993\n      104.583\n      31.808\n      4\n    \n    \n      2\n      1993\n      104.592\n      31.808\n      4\n    \n    \n      3\n      1993\n      104.600\n      31.808\n      4\n    \n    \n      4\n      1993\n      104.675\n      31.808\n      4\n    \n  \n\n\n\n\n\ndf['color'] = df['brightness'].apply(lambda val: [255, val * 4,  255, 255])\ndf.sample(10) ##255가 흰색\n\n\n\n\n\n  \n    \n      \n      year\n      lng\n      lat\n      brightness\n      color\n    \n  \n  \n    \n      114481\n      2001\n      105.075\n      29.567\n      17\n      [255, 68, 255, 255]\n    \n    \n      124735\n      2003\n      103.800\n      31.000\n      3\n      [255, 12, 255, 255]\n    \n    \n      297452\n      2005\n      104.667\n      29.533\n      18\n      [255, 72, 255, 255]\n    \n    \n      246171\n      2011\n      103.517\n      30.233\n      11\n      [255, 44, 255, 255]\n    \n    \n      10328\n      1993\n      105.633\n      30.492\n      4\n      [255, 16, 255, 255]\n    \n    \n      83641\n      2009\n      103.758\n      29.633\n      15\n      [255, 60, 255, 255]\n    \n    \n      239849\n      2011\n      104.283\n      30.517\n      13\n      [255, 52, 255, 255]\n    \n    \n      169534\n      2013\n      104.350\n      30.375\n      6\n      [255, 24, 255, 255]\n    \n    \n      196560\n      2007\n      104.458\n      30.733\n      3\n      [255, 12, 255, 255]\n    \n    \n      186670\n      2007\n      104.492\n      31.292\n      12\n      [255, 48, 255, 255]\n    \n  \n\n\n\n\n\nplottable = df[df['year'] == 1993].to_dict(orient='records')\n\nview_state = pdk.ViewState(\n    latitude=31.0,\n    longitude=104.5,\n    zoom=8,\n    max_zoom=8,\n    min_zoom=8)\nscatterplot = pdk.Layer(\n    'HeatmapLayer',\n    data=plottable,\n    get_position='[lng, lat]',\n    get_weight='brightness', #가중치를 변수 사용해서 두기\n    opacity=0.5,\n    pickable=False,\n    get_radius=800)\nr = pdk.Deck(\n    layers=[scatterplot],\n    initial_view_state=view_state,\n    views=[pdk.View(type='MapView', controller=None)])\nr.show()\n\nKeyError: 'year'\n\n\n\nimport ipywidgets as widgets\nfrom IPython.display import display\n#연도에 따라 어떻게 변하는지 확인 하기 위해 slider 사용\nslider = widgets.IntSlider(1992, min=1993, max=2013, step=2)\ndef on_change(v):\n    results = df[df['year'] == slider.value].to_dict(orient='records')\n    scatterplot.data = results\n    r.update()\n    \nslider.observe(on_change, names='value')\ndisplay(slider)\n\n\n\n\n\n#지도를 뛰웠을 때 막대를 찍으면 어떤 데이터인지 알려주는 것\ntooltip = {\n   \"html\": \"<b>Elevation Value:</b> {elevationValue}\",\n   \"style\": {\n        \"backgroundColor\": \"steelblue\",\n        \"color\": \"white\"\n   }\n}\n\n\n\nTooltip\n\nimport pydeck as pdk\n\nlayer = pdk.Layer(\n    'HexagonLayer',\n    UK_ACCIDENTS_DATA,\n    get_position='[lng, lat]',\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 3000],\n    extruded=True,\n    coverage=1)\n\n# Set the viewport location\nview_state = pdk.ViewState(\n    longitude=-1.415,\n    latitude=52.2323,\n    zoom=6,\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,\n    bearing=-27.36)\n\n# Combined all of it and render a viewport\nr = pdk.Deck(\n    layers=[layer],\n    initial_view_state=view_state,\n    tooltip={\n        'html': '<b>Elevation Value:</b> {elevationValue}',\n        'style': {\n            'color': 'white'  #text 색 정한것\n        }\n    }\n)\nr.show()\n\n\n\n\n\n그냥 텍스트로 하기\n\n\nimport pydeck as pdk\n\nlayer = pdk.Layer(\n    'HexagonLayer',\n    UK_ACCIDENTS_DATA,\n    get_position='[lng, lat]',\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 3000],\n    extruded=True,\n    coverage=1)\n\n# Set the viewport location\nview_state = pdk.ViewState(\n    longitude=-1.415,\n    latitude=52.2323,\n    zoom=6,\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,\n    bearing=-27.36)\n\n# Combined all of it and render a viewport\nr = pdk.Deck(\n    layers=[layer],\n    initial_view_state=view_state,\n    tooltip = {\n    \"text\": \"Elevation: {elevationValue}\"  #툽팁 말고 text -> 가독성이 떨어짐 \n    }   #원하는 text 사용해서 할수 있음\n)\nr.show()\n\n\n\n\n\n\nTooltip을 그냥 True값만 주기\n\n\nimport pydeck as pdk\n\nlayer = pdk.Layer(\n    'HexagonLayer',\n    UK_ACCIDENTS_DATA,\n    get_position='[lng, lat]',\n    auto_highlight=True,\n    elevation_scale=50,\n    pickable=True,\n    elevation_range=[0, 3000],\n    extruded=True,\n    coverage=1)\n\n# Set the viewport location\nview_state = pdk.ViewState(\n    longitude=-1.415,\n    latitude=52.2323,\n    zoom=6,\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,\n    bearing=-27.36)\n\n# Combined all of it and render a viewport\nr = pdk.Deck(\n    layers=[layer],\n    initial_view_state=view_state,\n    tooltip=True\n)\nr.show()\n\n\n\n\n\nUK_ACCIDENTS_DATA = 'https://raw.githubusercontent.com/uber-common/deck.gl-data/master/examples/3d-heatmap/heatmap-data.csv'\n\nuk_data = pd.read_csv(UK_ACCIDENTS_DATA)\n\n\nuk_data.head()\n\n\n\n\n\n  \n    \n      \n      lng\n      lat\n    \n  \n  \n    \n      0\n      -0.198465\n      51.505538\n    \n    \n      1\n      -0.178838\n      51.491836\n    \n    \n      2\n      -0.205590\n      51.514910\n    \n    \n      3\n      -0.208327\n      51.514952\n    \n    \n      4\n      -0.206022\n      51.496572\n    \n  \n\n\n\n\n\n\n미국 택시 데이터 시각화\n\npip install pandas-gbq -U\n\nCollecting pandas-gbq\n  Downloading pandas_gbq-0.19.2-py2.py3-none-any.whl (25 kB)\nCollecting db-dtypes<2.0.0,>=1.0.4\n  Downloading db_dtypes-1.1.1-py2.py3-none-any.whl (14 kB)\nCollecting pyarrow>=3.0.0\n  Downloading pyarrow-12.0.0-cp38-cp38-win_amd64.whl (21.5 MB)\n     -------------------------------------- 21.5/21.5 MB 500.1 kB/s eta 0:00:00\nCollecting google-auth-oauthlib>=0.7.0\n  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nCollecting google-cloud-bigquery-storage<3.0.0dev,>=2.16.2\n  Downloading google_cloud_bigquery_storage-2.19.1-py2.py3-none-any.whl (190 kB)\n     ------------------------------------ 190.1/190.1 kB 501.0 kB/s eta 0:00:00\nRequirement already satisfied: pandas>=1.1.4 in c:\\python\\lib\\site-packages (from pandas-gbq) (1.5.2)\nCollecting google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5\n  Downloading google_cloud_bigquery-3.10.0-py2.py3-none-any.whl (218 kB)\n     ------------------------------------ 218.4/218.4 kB 459.4 kB/s eta 0:00:00\nCollecting pydata-google-auth>=1.5.0\n  Downloading pydata_google_auth-1.8.0-py2.py3-none-any.whl (14 kB)\nCollecting google-api-core<3.0.0dev,>=2.10.2\n  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n     ------------------------------------ 120.3/120.3 kB 542.4 kB/s eta 0:00:00\nRequirement already satisfied: setuptools in c:\\python\\lib\\site-packages (from pandas-gbq) (52.0.0.post20210125)\nRequirement already satisfied: numpy>=1.16.6 in c:\\python\\lib\\site-packages (from pandas-gbq) (1.23.5)\nCollecting google-auth>=2.13.0\n  Downloading google_auth-2.18.1-py2.py3-none-any.whl (178 kB)\n     ------------------------------------ 178.9/178.9 kB 431.2 kB/s eta 0:00:00\nRequirement already satisfied: packaging>=17.0 in c:\\python\\lib\\site-packages (from db-dtypes<2.0.0,>=1.0.4->pandas-gbq) (21.3)\nCollecting googleapis-common-protos<2.0dev,>=1.56.2\n  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n     ------------------------------------ 223.6/223.6 kB 471.0 kB/s eta 0:00:00\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\python\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (2.25.1)\nCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n  Downloading protobuf-4.23.1-cp38-cp38-win_amd64.whl (422 kB)\n     ------------------------------------ 422.5/422.5 kB 560.9 kB/s eta 0:00:00\nCollecting pyasn1-modules>=0.2.1\n  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n     ------------------------------------ 181.3/181.3 kB 781.1 kB/s eta 0:00:00\nRequirement already satisfied: urllib3<2.0 in c:\\python\\lib\\site-packages (from google-auth>=2.13.0->pandas-gbq) (1.26.4)\nCollecting cachetools<6.0,>=2.0.0\n  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\nRequirement already satisfied: six>=1.9.0 in c:\\python\\lib\\site-packages (from google-auth>=2.13.0->pandas-gbq) (1.15.0)\nCollecting rsa<5,>=3.1.4\n  Downloading rsa-4.9-py3-none-any.whl (34 kB)\nCollecting requests-oauthlib>=0.7.0\n  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in c:\\python\\lib\\site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (2.8.1)\nCollecting proto-plus<2.0.0dev,>=1.15.0\n  Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n     -------------------------------------- 47.9/47.9 kB 597.7 kB/s eta 0:00:00\nCollecting google-resumable-media<3.0dev,>=0.6.0\n  Downloading google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB)\n     -------------------------------------- 77.7/77.7 kB 865.4 kB/s eta 0:00:00\nCollecting grpcio<2.0dev,>=1.47.0\n  Downloading grpcio-1.54.2-cp38-cp38-win_amd64.whl (4.1 MB)\n     ---------------------------------------- 4.1/4.1 MB 677.5 kB/s eta 0:00:00\nCollecting google-cloud-core<3.0.0dev,>=1.6.0\n  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\nRequirement already satisfied: pytz>=2020.1 in c:\\python\\lib\\site-packages (from pandas>=1.1.4->pandas-gbq) (2021.1)\nCollecting grpcio-status<2.0dev,>=1.33.2\n  Downloading grpcio_status-1.54.2-py3-none-any.whl (5.1 kB)\nCollecting google-crc32c<2.0dev,>=1.0\n  Downloading google_crc32c-1.5.0-cp38-cp38-win_amd64.whl (27 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python\\lib\\site-packages (from packaging>=17.0->db-dtypes<2.0.0,>=1.0.4->pandas-gbq) (2.4.7)\nCollecting pyasn1<0.6.0,>=0.4.6\n  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n     -------------------------------------- 83.9/83.9 kB 471.9 kB/s eta 0:00:00\nRequirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (2022.9.24)\nRequirement already satisfied: idna<3,>=2.5 in c:\\python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in c:\\python\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (4.0.0)\nCollecting oauthlib>=3.0.0\n  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n     ------------------------------------ 151.7/151.7 kB 282.9 kB/s eta 0:00:00\nInstalling collected packages: pyasn1, pyarrow, protobuf, oauthlib, grpcio, google-crc32c, cachetools, rsa, requests-oauthlib, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, grpcio-status, google-auth, db-dtypes, google-auth-oauthlib, google-api-core, pydata-google-auth, google-cloud-core, google-cloud-bigquery-storage, google-cloud-bigquery, pandas-gbq\nSuccessfully installed cachetools-5.3.0 db-dtypes-1.1.1 google-api-core-2.11.0 google-auth-2.18.1 google-auth-oauthlib-1.0.0 google-cloud-bigquery-3.10.0 google-cloud-bigquery-storage-2.19.1 google-cloud-core-2.3.2 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.59.0 grpcio-1.54.2 grpcio-status-1.54.2 oauthlib-3.2.2 pandas-gbq-0.19.2 proto-plus-1.22.2 protobuf-4.23.1 pyarrow-12.0.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pydata-google-auth-1.8.0 requests-oauthlib-1.3.1 rsa-4.9\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n[notice] A new release of pip available: 22.3.1 -> 23.1.2\n[notice] To update, run: python.exe -m pip install --upgrade pip\n\n\n\nimport pandas as pd\nimport pydata_google_auth\n\nSCOPES = [\n  'https://www.googleapis.com/auth/cloud-platform',\n  'https://www.googleapis.com/auth/drive',\n  'https://www.googleapis.com/auth/bigquery'\n]\n\ncredentials = pydata_google_auth.get_user_credentials(\nSCOPES, auth_local_webserver=True)\n\nModuleNotFoundError: No module named 'pydata_google_auth'\n\n\n\nquery = \"\"\"\nSELECT \n    *\nFROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015` \nWHERE EXTRACT(MONTH from pickup_datetime) = 1\nLIMIT 10000\n\"\"\"\n\n\n#%%time\ntaxi_df = pd.read_gbq(query=query, dialect='standard', project_id='persuasive-zoo-147513', credentials=credentials)\n\nWall time: 6.7 s\n\n\n\ntaxi_df\n\n\n\n\n\n  \n    \n      \n      vendor_id\n      pickup_datetime\n      dropoff_datetime\n      passenger_count\n      trip_distance\n      pickup_longitude\n      pickup_latitude\n      rate_code\n      store_and_fwd_flag\n      dropoff_longitude\n      dropoff_latitude\n      payment_type\n      fare_amount\n      extra\n      mta_tax\n      tip_amount\n      tolls_amount\n      imp_surcharge\n      total_amount\n    \n  \n  \n    \n      0\n      1\n      2015-01-02 16:26:22\n      2015-01-02 16:51:10\n      2\n      2.50\n      -73.993172\n      40.762901\n      <NA>\n      N\n      -73.962097\n      40.763584\n      1\n      15.7\n      1.0\n      0.5\n      3.50\n      0.0\n      0.0\n      21.00\n    \n    \n      1\n      1\n      2015-01-16 17:13:00\n      2015-01-16 17:16:10\n      1\n      0.40\n      -73.961601\n      40.771229\n      <NA>\n      N\n      -73.959419\n      40.775253\n      1\n      4.0\n      1.0\n      0.5\n      1.15\n      0.0\n      0.3\n      6.95\n    \n    \n      2\n      2\n      2015-01-24 04:25:01\n      2015-01-24 04:41:43\n      2\n      4.64\n      -74.000595\n      40.737167\n      <NA>\n      N\n      -73.995499\n      40.680763\n      1\n      16.0\n      0.5\n      0.5\n      19.50\n      0.0\n      0.3\n      36.80\n    \n    \n      3\n      2\n      2015-01-30 14:29:58\n      2015-01-30 15:27:13\n      1\n      18.39\n      -73.989914\n      40.729706\n      <NA>\n      N\n      -73.782310\n      40.644180\n      1\n      52.0\n      0.0\n      0.5\n      5.50\n      0.0\n      0.3\n      58.30\n    \n    \n      4\n      1\n      2015-01-14 21:24:13\n      2015-01-14 21:25:55\n      1\n      0.50\n      -73.954849\n      40.773220\n      <NA>\n      N\n      -73.959801\n      40.769432\n      1\n      3.5\n      0.5\n      0.5\n      0.96\n      0.0\n      0.3\n      5.76\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9995\n      1\n      2015-01-24 22:49:17\n      2015-01-24 22:56:20\n      2\n      0.70\n      -74.004578\n      40.724056\n      <NA>\n      N\n      -74.006958\n      40.732971\n      1\n      6.0\n      0.5\n      0.5\n      1.80\n      0.0\n      0.3\n      9.10\n    \n    \n      9996\n      2\n      2015-01-21 18:20:27\n      2015-01-21 18:30:16\n      6\n      1.14\n      -74.000038\n      40.748291\n      <NA>\n      N\n      -73.990608\n      40.738071\n      1\n      8.0\n      1.0\n      0.5\n      1.80\n      0.0\n      0.3\n      11.60\n    \n    \n      9997\n      2\n      2015-01-09 20:35:23\n      2015-01-09 20:43:55\n      1\n      2.00\n      -73.974876\n      40.748661\n      <NA>\n      N\n      -73.980530\n      40.768021\n      1\n      8.5\n      0.5\n      0.5\n      1.80\n      0.0\n      0.3\n      11.60\n    \n    \n      9998\n      2\n      2015-01-31 22:01:33\n      2015-01-31 22:11:22\n      1\n      1.42\n      -73.984718\n      40.728447\n      <NA>\n      N\n      -73.975380\n      40.745564\n      1\n      8.5\n      0.5\n      0.5\n      1.80\n      0.0\n      0.3\n      11.60\n    \n    \n      9999\n      2\n      2015-01-13 13:40:14\n      2015-01-13 13:51:28\n      5\n      1.57\n      -73.981232\n      40.747498\n      <NA>\n      N\n      -73.998116\n      40.733883\n      1\n      9.0\n      0.0\n      0.5\n      1.80\n      0.0\n      0.3\n      11.60\n    \n  \n\n10000 rows × 19 columns\n\n\n\n\n\nGridLayer\n\n10만개 데이터\n\n\narc_layer = pdk.Layer(\n    'GridLayer',\n    taxi_df,\n    get_position='[pickup_longitude, pickup_latitude]',\n    pickable=True, \n    auto_highlight=True,\n    tooltip=True\n)\n\nnyc_center = [-73.9808, 40.7648] \nview_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9)\n\nr = pdk.Deck(layers=[arc_layer], initial_view_state=view_state)\nr.show()\n\n\n\n\n\n\nArc Layer\n\nzip_code_query = \"\"\"\nWITH base_data AS \n(\n  SELECT \n    nyc_taxi.*, \n    pickup.zip_code as pickup_zip_code,\n    pickup.internal_point_lat as pickup_zip_code_lat,\n    pickup.internal_point_lon as pickup_zip_code_lon,\n    dropoff.zip_code as dropoff_zip_code,\n    dropoff.internal_point_lat as dropoff_zip_code_lat,\n    dropoff.internal_point_lon as dropoff_zip_code_lon\n  FROM (\n    SELECT *\n    FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`\n    WHERE \n        EXTRACT(MONTH from pickup_datetime) = 1\n        and pickup_latitude <= 90 and pickup_latitude >= -90\n        and dropoff_latitude <= 90 and dropoff_latitude >= -90\n    ) AS nyc_taxi\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY'\n    ) AS pickup \n  ON ST_CONTAINS(pickup.zip_code_geom, st_geogpoint(pickup_longitude, pickup_latitude))\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY' \n    ) AS dropoff\n  ON ST_CONTAINS(dropoff.zip_code_geom, st_geogpoint(dropoff_longitude, dropoff_latitude))\n  \n)\n\nSELECT \n  *\nFROM base_data \nlimit 10000\n\"\"\"\n\n\n%%time\ntaxi_df_by_zipcode = pd.read_gbq(query=zip_code_query, dialect='standard', project_id='persuasive-zoo-147513', credentials=credentials)\n\nWall time: 36.5 s\n\n\n\ntaxi_df_by_zipcode.head(3)\n\n\n\n\n\n  \n    \n      \n      vendor_id\n      pickup_datetime\n      dropoff_datetime\n      passenger_count\n      trip_distance\n      pickup_longitude\n      pickup_latitude\n      rate_code\n      store_and_fwd_flag\n      dropoff_longitude\n      ...\n      tip_amount\n      tolls_amount\n      imp_surcharge\n      total_amount\n      pickup_zip_code\n      pickup_zip_code_lat\n      pickup_zip_code_lon\n      dropoff_zip_code\n      dropoff_zip_code_lat\n      dropoff_zip_code_lon\n    \n  \n  \n    \n      0\n      2\n      2015-01-20 09:57:47\n      2015-01-20 10:14:24\n      1\n      2.54\n      -74.014793\n      40.714111\n      <NA>\n      N\n      -73.991997\n      ...\n      2.5\n      0.0\n      0.3\n      15.8\n      10280\n      40.709073\n      -74.016423\n      10003\n      40.731829\n      -73.989181\n    \n    \n      1\n      1\n      2015-01-15 04:50:46\n      2015-01-15 05:01:29\n      1\n      2.30\n      -74.015938\n      40.710976\n      <NA>\n      N\n      -73.996552\n      ...\n      0.0\n      0.0\n      0.3\n      11.3\n      10280\n      40.709073\n      -74.016423\n      10011\n      40.742043\n      -74.000620\n    \n    \n      2\n      2\n      2015-01-22 09:31:00\n      2015-01-22 09:50:42\n      1\n      4.13\n      -73.989738\n      40.701981\n      <NA>\n      N\n      -74.007828\n      ...\n      2.0\n      0.0\n      0.3\n      19.8\n      11201\n      40.693700\n      -73.989859\n      10011\n      40.742043\n      -74.000620\n    \n  \n\n3 rows × 25 columns\n\n\n\n\n\narc_layer = pdk.Layer(\n    'ArcLayer',\n    taxi_df_by_zipcode,\n    get_source_position='[pickup_zip_code_lon, pickup_zip_code_lat]',\n    get_target_position='[dropoff_zip_code_lon, dropoff_zip_code_lat]',\n    get_source_color='[255, 255, 120]', \n    get_target_color='[255, 0, 0]',\n    get_widht='elevationValue',\n    pickable=True, \n    auto_highlight=True,\n)\n\nnyc_center = [-73.9808, 40.7648] \nview_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9)\n\nr = pdk.Deck(layers=[arc_layer], initial_view_state=view_state)\nr.show()\n\n\n\n\n\n\nAggregate\n\nagg_query = \"\"\"\nWITH base_data AS \n(\n  SELECT \n    nyc_taxi.*, \n    pickup.zip_code as pickup_zip_code,\n    pickup.internal_point_lat as pickup_zip_code_lat,\n    pickup.internal_point_lon as pickup_zip_code_lon,\n    dropoff.zip_code as dropoff_zip_code,\n    dropoff.internal_point_lat as dropoff_zip_code_lat,\n    dropoff.internal_point_lon as dropoff_zip_code_lon\n  FROM (\n    SELECT *\n    FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`\n    WHERE \n        EXTRACT(MONTH from pickup_datetime) = 1\n        and pickup_latitude <= 90 and pickup_latitude >= -90\n        and dropoff_latitude <= 90 and dropoff_latitude >= -90\n    ) AS nyc_taxi\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY'\n    ) AS pickup \n  ON ST_CONTAINS(pickup.zip_code_geom, st_geogpoint(pickup_longitude, pickup_latitude))\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY' \n    ) AS dropoff\n  ON ST_CONTAINS(dropoff.zip_code_geom, st_geogpoint(dropoff_longitude, dropoff_latitude))\n  \n)\n\nSELECT \n  pickup_zip_code,\n  pickup_zip_code_lat,\n  pickup_zip_code_lon,\n  dropoff_zip_code,\n  dropoff_zip_code_lat,\n  dropoff_zip_code_lon,\n  COUNT(*) AS cnt\nFROM base_data \nGROUP BY 1,2,3,4,5,6\nlimit 10000\n\"\"\"\n\n\n%%time\nagg_df = pd.read_gbq(query=agg_query, dialect='standard', project_id='persuasive-zoo-147513', credentials=credentials)\n\nWall time: 15.9 s\n\n\n\nagg_df.head()\n\n\n\n\n\n  \n    \n      \n      pickup_zip_code\n      pickup_zip_code_lat\n      pickup_zip_code_lon\n      dropoff_zip_code\n      dropoff_zip_code_lat\n      dropoff_zip_code_lon\n      cnt\n    \n  \n  \n    \n      0\n      11693\n      40.590916\n      -73.809715\n      11414\n      40.657604\n      -73.844804\n      1\n    \n    \n      1\n      10040\n      40.858314\n      -73.930494\n      10040\n      40.858314\n      -73.930494\n      139\n    \n    \n      2\n      10473\n      40.818690\n      -73.858474\n      10030\n      40.818267\n      -73.942856\n      1\n    \n    \n      3\n      10451\n      40.820454\n      -73.925066\n      10031\n      40.825288\n      -73.950045\n      93\n    \n    \n      4\n      11209\n      40.621993\n      -74.030134\n      11228\n      40.616698\n      -74.013066\n      28\n    \n  \n\n\n\n\n\nagg_df = agg_df.sort_values('cnt', ascending=False)\n\n\nagg_df = agg_df[:100]\n\n\n\narc_layer = pdk.Layer(\n    'ArcLayer',\n    agg_df,\n    get_source_position='[pickup_zip_code_lon, pickup_zip_code_lat]',\n    get_target_position='[dropoff_zip_code_lon, dropoff_zip_code_lat]',\n    get_source_color='[255, 255, 120]', \n    get_target_color='[255, 0, 0]',\n    width_units='meters',\n    get_width=\"1+10*cnt/500\",\n    pickable=True, \n    auto_highlight=True,\n)\n\nnyc_center = [-73.9808, 40.7648] \nview_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9)\n\nr = pdk.Deck(layers=[arc_layer], initial_view_state=view_state,\n             tooltip={\n                 'html': '<b>count:</b> {cnt}',\n                 'style': {\n                     'color': 'white'\n                 }\n             }\n            )\nr.show()\n\n\n\n\n\n\n요일별 위젯\n\nagg_query2 = \"\"\"\nWITH base_data AS \n(\n  SELECT \n    nyc_taxi.*, \n    pickup.zip_code as pickup_zip_code,\n    pickup.internal_point_lat as pickup_zip_code_lat,\n    pickup.internal_point_lon as pickup_zip_code_lon,\n    dropoff.zip_code as dropoff_zip_code,\n    dropoff.internal_point_lat as dropoff_zip_code_lat,\n    dropoff.internal_point_lon as dropoff_zip_code_lon\n  FROM (\n    SELECT *\n    FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`\n    WHERE \n        EXTRACT(MONTH from pickup_datetime) = 1\n        and pickup_latitude <= 90 and pickup_latitude >= -90\n        and dropoff_latitude <= 90 and dropoff_latitude >= -90\n    LIMIT 100000\n    ) AS nyc_taxi\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY'\n    ) AS pickup \n  ON ST_CONTAINS(pickup.zip_code_geom, st_geogpoint(pickup_longitude, pickup_latitude))\n  JOIN (\n    SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon \n    FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE state_code='NY' \n    ) AS dropoff\n  ON ST_CONTAINS(dropoff.zip_code_geom, st_geogpoint(dropoff_longitude, dropoff_latitude))\n  \n)\n\nSELECT \n  CAST(format_datetime('%u', pickup_datetime) AS INT64) -1 AS weekday,\n  pickup_zip_code,\n  pickup_zip_code_lat,\n  pickup_zip_code_lon,\n  dropoff_zip_code,\n  dropoff_zip_code_lat,\n  dropoff_zip_code_lon,\n  COUNT(*) AS cnt\nFROM base_data \nGROUP BY 1,2,3,4,5,6,7\n\"\"\"\n\n\n%%time\nagg_df2 = pd.read_gbq(query=agg_query2, dialect='standard', project_id='persuasive-zoo-147513', credentials=credentials)\n\nWall time: 7.41 s\n\n\n\nagg_df2.head()\n\n\n\n\n\n  \n    \n      \n      weekday\n      pickup_zip_code\n      pickup_zip_code_lat\n      pickup_zip_code_lon\n      dropoff_zip_code\n      dropoff_zip_code_lat\n      dropoff_zip_code_lon\n      cnt\n    \n  \n  \n    \n      0\n      4\n      11214\n      40.599148\n      -73.996090\n      10035\n      40.795458\n      -73.929570\n      1\n    \n    \n      1\n      6\n      10171\n      40.755899\n      -73.973858\n      11430\n      40.646809\n      -73.786169\n      2\n    \n    \n      2\n      5\n      10461\n      40.847394\n      -73.840583\n      10475\n      40.874375\n      -73.823656\n      1\n    \n    \n      3\n      0\n      10172\n      40.755273\n      -73.974315\n      10065\n      40.764628\n      -73.963144\n      1\n    \n    \n      4\n      6\n      10162\n      40.769308\n      -73.949924\n      11430\n      40.646809\n      -73.786169\n      1\n    \n  \n\n\n\n\n\ndefault_data = agg_df2[agg_df2['weekday'] == 0].to_dict(orient='records')\n\n\narc_layer = pdk.Layer(\n    'ArcLayer',\n    default_data,\n    get_source_position='[pickup_zip_code_lon, pickup_zip_code_lat]',\n    get_target_position='[dropoff_zip_code_lon, dropoff_zip_code_lat]',\n    get_source_color='[255, 255, 120]', \n    get_target_color='[255, 0, 0]',\n    width_units='meters',\n    get_width=\"1+10*cnt/500\",\n    pickable=True, \n    auto_highlight=True,\n)\n\nnyc_center = [-73.9808, 40.7648] \nview_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9)\n\nr = pdk.Deck(layers=[arc_layer], initial_view_state=view_state,\n             tooltip={\n                 'html': '<b>count:</b> {cnt}',\n                 'style': {\n                     'color': 'white'\n                 }\n             }\n            )\nr.show()\n\n\n\n\n\n# Widget 슬라이더 생성\nimport ipywidgets as widgets\nfrom IPython.display import display\nslider = widgets.IntSlider(0, min=0, max=6, step=1)\n\n# Widget에서 사용할 함수 정의 \ndef on_change(v):\n    results = agg_df2[agg_df2['weekday'] == slider.value].to_dict(orient='records')\n    arc_layer.data = results\n    r.update()\n\n# Deck과 슬라이더 연결\nslider.observe(on_change, names='value')\ndisplay(slider)"
  },
  {
    "objectID": "Data/5.Proximity Analysis.html",
    "href": "Data/5.Proximity Analysis.html",
    "title": "5.Proximity Analysis",
    "section": "",
    "text": "Introduction\nIn this tutorial, you’ll explore several techniques for proximity analysis. In particular, you’ll learn how to do such things as:\nmeasure the distance between points on a map, and select all points within some radius of a feature.\n\nimport folium\nfrom folium import Marker, GeoJson\nfrom folium.plugins import HeatMap\n\nimport pandas as pd\nimport geopandas as gpd\n\n\nreleases = gpd.read_file(\"D:/archive (1)/toxic_release_pennsylvania/toxic_release_pennsylvania/toxic_release_pennsylvania.shp\") \nreleases.head()\n\n\n\n\n\n  \n    \n      \n      YEAR\n      CITY\n      COUNTY\n      ST\n      LATITUDE\n      LONGITUDE\n      CHEMICAL\n      UNIT_OF_ME\n      TOTAL_RELE\n      geometry\n    \n  \n  \n    \n      0\n      2016\n      PHILADELPHIA\n      PHILADELPHIA\n      PA\n      40.005901\n      -75.072103\n      FORMIC ACID\n      Pounds\n      0.160\n      POINT (2718560.227 256380.179)\n    \n    \n      1\n      2016\n      PHILADELPHIA\n      PHILADELPHIA\n      PA\n      39.920120\n      -75.146410\n      ETHYLENE GLYCOL\n      Pounds\n      13353.480\n      POINT (2698674.606 224522.905)\n    \n    \n      2\n      2016\n      PHILADELPHIA\n      PHILADELPHIA\n      PA\n      40.023880\n      -75.220450\n      CERTAIN GLYCOL ETHERS\n      Pounds\n      104.135\n      POINT (2676833.394 261701.856)\n    \n    \n      3\n      2016\n      PHILADELPHIA\n      PHILADELPHIA\n      PA\n      39.913540\n      -75.198890\n      LEAD COMPOUNDS\n      Pounds\n      1730.280\n      POINT (2684030.004 221697.388)\n    \n    \n      4\n      2016\n      PHILADELPHIA\n      PHILADELPHIA\n      PA\n      39.913540\n      -75.198890\n      BENZENE\n      Pounds\n      39863.290\n      POINT (2684030.004 221697.388)\n    \n  \n\n\n\n\n\nstations = gpd.read_file(\"D:/archive (1)/PhillyHealth_Air_Monitoring_Stations/PhillyHealth_Air_Monitoring_Stations/PhillyHealth_Air_Monitoring_Stations.shp\") \nstations.head()\n\n\n\n\n\n  \n    \n      \n      SITE_NAME\n      ADDRESS\n      BLACK_CARB\n      ULTRAFINE_\n      CO\n      SO2\n      OZONE\n      NO2\n      NOY_NO\n      PM10\n      ...\n      PAMS_VOC\n      TSP_11101\n      TSP_METALS\n      TSP_LEAD\n      TOXICS_TO1\n      MET\n      COMMUNITY_\n      LATITUDE\n      LONGITUDE\n      geometry\n    \n  \n  \n    \n      0\n      LAB\n      1501 East Lycoming Avenue\n      N\n      N\n      Y\n      N\n      Y\n      Y\n      Y\n      N\n      ...\n      Y\n      N\n      Y\n      N\n      y\n      N\n      N\n      40.008606\n      -75.097624\n      POINT (2711384.641 257149.310)\n    \n    \n      1\n      ROX\n      Eva and Dearnley Streets\n      N\n      N\n      N\n      N\n      N\n      N\n      N\n      N\n      ...\n      N\n      N\n      Y\n      N\n      Y\n      N\n      N\n      40.050461\n      -75.236966\n      POINT (2671934.290 271248.900)\n    \n    \n      2\n      NEA\n      Grant Avenue and Ashton Street\n      N\n      N\n      N\n      N\n      Y\n      N\n      N\n      N\n      ...\n      N\n      N\n      N\n      N\n      N\n      Y\n      N\n      40.072073\n      -75.013128\n      POINT (2734326.638 280980.247)\n    \n    \n      3\n      CHS\n      500 South Broad Street\n      N\n      N\n      N\n      N\n      N\n      N\n      N\n      N\n      ...\n      N\n      N\n      Y\n      N\n      Y\n      N\n      N\n      39.944510\n      -75.165442\n      POINT (2693078.580 233247.101)\n    \n    \n      4\n      NEW\n      2861 Lewis Street\n      N\n      N\n      Y\n      Y\n      Y\n      N\n      Y\n      Y\n      ...\n      N\n      Y\n      N\n      Y\n      N\n      Y\n      N\n      39.991688\n      -75.080378\n      POINT (2716399.773 251134.976)\n    \n  \n\n5 rows × 24 columns\n\n\n\n\n\nMeasuring distance\nTo measure distances between points from two different GeoDataFrames, we first have to make sure that they use the same coordinate reference system (CRS). Thankfully, this is the case here, where both use EPSG 2272.\n\nprint(stations.crs)\nprint(releases.crs)\n#좌표계가 같은지 확인 해야한다\n#다르면 crs를 맞춰야한다\n\n#미국은 fit를 사용하기 때문에 주의 해야함\n\nEPSG:2272\nEPSG:2272\n\n\n\n# Select one release incident in particular\nrecent_release = releases.iloc[360]\n\n# Measure distance from release to each station\n\n#distance :거리 계산하는 함수\n\ndistances = stations.geometry.distance(recent_release.geometry)\ndistances   #station과 recent_releas 사이의 거리 계산 해준것\n#직선거리를 구한것 \n\n0     44778.509761\n1     51006.456589\n2     77744.509207\n3     14672.170878\n4     43753.554393\n5      4711.658655\n6     23197.430858\n7     12072.823097\n8     79081.825506\n9      3780.623591\n10    27577.474903\n11    19818.381002\ndtype: float64\n\n\n\nprint('Mean distance to monitoring stations: {} feet'.format(distances.mean()))\n\nMean distance to monitoring stations: 33516.28487007786 feet\n\n\n\nprint('Closest monitoring station ({} feet):'.format(distances.min()))\nprint(stations.iloc[distances.idxmin()][[\"ADDRESS\", \"LATITUDE\", \"LONGITUDE\"]])\n\nClosest monitoring station (3780.623590556444 feet):\nADDRESS      3100 Penrose Ferry Road\nLATITUDE                    39.91279\nLONGITUDE                 -75.185448\nName: 9, dtype: object\n\n\n\n\nCreating a buffer\nIf we want to understand all points on a map that are some radius away from a point, the simplest way is to create a buffer.\nThe code cell below creates a GeoSeries two_mile_buffer containing 12 different Polygon objects. Each polygon is a buffer of 2 miles (or, 2*5280 feet) around a different air monitoring station.\n\n#일정한 원을 만들어 주는것\ntwo_mile_buffer = stations.geometry.buffer(2*5280) #마일단위임 / 1마일 = 5280\ntwo_mile_buffer.info()\n#버퍼를 만들어서 폴리곤이 되는것\n\n<class 'geopandas.geoseries.GeoSeries'>\nRangeIndex: 12 entries, 0 to 11\nSeries name: None\nNon-Null Count  Dtype   \n--------------  -----   \n12 non-null     geometry\ndtypes: geometry(1)\nmemory usage: 228.0 bytes\n\n\n\n# Create map with release incidents and monitoring stations\nm = folium.Map(location=[39.9526,-75.1652], zoom_start=11)\nHeatMap(data=releases[['LATITUDE', 'LONGITUDE']], radius=15).add_to(m) #릴리스 포인트 찍어야함 \nfor idx, row in stations.iterrows(): #마커로 찍기 \n    Marker([row['LATITUDE'], row['LONGITUDE']]).add_to(m)\n    \n# Plot each polygon on the map\n#two_mile_buffer에는 폴리곤데이터가 들어가는 것임\nGeoJson(two_mile_buffer).add_to(m)\n\n# .to_crs(epsg=4326) 위에 좌표계랑 똑같아서 굳이 안바꿔도됨\n# Show the map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n#유해물질이 역의 안쪽에 있는지 밖에 있는지 확인\n\n# Turn group of polygons into single multipolygon\nmy_union = two_mile_buffer.geometry.unary_union #하나로 합치기\nprint('Type:', type(my_union))\n\n# Show the MultiPolygon object\nmy_union\n\nType: <class 'shapely.geometry.multipolygon.MultiPolygon'>\n\n\n\n\n\n\n# The closest station is less than two miles away\n#포인트가 포함하고 있는지 없는지 확인하는것 \n\nmy_union.contains(releases.iloc[360].geometry)\n\nTrue\n\n\n\n# The closest station is more than two miles away\n#합치지 않고 하는거\nmy_union.contains(releases.iloc[358].geometry)\n\nFalse"
  },
  {
    "objectID": "Data/2. Coordinate Reference System.html",
    "href": "Data/2. Coordinate Reference System.html",
    "title": "2. Coordinate Reference Systems",
    "section": "",
    "text": "Introduction\n\n\n\nimage.png\n\n\n\nimport geopandas as gpd\nimport pandas as pd\n\n\n\nSetting the CRS\n\n# Load a GeoDataFrame containing regions in Ghana\nregions = gpd.read_file(\"D:/archive (1)/ghana/ghana/Regions/Map_of_Regions_in_Ghana.shp\")\nprint(regions.crs)\n\nEPSG:32630\n\n\n\n# Create a DataFrame with health facilities in Ghana\nfacilities_df = pd.read_csv(\"D:/archive (1)/ghana/ghana/health_facilities.csv\")\n\n# Convert the DataFrame to a GeoDataFrame\nfacilities = gpd.GeoDataFrame(facilities_df, geometry=gpd.points_from_xy(facilities_df.Longitude, facilities_df.Latitude))\n\n# Set the coordinate reference system (CRS) to EPSG 4326\nfacilities.crs = {'init': 'epsg:4326'}\n\n# View the first five rows of the GeoDataFrame\nfacilities.head()\n\nc:\\Users\\jiyeo\\anaconda3\\envs\\dayeon\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      Region\n      District\n      FacilityName\n      Type\n      Town\n      Ownership\n      Latitude\n      Longitude\n      geometry\n    \n  \n  \n    \n      0\n      Ashanti\n      Offinso North\n      A.M.E Zion Clinic\n      Clinic\n      Afrancho\n      CHAG\n      7.40801\n      -1.96317\n      POINT (-1.96317 7.40801)\n    \n    \n      1\n      Ashanti\n      Bekwai Municipal\n      Abenkyiman Clinic\n      Clinic\n      Anwiankwanta\n      Private\n      6.46312\n      -1.58592\n      POINT (-1.58592 6.46312)\n    \n    \n      2\n      Ashanti\n      Adansi North\n      Aboabo Health Centre\n      Health Centre\n      Aboabo No 2\n      Government\n      6.22393\n      -1.34982\n      POINT (-1.34982 6.22393)\n    \n    \n      3\n      Ashanti\n      Afigya-Kwabre\n      Aboabogya Health Centre\n      Health Centre\n      Aboabogya\n      Government\n      6.84177\n      -1.61098\n      POINT (-1.61098 6.84177)\n    \n    \n      4\n      Ashanti\n      Kwabre\n      Aboaso Health Centre\n      Health Centre\n      Aboaso\n      Government\n      6.84177\n      -1.61098\n      POINT (-1.61098 6.84177)\n    \n  \n\n\n\n\n\n\nRe-projecting\n\n# Create a map\nax = regions.plot(figsize=(8,8), color='whitesmoke', linestyle=':', edgecolor='black')\nfacilities.to_crs(epsg=32630).plot(markersize=1, ax=ax)\n\n<Axes: >\n\n\n\n\n\n\n# The \"Latitude\" and \"Longitude\" columns are unchanged\nfacilities.to_crs(epsg=32630).head()\n\n\n\n\n\n  \n    \n      \n      Region\n      District\n      FacilityName\n      Type\n      Town\n      Ownership\n      Latitude\n      Longitude\n      geometry\n    \n  \n  \n    \n      0\n      Ashanti\n      Offinso North\n      A.M.E Zion Clinic\n      Clinic\n      Afrancho\n      CHAG\n      7.40801\n      -1.96317\n      POINT (614422.662 818986.851)\n    \n    \n      1\n      Ashanti\n      Bekwai Municipal\n      Abenkyiman Clinic\n      Clinic\n      Anwiankwanta\n      Private\n      6.46312\n      -1.58592\n      POINT (656373.863 714616.547)\n    \n    \n      2\n      Ashanti\n      Adansi North\n      Aboabo Health Centre\n      Health Centre\n      Aboabo No 2\n      Government\n      6.22393\n      -1.34982\n      POINT (682573.395 688243.477)\n    \n    \n      3\n      Ashanti\n      Afigya-Kwabre\n      Aboabogya Health Centre\n      Health Centre\n      Aboabogya\n      Government\n      6.84177\n      -1.61098\n      POINT (653484.490 756478.812)\n    \n    \n      4\n      Ashanti\n      Kwabre\n      Aboaso Health Centre\n      Health Centre\n      Aboaso\n      Government\n      6.84177\n      -1.61098\n      POINT (653484.490 756478.812)\n    \n  \n\n\n\n\n\n# Change the CRS to EPSG 4326\nregions.to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\").head()\n\n\n\n\n\n  \n    \n      \n      Region\n      geometry\n    \n  \n  \n    \n      0\n      Ashanti\n      POLYGON ((-1.30985 7.62302, -1.30786 7.62198, ...\n    \n    \n      1\n      Brong Ahafo\n      POLYGON ((-2.54567 8.76089, -2.54473 8.76071, ...\n    \n    \n      2\n      Central\n      POLYGON ((-2.06723 6.29473, -2.06658 6.29420, ...\n    \n    \n      3\n      Eastern\n      POLYGON ((-0.21751 7.21009, -0.21747 7.20993, ...\n    \n    \n      4\n      Greater Accra\n      POLYGON ((0.23456 6.10986, 0.23484 6.10974, 0....\n    \n  \n\n\n\n\n\n\nAttributes of geometric objects\n\n# Get the x-coordinate of each point\nfacilities.geometry.head().x\n\n\n# Calculate the area (in square meters) of each polygon in the GeoDataFrame \nregions.loc[:, \"AREA\"] = regions.geometry.area / 10**6\n\nprint(\"Area of Ghana: {} square kilometers\".format(regions.AREA.sum()))\nprint(\"CRS:\", regions.crs)\nregions.head()\n\nArea of Ghana: 239584.5760055668 square kilometers\nCRS: epsg:32630\n\n\n\n\n\n\n  \n    \n      \n      Region\n      geometry\n      AREA\n    \n  \n  \n    \n      0\n      Ashanti\n      POLYGON ((686446.075 842986.894, 686666.193 84...\n      24379.017777\n    \n    \n      1\n      Brong Ahafo\n      POLYGON ((549970.457 968447.094, 550073.003 96...\n      40098.168231\n    \n    \n      2\n      Central\n      POLYGON ((603176.584 695877.238, 603248.424 69...\n      9665.626760\n    \n    \n      3\n      Eastern\n      POLYGON ((807307.254 797910.553, 807311.908 79...\n      18987.625847\n    \n    \n      4\n      Greater Accra\n      POLYGON ((858081.638 676424.913, 858113.115 67...\n      3706.511145"
  },
  {
    "objectID": "Exercise/3. Interactive Maps.html",
    "href": "Exercise/3. Interactive Maps.html",
    "title": "3. Interactive Maps",
    "section": "",
    "text": "Introduction\nYou are an urban safety planner in Japan, and you are analyzing which areas of Japan need extra earthquake reinforcement. Which areas are both high in population density and prone to earthquakes?\n\n\n\nBefore you get started, run the code cell below to set everything up.\n\nimport pandas as pd\nimport geopandas as gpd\n\nimport folium\nfrom folium import Choropleth\nfrom folium.plugins import HeatMap\n\n#from learntools.core import binder\n#binder.bind(globals())\n#from learntools.geospatial.ex3 import *\n\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\n\n지진은 판 경계와 일치하나요?\nDataFrame,plate_boundaries를 생성한다.\n“좌표”열은 경계를 따라 위치(위도,경도)의 목록임\n\nplate_boundaries = gpd.read_file(\"D:/archive (1)/Plate_Boundaries/Plate_Boundaries/Plate_Boundaries.shp\")\nplate_boundaries['coordinates'] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis='columns')\nplate_boundaries.drop('geometry', axis=1, inplace=True)\n\nplate_boundaries.head()\n\n\n\n\n\n  \n    \n      \n      HAZ_PLATES\n      HAZ_PLAT_1\n      HAZ_PLAT_2\n      Shape_Leng\n      coordinates\n    \n  \n  \n    \n      0\n      TRENCH\n      SERAM TROUGH (ACTIVE)\n      6722\n      5.843467\n      [(-5.444200361999947, 133.6808931800001), (-5....\n    \n    \n      1\n      TRENCH\n      WETAR THRUST\n      6722\n      1.829013\n      [(-7.760600482999962, 125.47879802900002), (-7...\n    \n    \n      2\n      TRENCH\n      TRENCH WEST OF LUZON (MANILA TRENCH) NORTHERN ...\n      6621\n      6.743604\n      [(19.817899819000047, 120.09999798800004), (19...\n    \n    \n      3\n      TRENCH\n      BONIN TRENCH\n      9821\n      8.329381\n      [(26.175899215000072, 143.20620700100005), (26...\n    \n    \n      4\n      TRENCH\n      NEW GUINEA TRENCH\n      8001\n      11.998145\n      [(0.41880004000006466, 132.8273013480001), (0....\n    \n  \n\n\n\n\n\n# Load the data and print the first 5 rows\nearthquakes = pd.read_csv(\"D:/archive (1)/earthquakes1970-2014.csv\", parse_dates=[\"DateTime\"])\nearthquakes.head()\n\n\n\n\n\n  \n    \n      \n      DateTime\n      Latitude\n      Longitude\n      Depth\n      Magnitude\n      MagType\n      NbStations\n      Gap\n      Distance\n      RMS\n      Source\n      EventID\n    \n  \n  \n    \n      0\n      1970-01-04 17:00:40.200\n      24.139\n      102.503\n      31.0\n      7.5\n      Ms\n      90.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970010e+09\n    \n    \n      1\n      1970-01-06 05:35:51.800\n      -9.628\n      151.458\n      8.0\n      6.2\n      Ms\n      85.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      2\n      1970-01-08 17:12:39.100\n      -34.741\n      178.568\n      179.0\n      6.1\n      Mb\n      59.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      3\n      1970-01-10 12:07:08.600\n      6.825\n      126.737\n      73.0\n      6.1\n      Mb\n      91.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      4\n      1970-01-16 08:05:39.000\n      60.280\n      -152.660\n      85.0\n      6.0\n      ML\n      0.0\n      NaN\n      NaN\n      NaN\n      AK\n      NaN\n    \n  \n\n\n\n\n지도에서 경계를 시각화 하기\n동일한 지도에 히트맴을 추가하여 지진이 판 경계와 일치하는지 확인 합니다.\n\n#경계 표시\nm_1 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_1)\n\nHeatMap(data=earthquakes[['Latitude', 'Longitude']], radius=20).add_to(m_1)\n\nm_1\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n2. 일본에서 지진의 깊이와 판 경계에 대한 근접도 사이에관계가 있습니까?\n\nearthquakes['Depth'].value_counts()\n\nDepth\n33.00     914\n10.00     716\n35.00      75\n20.00      64\n14.00      63\n         ... \n161.80      1\n238.70      1\n518.70      1\n591.30      1\n16.06       1\nName: count, Length: 1394, dtype: int64\n\n\n\n# Create a base map with plate boundaries\nm_2 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_2)\n \n\ndef color_producer(val):\n    if val < 30:\n        return 'forestgreen' \n    elif val < 60:\n        return 'darkblue'\n    elif val <90:\n        return 'red'\n    else:\n        return 'orange'\n    \n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        radius=20,\n        color=color_producer(earthquakes.iloc[i]['Depth'])).add_to(m_2)\n    \n\n \nm_2\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n3. 인구밀도가 높은 현(행정구역)은?\n\n# GeoDataFrame with prefecture boundaries\n# 경계 값\nprefectures = gpd.read_file(\"D:/archive (1)/japan-prefecture-boundaries/japan-prefecture-boundaries/japan-prefecture-boundaries.shp\")\nprefectures.set_index('prefecture', inplace=True)\nprefectures.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n    \n    \n      prefecture\n      \n    \n  \n  \n    \n      Aichi\n      MULTIPOLYGON (((137.09523 34.65330, 137.09546 ...\n    \n    \n      Akita\n      MULTIPOLYGON (((139.55725 39.20330, 139.55765 ...\n    \n    \n      Aomori\n      MULTIPOLYGON (((141.39860 40.92472, 141.39806 ...\n    \n    \n      Chiba\n      MULTIPOLYGON (((139.82488 34.98967, 139.82434 ...\n    \n    \n      Ehime\n      MULTIPOLYGON (((132.55859 32.91224, 132.55904 ...\n    \n  \n\n\n\n\n\n# DataFrame containing population of each prefecture\n#인구밀도\npopulation = pd.read_csv(\"D:/archive (1)/japan-prefecture-population.csv\")\npopulation.set_index('prefecture', inplace=True)\n\n# Calculate area (in square kilometers) of each prefecture\narea_sqkm = pd.Series(prefectures.geometry.to_crs(epsg=32654).area / 10**6, name='area_sqkm')\nstats = population.join(area_sqkm)\n\n# Add density (per square kilometer) of each prefecture\nstats['density'] = stats[\"population\"] / stats[\"area_sqkm\"]\nstats.head()\n\n\n\n\n\n  \n    \n      \n      population\n      area_sqkm\n      density\n    \n    \n      prefecture\n      \n      \n      \n    \n  \n  \n    \n      Tokyo\n      12868000\n      1800.614782\n      7146.448049\n    \n    \n      Kanagawa\n      8943000\n      2383.038975\n      3752.771186\n    \n    \n      Osaka\n      8801000\n      1923.151529\n      4576.342460\n    \n    \n      Aichi\n      7418000\n      5164.400005\n      1436.372085\n    \n    \n      Saitama\n      7130000\n      3794.036890\n      1879.264806\n    \n  \n\n\n\n\n\n# Create a base map\nm_3 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n\nChoropleth(geo_data=prefectures['geometry'].__geo_interface__, \n           data=stats['density'], \n           key_on=\"feature.id\", \n           fill_color='YlGnBu',  #색지정\n           legend_name='population density'\n          ).add_to(m_3)\n\nm_3\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n4) 규모가 큰 지진이 발생하기 쉬운 고밀도 현은 어디입니까?\n지진 보강으로 혜택을 받는 현을 제안하는 지도 만들어라.\n밀도, 지진 규모 시각화 하기\n\nm_4 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\ndef color_producer(magnitude):\n    if magnitude > 6.5:\n        return 'blue'\n    else:\n        return 'yellow'\n    \n\nChoropleth(geo_data=prefectures['geometry'].__geo_interface__, \n           data=stats['density'], \n           key_on=\"feature.id\", \n           fill_color='YlGnBu',  #색지정\n           legend_name='population density'\n          ).add_to(m_4)\n\n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        popup=(\"{} ({})\").format(\n            earthquakes.iloc[i]['Magnitude'],\n            earthquakes.iloc[i]['DateTime'].year),\n        radius=earthquakes.iloc[i]['Magnitude']**5.5,\n        color=color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)\n    \nm_4\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "Data/3. Interactive Maps.html",
    "href": "Data/3. Interactive Maps.html",
    "title": "3. Coordinate Reference Systems",
    "section": "",
    "text": "Introduction\n\nimport geopandas as gpd\nimport pandas as pd\nimport math\n\n\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\n\n\nYour first interactive map\n\n# Mapbox에서 발급받은 API 토큰을 입력합니다.\nAPI_TOKEN = \"pk.eyJ1Ijoia2ltZGF5ZW9uIiwiYSI6ImNsaHlnY2ZoYTBqZzkzZW41a3ViOTh4bDkifQ.xRqf22eMYZ1f4Jg3mRXWsg\"\n\n# Folium으로 Map을 생성합니다.\nm = folium.Map(\n    location=[37.266534, 127.000802], # 중심 좌표\n    zoom_start=13, # 확대/축소 레벨\n)\n\n# Mapbox에서 제공하는 \"Light\" 지도 타일을 추가합니다.\nfolium.TileLayer(\n    tiles=f\"mapbox://styles/jungseok0324/clh8r7zlw00bs01po3l2jfrej\",\n    attr=\"Mapbox attribution\",\n    name=\"Mapbox Light\"\n).add_to(m)\n\n# 생성한 맵을 HTML 파일로 저장합니다.\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Create a map    #tiles : 맵의 종류  #location에 위 경도 gpt에 검색하면 찾아짐\nm_1 = folium.Map(location=[42.32,-71.0589], tiles='openstreetmap', zoom_start=10)\n#zoom_start : 얼마나 확대해서 볼건지?\n# Display the map\nm_1\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nThe data\n\n# 데이터 타입 확인 DataFrame인걸 확인\ncrimes = pd.read_csv(\"D:/archive (1)/crimes-in-boston/crimes-in-boston/crime.csv\", encoding='latin-1')\n\ntype(crimes)\n\npandas.core.frame.DataFrame\n\n\n\n# Load the data\ncrimes = pd.read_csv(\"D:/archive (1)/crimes-in-boston/crimes-in-boston/crime.csv\", encoding='latin-1')\n\n# Drop rows with missing locations\n#위경도 좌표가 없는 NA값 제거 dropna사용\ncrimes.dropna(subset=['Lat', 'Long', 'DISTRICT'], inplace=True)\n\n# Focus on major crimes in 2018\n#범죄의 종류\ncrimes = crimes[crimes.OFFENSE_CODE_GROUP.isin([\n    'Larceny', 'Auto Theft', 'Robbery', 'Larceny From Motor Vehicle', 'Residential Burglary',\n    'Simple Assault', 'Harassment', 'Ballistics', 'Aggravated Assault', 'Other Burglary', \n    'Arson', 'Commercial Burglary', 'HOME INVASION', 'Homicide', 'Criminal Harassment', \n    'Manslaughter'])]\ncrimes = crimes[crimes.YEAR>=2018]\n\n# Print the first five rows of the table\ncrimes.head()\n\n\n\n\n\n  \n    \n      \n      INCIDENT_NUMBER\n      OFFENSE_CODE\n      OFFENSE_CODE_GROUP\n      OFFENSE_DESCRIPTION\n      DISTRICT\n      REPORTING_AREA\n      SHOOTING\n      OCCURRED_ON_DATE\n      YEAR\n      MONTH\n      DAY_OF_WEEK\n      HOUR\n      UCR_PART\n      STREET\n      Lat\n      Long\n      Location\n    \n  \n  \n    \n      0\n      I182070945\n      619\n      Larceny\n      LARCENY ALL OTHERS\n      D14\n      808\n      NaN\n      2018-09-02 13:00:00\n      2018\n      9\n      Sunday\n      13\n      Part One\n      LINCOLN ST\n      42.357791\n      -71.139371\n      (42.35779134, -71.13937053)\n    \n    \n      6\n      I182070933\n      724\n      Auto Theft\n      AUTO THEFT\n      B2\n      330\n      NaN\n      2018-09-03 21:25:00\n      2018\n      9\n      Monday\n      21\n      Part One\n      NORMANDY ST\n      42.306072\n      -71.082733\n      (42.30607218, -71.08273260)\n    \n    \n      8\n      I182070931\n      301\n      Robbery\n      ROBBERY - STREET\n      C6\n      177\n      NaN\n      2018-09-03 20:48:00\n      2018\n      9\n      Monday\n      20\n      Part One\n      MASSACHUSETTS AVE\n      42.331521\n      -71.070853\n      (42.33152148, -71.07085307)\n    \n    \n      19\n      I182070915\n      614\n      Larceny From Motor Vehicle\n      LARCENY THEFT FROM MV - NON-ACCESSORY\n      B2\n      181\n      NaN\n      2018-09-02 18:00:00\n      2018\n      9\n      Sunday\n      18\n      Part One\n      SHIRLEY ST\n      42.325695\n      -71.068168\n      (42.32569490, -71.06816778)\n    \n    \n      24\n      I182070908\n      522\n      Residential Burglary\n      BURGLARY - RESIDENTIAL - NO FORCE\n      B2\n      911\n      NaN\n      2018-09-03 18:38:00\n      2018\n      9\n      Monday\n      18\n      Part One\n      ANNUNCIATION RD\n      42.335062\n      -71.093168\n      (42.33506218, -71.09316781)\n    \n  \n\n\n\n\n\n\nPlotting points\n\n#강도인건만 가지고 오고 시가이 9시부터 18시까지 인걸 가지고 온다\n# &| 연산자 사용 후 둘다 사용\ndaytime_robberies = crimes[((crimes.OFFENSE_CODE_GROUP == 'Robbery') & \\\n                            (crimes.HOUR.isin(range(9,18))))]\n\n\n\nfolium.Marker\n\n\n# Create a map\n#cartobpositron : 밑 지도 처럼 위치 표시\nm_2 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=12)\n\n# Add points to the map\n#loops문 사용 / add_to 사용해서 하나씩 사용\nfor idx, row in daytime_robberies.iterrows():\n    Marker([row['Lat'], row['Long']]).add_to(m_2)\n\n# Display the map\nm_2\n#Make this Notebook Trusted to load map: File \n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nfolium.plugins.MarkerCluster\n\n# Create the map\nm_3 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)\n\n# Add points to the map\nmc = MarkerCluster()\nfor idx, row in daytime_robberies.iterrows():\n    if not math.isnan(row['Long']) and not math.isnan(row['Lat']): #num 값이 없을때 add.child를 추가해라\n        mc.add_child(Marker([row['Lat'], row['Long']]))\nm_3.add_child(mc)\n\n# Display the map\nm_3\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nBubble maps\n\n# Create a base map\nm_4 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)\n\ndef color_producer(val):\n    if val <= 12:\n        return 'forestgreen' #12보다 크면 darked\n    else:\n        return 'darkred'\n\n# Add a bubble map to the base map\nfor i in range(0,len(daytime_robberies)): #각 행에 대해 반봅\n    Circle(\n        location=[daytime_robberies.iloc[i]['Lat'], daytime_robberies.iloc[i]['Long']], #위도 경도 지정\n        radius=20, #반지름\n        color=color_producer(daytime_robberies.iloc[i]['HOUR'])).add_to(m_4)  #위 color를 지정한걸 hour에 적용\n    \n    \n#색과 크기 변경 가능\n# Display the map\nm_4\n#Make this Notebook Trusted to load map: File\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nHeatmaps\n\n# Create a base map\nm_5 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=12)\n\n# Add a heatmap to the base map  #radius : 해상도 조절(원 범위 조정)\nHeatMap(data=crimes[['Lat', 'Long']], radius=20).add_to(m_5)\n\n# Display the map\nm_5\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nChoropleth maps\n\n# GeoDataFrame with geographical boundaries of Boston police districts\ndistricts_full = gpd.read_file(\"D:/archive (1)/Police_Districts/Police_Districts/Police_Districts.shp\")\ndistricts = districts_full[[\"DISTRICT\", \"geometry\"]].set_index(\"DISTRICT\")\ndistricts.info()\n\n<class 'geopandas.geodataframe.GeoDataFrame'>\nIndex: 12 entries, A15 to B2\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   geometry  12 non-null     geometry\ndtypes: geometry(1)\nmemory usage: 492.0+ bytes\n\n\n\n# Number of crimes in each police district\n#각각의 행 확인\nplot_dict = crimes.DISTRICT.value_counts()\nplot_dict.head()\n\nD4     2885\nB2     2231\nA1     2130\nC11    1899\nB3     1421\nName: DISTRICT, dtype: int64\n\n\n\n# Create a base map\n#각 지방자치 구역으로 나눠서 볼 수 있음\nm_6 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=12)\n#색 변경도 가능 gpt에 물어봐/ 패키지 설치\n# Add a choropleth map to the base map\nChoropleth(geo_data=districts.__geo_interface__, \n           data=plot_dict, \n           key_on=\"feature.id\", \n           fill_color='YlGnBu',  #색지정\n           legend_name='Major criminal incidents (Jan-Aug 2018)'\n          ).add_to(m_6)\n\n# Display the map\nm_6\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "Data/4. Manipulating Geospatial Data.html",
    "href": "Data/4. Manipulating Geospatial Data.html",
    "title": "4. Manipulating Geospatial Data",
    "section": "",
    "text": "import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport folium\n\nfrom folium import Marker\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\nIntroduction\n\nGeocoding Geocoding is the process of converting the name of a place or an address to a location on a map. If you have ever looked up a geographic location based on a landmark description with Google Maps, Bing Maps, or Baidu Maps, for instance, then you have used a geocoder!\n\nGeocoding Geocoding is the process of converting the name of a place or an address to a location on a map. If you have ever looked up a geographic location based on a landmark description with Google Maps, Bing Maps, or Baidu Maps, for instance, then you have used a geocoder!\nWe’ll use geopy to do all of our geocoding.\n\n\n\nimage.png\n\n\n\nfrom geopy.geocoders import Nominatim\n\n\ngeolocator = Nominatim(user_agent=\"kaggle_learn\")\nlocation = geolocator.geocode(\"Pyramid of Khufu\")\n\nprint(location.point)\nprint(location.address)\n\n29 58m 44.976s N, 31 8m 3.17625s E\nهرم خوفو, شارع ابو الهول السياحي, نزلة البطران, الجيزة, 12125, مصر\n\n\n\npoint = location.point\nprint(\"Latitude:\", point.latitude)\nprint(\"Longitude:\", point.longitude)\n\nLatitude: 29.97916\nLongitude: 31.134215625236113\n\n\n\nuniversities = pd.read_csv(\"D:/archive (1)/top_universities.csv\")\nuniversities.head()\n\n\n\n\n\n  \n    \n      \n      Name\n    \n  \n  \n    \n      0\n      University of Oxford\n    \n    \n      1\n      University of Cambridge\n    \n    \n      2\n      Imperial College London\n    \n    \n      3\n      ETH Zurich\n    \n    \n      4\n      UCL\n    \n  \n\n\n\n\n\n#좌표로 나오게 함\n\ndef my_geocoder(row):\n    try:\n        point = geolocator.geocode(row).point\n        return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})\n    except:\n        return None\n\nuniversities[['Latitude', 'Longitude']] = universities.apply(lambda x: my_geocoder(x['Name']), axis=1)\n\nprint(\"{}% of addresses were geocoded!\".format(\n    (1 - sum(np.isnan(universities[\"Latitude\"])) / len(universities)) * 100))\n\n# Drop universities that were not successfully geocoded\nuniversities = universities.loc[~np.isnan(universities[\"Latitude\"])]\nuniversities = gpd.GeoDataFrame(\n    universities, geometry=gpd.points_from_xy(universities.Longitude, universities.Latitude))\nuniversities.crs = {'init': 'epsg:4326'}\nuniversities.head()\n\n91.0% of addresses were geocoded!\n\n\n\n\n\n\n  \n    \n      \n      Name\n      Latitude\n      Longitude\n      geometry\n    \n  \n  \n    \n      0\n      University of Oxford\n      51.759037\n      -1.252430\n      POINT (-1.25243 51.75904)\n    \n    \n      1\n      University of Cambridge\n      52.200623\n      0.110474\n      POINT (0.11047 52.20062)\n    \n    \n      2\n      Imperial College London\n      51.498959\n      -0.175641\n      POINT (-0.17564 51.49896)\n    \n    \n      3\n      ETH Zurich\n      47.562772\n      7.580947\n      POINT (7.58095 47.56277)\n    \n    \n      4\n      UCL\n      51.521785\n      -0.135151\n      POINT (-0.13515 51.52179)\n    \n  \n\n\n\n\n\n# Create a map\nm = folium.Map(location=[54, 15], tiles='openstreetmap', zoom_start=2)\n\n# Add points to the map\nfor idx, row in universities.iterrows():\n    Marker([row['Latitude'], row['Longitude']], popup=row['Name']).add_to(m)\n\n# Display the map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nTable joins\nNow, we’ll switch topics and think about how to combine data from different sources.\nAttribute join You already know how to use pd.DataFrame.join() to combine information from multiple DataFrames with a shared index. We refer to this way of joining data (by simpling matching values in the index) as an attribute join.\nWhen performing an attribute join with a GeoDataFrame, it’s best to use the gpd.GeoDataFrame.merge(). To illustrate this, we’ll work with a GeoDataFrame europe_boundaries containing the boundaries for every country in Europe. The first five rows of this GeoDataFrame are printed below.\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\neurope = world.loc[world.continent == 'Europe'].reset_index(drop=True)  #geometry dataframe이 아님\n\neurope_stats = europe[[\"name\", \"pop_est\", \"gdp_md_est\"]]\neurope_boundaries = europe[[\"name\", \"geometry\"]]  #이름과 geometry만 가지고 온거\n\n\neurope_boundaries.head()  #국가별로 공간정보를 가지고 있음\n\n\n\n\n\n  \n    \n      \n      name\n      geometry\n    \n  \n  \n    \n      0\n      Russia\n      MULTIPOLYGON (((180.00000 71.51571, 180.00000 ...\n    \n    \n      1\n      Norway\n      MULTIPOLYGON (((15.14282 79.67431, 15.52255 80...\n    \n    \n      2\n      France\n      MULTIPOLYGON (((-51.65780 4.15623, -52.24934 3...\n    \n    \n      3\n      Sweden\n      POLYGON ((11.02737 58.85615, 11.46827 59.43239...\n    \n    \n      4\n      Belarus\n      POLYGON ((28.17671 56.16913, 29.22951 55.91834...\n    \n  \n\n\n\n\n\neurope_stats.head() #국가별로 수치별로 가지고 옴\n\n\n\n\n\n  \n    \n      \n      name\n      pop_est\n      gdp_md_est\n    \n  \n  \n    \n      0\n      Russia\n      144373535.0\n      1699876\n    \n    \n      1\n      Norway\n      5347896.0\n      403336\n    \n    \n      2\n      France\n      67059887.0\n      2715518\n    \n    \n      3\n      Sweden\n      10285453.0\n      530883\n    \n    \n      4\n      Belarus\n      9466856.0\n      63080\n    \n  \n\n\n\n\n\n# Use an attribute join to merge data about countries in Europe\n#이름으로 합치기 \neurope = europe_boundaries.merge(europe_stats, on=\"name\")\neurope.head()\n\n\n\n\n\n  \n    \n      \n      name\n      geometry\n      pop_est\n      gdp_md_est\n    \n  \n  \n    \n      0\n      Russia\n      MULTIPOLYGON (((180.00000 71.51571, 180.00000 ...\n      144373535.0\n      1699876\n    \n    \n      1\n      Norway\n      MULTIPOLYGON (((15.14282 79.67431, 15.52255 80...\n      5347896.0\n      403336\n    \n    \n      2\n      France\n      MULTIPOLYGON (((-51.65780 4.15623, -52.24934 3...\n      67059887.0\n      2715518\n    \n    \n      3\n      Sweden\n      POLYGON ((11.02737 58.85615, 11.46827 59.43239...\n      10285453.0\n      530883\n    \n    \n      4\n      Belarus\n      POLYGON ((28.17671 56.16913, 29.22951 55.91834...\n      9466856.0\n      63080\n    \n  \n\n\n\n\n\n\nSpatial join 두개의 공간 합치기\nAnother type of join is a spatial join. With a spatial join, we combine GeoDataFrames based on the spatial relationship between the objects in the “geometry” columns. For instance, we already have a GeoDataFrame universities containing geocoded addresses of European universities.\nThen we can use a spatial join to match each university to its corresponding country. We do this with gpd.sjoin().\n\nuniversities\n\n\n\n\n\n  \n    \n      \n      Name\n      Latitude\n      Longitude\n      geometry\n    \n  \n  \n    \n      0\n      University of Oxford\n      51.759037\n      -1.252430\n      POINT (-1.25243 51.75904)\n    \n    \n      1\n      University of Cambridge\n      52.200623\n      0.110474\n      POINT (0.11047 52.20062)\n    \n    \n      2\n      Imperial College London\n      51.498959\n      -0.175641\n      POINT (-0.17564 51.49896)\n    \n    \n      3\n      ETH Zurich\n      47.562772\n      7.580947\n      POINT (7.58095 47.56277)\n    \n    \n      4\n      UCL\n      51.521785\n      -0.135151\n      POINT (-0.13515 51.52179)\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      94\n      University of East Anglia\n      52.622251\n      1.241158\n      POINT (1.24116 52.62225)\n    \n    \n      95\n      Aalborg University\n      57.015907\n      9.975308\n      POINT (9.97531 57.01591)\n    \n    \n      97\n      University of Bergen\n      60.368876\n      5.351112\n      POINT (5.35111 60.36888)\n    \n    \n      98\n      Lomonosov Moscow State University\n      55.705541\n      37.536060\n      POINT (37.53606 55.70554)\n    \n    \n      99\n      University of Antwerp\n      51.184386\n      4.419939\n      POINT (4.41994 51.18439)\n    \n  \n\n90 rows × 4 columns\n\n\n\n\n# Use spatial join to match universities to countries in Europe\n#왼쪽이 기준 / 오른쪽이 붙음 \neuropean_universities = gpd.sjoin(universities, europe)\n\n# Investigate the result\nprint(\"We located {} universities.\".format(len(universities)))\nprint(\"We located {} universities.\".format(len(universities))) #{} 안에 universities의 len이 들어감\n# f-string으로 변경할수 있음\n#print(f\"We located {len(universities)} universities.\")\n\nprint(\"Only {} of the universities were located in Europe (in {} different countries).\".format(\n  len(european_universities), len(european_universities.name.unique())))\n\n#너무 길면 \\ 사용하면 됨\n\n#f - string 사용 방법\n#print(f\"Only {european_universities} of the universities were located in Europe \\\n#      (in {european_universities.name.unique()} different countries).\")\n\neuropean_universities.head()\n\nWe located 90 universities.\nOnly                                                 Name   Latitude  Longitude   \n0                               University of Oxford  51.759037  -1.252430  \\\n1                            University of Cambridge  52.200623   0.110474   \n2                            Imperial College London  51.498959  -0.175641   \n4                                                UCL  51.521785  -0.135151   \n5   London School of Economics and Political Science  51.514211  -0.116808   \n..                                               ...        ...        ...   \n97                              University of Bergen  60.368876   5.351112   \n63                              University of Vienna  48.213128  16.360686   \n71                        Scuola Superiore Sant’Anna  43.720608  10.402644   \n75                  Scuola Normale Superiore di Pisa  43.719669  10.400433   \n98                 Lomonosov Moscow State University  55.705541  37.536060   \n\n                     geometry  index_right            name      pop_est   \n0   POINT (-1.25243 51.75904)           28  United Kingdom   66834405.0  \\\n1    POINT (0.11047 52.20062)           28  United Kingdom   66834405.0   \n2   POINT (-0.17564 51.49896)           28  United Kingdom   66834405.0   \n4   POINT (-0.13515 51.52179)           28  United Kingdom   66834405.0   \n5   POINT (-0.11681 51.51421)           28  United Kingdom   66834405.0   \n..                        ...          ...             ...          ...   \n97   POINT (5.35111 60.36888)            1          Norway    5347896.0   \n63  POINT (16.36069 48.21313)            7         Austria    8877067.0   \n71  POINT (10.40264 43.72061)           26           Italy   60297396.0   \n75  POINT (10.40043 43.71967)           26           Italy   60297396.0   \n98  POINT (37.53606 55.70554)            0          Russia  144373535.0   \n\n    gdp_md_est  \n0      2829108  \n1      2829108  \n2      2829108  \n4      2829108  \n5      2829108  \n..         ...  \n97      403336  \n63      445075  \n71     2003576  \n75     2003576  \n98     1699876  \n\n[85 rows x 8 columns] of the universities were located in Europe       (in ['United Kingdom' 'Switzerland' 'Germany' 'Sweden' 'Belgium' 'Netherlands'\n 'France' 'Finland' 'Denmark' 'Ireland' 'Norway' 'Austria' 'Italy'\n 'Russia'] different countries).\n\n\n\n\n\n\n  \n    \n      \n      Name\n      Latitude\n      Longitude\n      geometry\n      index_right\n      name\n      pop_est\n      gdp_md_est\n    \n  \n  \n    \n      0\n      University of Oxford\n      51.759037\n      -1.252430\n      POINT (-1.25243 51.75904)\n      28\n      United Kingdom\n      66834405.0\n      2829108\n    \n    \n      1\n      University of Cambridge\n      52.200623\n      0.110474\n      POINT (0.11047 52.20062)\n      28\n      United Kingdom\n      66834405.0\n      2829108\n    \n    \n      2\n      Imperial College London\n      51.498959\n      -0.175641\n      POINT (-0.17564 51.49896)\n      28\n      United Kingdom\n      66834405.0\n      2829108\n    \n    \n      4\n      UCL\n      51.521785\n      -0.135151\n      POINT (-0.13515 51.52179)\n      28\n      United Kingdom\n      66834405.0\n      2829108\n    \n    \n      5\n      London School of Economics and Political Science\n      51.514211\n      -0.116808\n      POINT (-0.11681 51.51421)\n      28\n      United Kingdom\n      66834405.0\n      2829108"
  },
  {
    "objectID": "Exercise/4. Manipulating Geospatial Data.html",
    "href": "Exercise/4. Manipulating Geospatial Data.html",
    "title": "4. Manipulating Geospatial Data",
    "section": "",
    "text": "Introduction\nYou are a Starbucks big data analyst (that’s a real job!) looking to find the next store into a Starbucks Reserve Roastery. These roasteries are much larger than a typical Starbucks store and have several additional features, including various food and wine options, along with upscale lounge areas. You’ll investigate the demographics of various counties in the state of California, to determine potentially suitable locations.\n\n\n\nBefore you get started, run the code cell below to set everything up.\n\nimport math\nimport pandas as pd\nimport geopandas as gpd\nfrom geopy.geocoders import Nominatim            # What you'd normally run\n#from learntools.geospatial.tools import Nominatim # Just for this exercise\n\nimport folium \nfrom folium import Marker\nfrom folium.plugins import MarkerCluster\n\n\n#from learntools.core import binder\n#binder.bind(globals())\n#from learntools.geospatial.ex4 import *\n\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\n\nExercises\n\n1) Geocode the missing locations.\nRun the next code cell to create a DataFrame starbucks containing Starbucks locations in the state of California.\n\n# Load and preview Starbucks locations in California\nstarbucks = pd.read_csv(\"D:/archive (1)/starbucks_locations.csv\")\nstarbucks.head()\n\n\n\n\n\n  \n    \n      \n      Store Number\n      Store Name\n      Address\n      City\n      Longitude\n      Latitude\n    \n  \n  \n    \n      0\n      10429-100710\n      Palmdale & Hwy 395\n      14136 US Hwy 395 Adelanto CA\n      Adelanto\n      -117.40\n      34.51\n    \n    \n      1\n      635-352\n      Kanan & Thousand Oaks\n      5827 Kanan Road Agoura CA\n      Agoura\n      -118.76\n      34.16\n    \n    \n      2\n      74510-27669\n      Vons-Agoura Hills #2001\n      5671 Kanan Rd. Agoura Hills CA\n      Agoura Hills\n      -118.76\n      34.15\n    \n    \n      3\n      29839-255026\n      Target Anaheim T-0677\n      8148 E SANTA ANA CANYON ROAD AHAHEIM CA\n      AHAHEIM\n      -117.75\n      33.87\n    \n    \n      4\n      23463-230284\n      Safeway - Alameda 3281\n      2600 5th Street Alameda CA\n      Alameda\n      -122.28\n      37.79\n    \n  \n\n\n\n\n\n대부분의 상점에는 알려진 위치(위도, 경도)가 있습니다. 그러나 Berkeley 시의 모든 위치가 누락되었습니다.\n\n\n# How many rows in each column have missing values?\nprint(starbucks.isnull().sum())\n\n# View rows with missing locations\nrows_with_missing = starbucks[starbucks[\"City\"]==\"Berkeley\"]\nrows_with_missing\n\nStore Number    0\nStore Name      0\nAddress         0\nCity            0\nLongitude       5\nLatitude        5\ndtype: int64\n\n\n\n\n\n\n  \n    \n      \n      Store Number\n      Store Name\n      Address\n      City\n      Longitude\n      Latitude\n    \n  \n  \n    \n      153\n      5406-945\n      2224 Shattuck - Berkeley\n      2224 Shattuck Avenue Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      154\n      570-512\n      Solano Ave\n      1799 Solano Avenue Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      155\n      17877-164526\n      Safeway - Berkeley #691\n      1444 Shattuck Place Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      156\n      19864-202264\n      Telegraph & Ashby\n      3001 Telegraph Avenue Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      157\n      9217-9253\n      2128 Oxford St.\n      2128 Oxford Street Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n  \n\n\n\n\n튜토리얼에서 우리는 (geopy.geocoders의) Nominatim()을 사용하여 값을 지오코딩했으며 이는 이 과정 외부에서 자신의 프로젝트에서 사용할 수 있는 것입니다.\n이 연습에서는 약간 다른 함수 Nominatim()(learntools.geospatial.tools에서)을 사용합니다. 이 기능은 노트북 상단에서 가져온 기능으로 GeoPandas의 기능과 동일하게 작동합니다.\n*즉, 다음과 같은 경우:\n노트북 상단의 import 문을 변경하지 않고 아래 코드 셀에서 지오코딩 함수를 geocode()로 호출합니다.\n\n# Create the geocoder\ngeolocator = Nominatim(user_agent=\"kaggle_learn\")\n\ndef my_geocoder(row):\n    point = geolocator.geocode(row).point\n    return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})\n\nberkeley = rows_with_missing.apply(lambda x: my_geocoder(x['Address']), axis=1)\nstarbucks.update(berkeley)\n\n\n\n\n2) View Berkeley locations.\nOpenStreetMap 스타일로 Berkeley의 (위도, 경도) 위치를 시각화합니다.\n\n# Create a base map\nm_2= folium.Map(location=[37.88,-122.26], tiles='openstreetmap',zoom_start=13)\n\nfor idx, row in starbucks[starbucks[\"City\"]=='Berkeley'].iterrows():\n    Marker([row['Latitude'], row['Longitude']]).add_to(m_2)\n\nm_2\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n3) Consolidate your data.\n아래 코드를 실행하여 캘리포니아 주의 각 카운티에 대한 이름, 지역(평방 킬로미터) 및 고유 ID(“GEOID” 열에 있음)가 포함된 GeoDataFrame CA_counties를 로드합니다. “도형” 열에는 카운티 경계가 있는 다각형이 포함되어 있습니다.\n\nCA_counties = gpd.read_file(\"D:/archive (1)/CA_county_boundaries/CA_county_boundaries/CA_county_boundaries.shp\")\nCA_counties.crs = {'init': 'epsg:4326'}\nCA_counties.head()\n\nc:\\Users\\jiyeo\\anaconda3\\envs\\dayeon\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      GEOID\n      name\n      area_sqkm\n      geometry\n    \n  \n  \n    \n      0\n      6091\n      Sierra County\n      2491.995494\n      POLYGON ((-120.65560 39.69357, -120.65554 39.6...\n    \n    \n      1\n      6067\n      Sacramento County\n      2575.258262\n      POLYGON ((-121.18858 38.71431, -121.18732 38.7...\n    \n    \n      2\n      6083\n      Santa Barbara County\n      9813.817958\n      MULTIPOLYGON (((-120.58191 34.09856, -120.5822...\n    \n    \n      3\n      6009\n      Calaveras County\n      2685.626726\n      POLYGON ((-120.63095 38.34111, -120.63058 38.3...\n    \n    \n      4\n      6111\n      Ventura County\n      5719.321379\n      MULTIPOLYGON (((-119.63631 33.27304, -119.6360...\n    \n  \n\n\n\n\n\nCA_pop에는 각 카운티의 인구 추정치가 포함되어 있습니다.\nCA_high_earners는 연간 소득이 $150,000 이상인 가구 수를 포함합니다.\nCA_median_age에는 각 카운티의 평균 연령이 포함됩니다.\n\n\nCA_pop = pd.read_csv(\"D:/archive (1)/CA_county_population.csv\", index_col=\"GEOID\")\nCA_high_earners = pd.read_csv(\"D:/archive (1)/CA_county_high_earners.csv\", index_col=\"GEOID\")\nCA_median_age = pd.read_csv(\"D:/archive (1)/CA_county_median_age.csv\", index_col=\"GEOID\")\n\n\nCA_pop, CA_high_earners 및 CA_median_age와 함께 CA_counties GeoDataFrame을 조인하기\n\n결과 GeoDataFrame CA_stats의 이름을 지정하고 “GEOID”, “name”, “area_sqkm”, “geometry”, “population”, “high_earners” 및 “median_age”의 8개 열이 있는지 확인합니다.\n\na=CA_counties.merge(CA_pop, on = \"GEOID\")\nab = a.merge(CA_high_earners,on = \"GEOID\")\nCA_stats = ab.merge(CA_median_age,on = \"GEOID\")\nCA_stats.head()\n\n\n\n\n\n  \n    \n      \n      GEOID\n      name\n      area_sqkm\n      geometry\n      population\n      high_earners\n      median_age\n    \n  \n  \n    \n      0\n      6091\n      Sierra County\n      2491.995494\n      POLYGON ((-120.65560 39.69357, -120.65554 39.6...\n      2987\n      111\n      55.0\n    \n    \n      1\n      6067\n      Sacramento County\n      2575.258262\n      POLYGON ((-121.18858 38.71431, -121.18732 38.7...\n      1540975\n      65768\n      35.9\n    \n    \n      2\n      6083\n      Santa Barbara County\n      9813.817958\n      MULTIPOLYGON (((-120.58191 34.09856, -120.5822...\n      446527\n      25231\n      33.7\n    \n    \n      3\n      6009\n      Calaveras County\n      2685.626726\n      POLYGON ((-120.63095 38.34111, -120.63058 38.3...\n      45602\n      2046\n      51.6\n    \n    \n      4\n      6111\n      Ventura County\n      5719.321379\n      MULTIPOLYGON (((-119.63631 33.27304, -119.6360...\n      850967\n      57121\n      37.5\n    \n  \n\n\n\n\n\nCA_stats[\"density\"] = CA_stats[\"population\"] / CA_stats[\"area_sqkm\"]\nCA_stats.head()\n\n\n\n\n\n  \n    \n      \n      GEOID\n      name\n      area_sqkm\n      geometry\n      population\n      high_earners\n      median_age\n      density\n    \n  \n  \n    \n      0\n      6091\n      Sierra County\n      2491.995494\n      POLYGON ((-120.65560 39.69357, -120.65554 39.6...\n      2987\n      111\n      55.0\n      1.198638\n    \n    \n      1\n      6067\n      Sacramento County\n      2575.258262\n      POLYGON ((-121.18858 38.71431, -121.18732 38.7...\n      1540975\n      65768\n      35.9\n      598.376878\n    \n    \n      2\n      6083\n      Santa Barbara County\n      9813.817958\n      MULTIPOLYGON (((-120.58191 34.09856, -120.5822...\n      446527\n      25231\n      33.7\n      45.499825\n    \n    \n      3\n      6009\n      Calaveras County\n      2685.626726\n      POLYGON ((-120.63095 38.34111, -120.63058 38.3...\n      45602\n      2046\n      51.6\n      16.980022\n    \n    \n      4\n      6111\n      Ventura County\n      5719.321379\n      MULTIPOLYGON (((-119.63631 33.27304, -119.6360...\n      850967\n      57121\n      37.5\n      148.788107\n    \n  \n\n\n\n\n\n\n4) Which counties look promising?\n모든 정보를 단일 GeoDataFrame으로 축소하면 특정 기준을 충족하는 카운티를 훨씬 쉽게 선택할 수 있습니다.\n다음 코드 셀을 사용하여 CA_stats GeoDataFrame에서 행(및 모든 열)의 하위 집합을 포함하는 GeoDataFrame sel_counties를 생성합니다. 특히 다음과 같은 카운티를 선택해야 합니다.\n연간 $150,000를 버는 가구가 100,000가구 이상이며, 평균 연령은 38.5세 미만이며, 주민 밀도는 최소 285(평방 킬로미터당)입니다. 또한 선택한 카운티는 다음 기준 중 하나 이상을 충족해야 합니다.\n연간 $150,000를 버는 가구가 500,000가구 이상이며, 평균 연령이 35.5세 미만이거나 주민 밀도는 최소 1400(평방 킬로미터당)입니다.\n\nsel_counties= CA_stats[(CA_stats.high_earners >= 100000) & (CA_stats.median_age < 38.5) & (CA_stats.density>285) &\n         (CA_stats.high_earners >= 500000) & (CA_stats.median_age < 35.5) | (CA_stats.density>1400)]\n\n\n\n5) How many stores did you identify?\n다음 Starbucks Reserve Roastery 위치를 찾을 때 선택한 카운티 내의 모든 매장을 고려하고 싶을 것입니다. 그렇다면 선택한 카운티 내에 몇 개의 매장이 있습니까?\n이 질문에 답할 준비를 하려면 다음 코드 셀을 실행하여 모든 starbucks 위치가 포함된 GeoDataFrame starbucks_gdf를 만듭니다.\n\nstarbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude))\nstarbucks_gdf.crs = {'init': 'epsg:4326'}\nstarbucks_gdf.head()\n\nc:\\Users\\jiyeo\\anaconda3\\envs\\dayeon\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      Store Number\n      Store Name\n      Address\n      City\n      Longitude\n      Latitude\n      geometry\n    \n  \n  \n    \n      0\n      10429-100710\n      Palmdale & Hwy 395\n      14136 US Hwy 395 Adelanto CA\n      Adelanto\n      -117.40\n      34.51\n      POINT (-117.40000 34.51000)\n    \n    \n      1\n      635-352\n      Kanan & Thousand Oaks\n      5827 Kanan Road Agoura CA\n      Agoura\n      -118.76\n      34.16\n      POINT (-118.76000 34.16000)\n    \n    \n      2\n      74510-27669\n      Vons-Agoura Hills #2001\n      5671 Kanan Rd. Agoura Hills CA\n      Agoura Hills\n      -118.76\n      34.15\n      POINT (-118.76000 34.15000)\n    \n    \n      3\n      29839-255026\n      Target Anaheim T-0677\n      8148 E SANTA ANA CANYON ROAD AHAHEIM CA\n      AHAHEIM\n      -117.75\n      33.87\n      POINT (-117.75000 33.87000)\n    \n    \n      4\n      23463-230284\n      Safeway - Alameda 3281\n      2600 5th Street Alameda CA\n      Alameda\n      -122.28\n      37.79\n      POINT (-122.28000 37.79000)\n    \n  \n\n\n\n\n\nlocations= gpd.sjoin(starbucks_gdf, sel_counties)\nnum_stores = len(locations)\nnum_stores\n\n87\n\n\n\n\n6) Visualize the store locations.\n이전 질문에서 식별한 매장의 위치를 ​​표시하는 지도를 만듭니다.\n\nm_6 = folium.Map(location=[37,-120], zoom_start=6)\n\n\nmc = MarkerCluster()\n\nlocations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\nfor idx, row in locations_of_interest.iterrows():\n    if not math.isnan(row['Longitude']) and not math.isnan(row['Latitude']):\n        mc.add_child(folium.Marker([row['Latitude'], row['Longitude']]))\n\nm_6.add_child(mc)\n\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "프로젝트.html",
    "href": "프로젝트.html",
    "title": "프로젝트",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n공장 토양오염\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nkim dayeon\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "프로젝트/프로젝트.html",
    "href": "프로젝트/프로젝트.html",
    "title": "공장 토양오염",
    "section": "",
    "text": "충남 산업폐수 배출량 ‘전국 2위’ 수준\n\n\n\nimage.png\n\n\n\nimport geopandas as gpd\n\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport math\n\nfrom geopy.geocoders import Nominatim\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\n\n#factor_df=pd.read_csv(\"C:/Users/jiyeo/Desktop/데이터 마이닝 프로젝트/충남 공장현황.csv\")\n#factor_df = factor_df.drop(columns = 'Unnamed: 0')\n#factor = gpd.GeoDataFrame(factor_df,geometry = gpd.points_from_xy(factor_df['위도'],factor_df['경도']))\n#factor.crs = {'init': 'epsg:4326'}\n#factor.head()\n\n\n# Read in the data\nfull_data = gpd.read_file(\"C:/Users/jiyeo/Desktop/데이터 마이닝 프로젝트/LARD_ADM_SECT_SGG_충남/LARD_ADM_SECT_SGG_44.shp\",encoding='CP949')\n# districts = full_data[[\"SGG_NM\", \"geometry\"]].set_index(\"SGG_NM\")\n# 데이터 확인\nfull_data\n\n\n\n\n\n  \n    \n      \n      ADM_SECT_C\n      SGG_NM\n      SGG_OID\n      COL_ADM_SE\n      GID\n      geometry\n    \n  \n  \n    \n      0\n      44825\n      태안군\n      NaN\n      44825\n      38\n      MULTIPOLYGON (((887303.170 1874983.047, 887246...\n    \n    \n      1\n      44810\n      예산군\n      NaN\n      44810\n      39\n      POLYGON ((929699.323 1867530.281, 929701.417 1...\n    \n    \n      2\n      44800\n      홍성군\n      NaN\n      44800\n      40\n      MULTIPOLYGON (((906695.008 1844568.917, 906695...\n    \n    \n      3\n      44790\n      청양군\n      NaN\n      44790\n      41\n      POLYGON ((925804.573 1829651.798, 925814.822 1...\n    \n    \n      4\n      44770\n      서천군\n      NaN\n      44770\n      42\n      MULTIPOLYGON (((919207.899 1777520.293, 919117...\n    \n    \n      5\n      44760\n      부여군\n      NaN\n      44760\n      43\n      POLYGON ((926266.353 1815231.150, 926388.655 1...\n    \n    \n      6\n      44710\n      금산군\n      NaN\n      44710\n      44\n      POLYGON ((988618.155 1808331.267, 988701.439 1...\n    \n    \n      7\n      44270\n      당진시\n      NaN\n      44270\n      45\n      MULTIPOLYGON (((918263.561 1871005.741, 918263...\n    \n    \n      8\n      44250\n      계룡시\n      NaN\n      44250\n      46\n      POLYGON ((972722.679 1807070.393, 972723.823 1...\n    \n    \n      9\n      44230\n      논산시\n      NaN\n      44230\n      47\n      POLYGON ((965710.690 1813900.889, 965729.101 1...\n    \n    \n      10\n      44210\n      서산시\n      NaN\n      44210\n      48\n      MULTIPOLYGON (((902621.942 1845625.814, 902606...\n    \n    \n      11\n      44200\n      아산시\n      NaN\n      44200\n      49\n      POLYGON ((961587.843 1882485.024, 961618.275 1...\n    \n    \n      12\n      44180\n      보령시\n      NaN\n      44180\n      50\n      MULTIPOLYGON (((862402.125 1805616.130, 862475...\n    \n    \n      13\n      44150\n      공주시\n      NaN\n      44150\n      51\n      POLYGON ((952436.598 1853608.129, 952532.895 1...\n    \n    \n      14\n      44133\n      천안시서북구\n      NaN\n      44130\n      52\n      POLYGON ((965794.161 1885898.908, 965821.523 1...\n    \n    \n      15\n      44131\n      천안시동남구\n      NaN\n      44130\n      53\n      POLYGON ((981821.389 1876846.713, 981931.111 1...\n    \n  \n\n\n\n\n\n# Number of crimes in each police district\n\n#위 데이터 공장현황과 충청남도 위경도 데이터를 합침\n# 공장현황 데이터에서 군구 마다 몇개의 공장이 있는지 확인후 csv파일로 만듬\ncount = pd.read_csv(\"C:/Users/jiyeo/Desktop/데이터 마이닝 프로젝트/공장수.csv\")\ncount = count.drop(columns = 'Unnamed: 0')\ncount\n\n\n\n\n\n  \n    \n      \n      SGG_NM\n      공장수\n    \n  \n  \n    \n      0\n      태안군\n      108\n    \n    \n      1\n      예산군\n      459\n    \n    \n      2\n      홍성군\n      399\n    \n    \n      3\n      청양군\n      136\n    \n    \n      4\n      서천군\n      235\n    \n    \n      5\n      부여군\n      170\n    \n    \n      6\n      금산군\n      900\n    \n    \n      7\n      당진시\n      1011\n    \n    \n      8\n      계룡시\n      78\n    \n    \n      9\n      논산시\n      847\n    \n    \n      10\n      서산시\n      462\n    \n    \n      11\n      아산시\n      2292\n    \n    \n      12\n      보령시\n      463\n    \n    \n      13\n      공주시\n      607\n    \n    \n      14\n      천안시 서북구\n      2273\n    \n    \n      15\n      천안시 동남구\n      1228\n    \n  \n\n\n\n\n\n# Create a base map\n#각 지방자치 구역으로 나눠서 볼 수 있음\nmm = folium.Map(location=[36.8, 127.2], tiles='cartodbpositron', zoom_start=9)\n#색 변경도 가능 gpt에 물어봐/ 패키지 설치\n# Add a choropleth map to the base map\nChoropleth(geo_data=full_data['geometry'], \n           data=count['공장수'], \n           key_on=\"feature.id\", \n           fill_color='YlGnBu',  #색지정\n           legend_name='시도별 공장수'\n          ).add_to(mm)\n\n# Display the map\nmm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n공장 대기 및 폐수 정도 시각화\n’ 1종 = 빨간 / 2종 = 주황 / 3종 = 초록 / 4종 = 하늘 / 5종 = 파랑 순으로 표시함 ’\n\n대기 = 네모\n폐수 = 동그라미\n\n\n\n\nimage.png\n\n\n\nairwater= pd.read_csv(\"C:/Users/jiyeo/Desktop/데이터 마이닝 프로젝트/new_대기 및 폐수배출시설.csv\")\nairwater = gpd.GeoDataFrame(airwater,geometry = gpd.points_from_xy(airwater['위도'],airwater['경도']))\nairwater.crs = {'init': 'epsg:4326'}\nairwater = airwater.drop(columns = 'Unnamed: 0')\nairwater.head()\n\nc:\\Users\\jiyeo\\anaconda3\\envs\\dayeon\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      업소명\n      주소\n      대기종별\n      수질종별\n      업종\n      위도\n      경도\n      geometry\n    \n  \n  \n    \n      0\n      해태에이치티비㈜ 천안공장\n      충청남도 천안시 동남구 청당산업길 250\n      1.0\n      1.0\n      기타 비알콜음료 제조업(11209)\n      36.757613\n      127.146665\n      POINT (36.75761 127.14667)\n    \n    \n      1\n      한솔제지㈜\n      충청남도 천안시 동남구 광덕면 세종로 4186\n      2.0\n      1.0\n      그외 기타 종이 및 판지 제품 제조업(17909)\n      36.690954\n      127.158836\n      POINT (36.69095 127.15884)\n    \n    \n      2\n      삼광글라스㈜천안유리공장\n      충청남도 천안시 서북구 입장면 망향로 1112-84\n      1.0\n      3.0\n      포장용 유리용기 제조업(23192)\n      36.908517\n      127.232032\n      POINT (36.90852 127.23203)\n    \n    \n      3\n      케이씨글라스㈜\n      충청남도 천안시 서북구 입장면 성진로 1105\n      1.0\n      4.0\n      포장용 유리용기 제조업(23192)\n      36.916594\n      127.251390\n      POINT (36.91659 127.25139)\n    \n    \n      4\n      성우오토텍㈜\n      충청남도 천안시 동남구 동면 충절로 2294-42\n      2.0\n      4.0\n      그외 기타 자동차 부품 제조업(30399)\n      36.776124\n      127.357712\n      POINT (36.77612 127.35771)\n    \n  \n\n\n\n\n\nair_data = airwater[['업소명','대기종별','수질종별','위도','경도','geometry']]\nair_data.head()\n\n\n\n\n\n  \n    \n      \n      업소명\n      대기종별\n      수질종별\n      위도\n      경도\n      geometry\n    \n  \n  \n    \n      0\n      해태에이치티비㈜ 천안공장\n      1.0\n      1.0\n      36.757613\n      127.146665\n      POINT (36.75761 127.14667)\n    \n    \n      1\n      한솔제지㈜\n      2.0\n      1.0\n      36.690954\n      127.158836\n      POINT (36.69095 127.15884)\n    \n    \n      2\n      삼광글라스㈜천안유리공장\n      1.0\n      3.0\n      36.908517\n      127.232032\n      POINT (36.90852 127.23203)\n    \n    \n      3\n      케이씨글라스㈜\n      1.0\n      4.0\n      36.916594\n      127.251390\n      POINT (36.91659 127.25139)\n    \n    \n      4\n      성우오토텍㈜\n      2.0\n      4.0\n      36.776124\n      127.357712\n      POINT (36.77612 127.35771)\n    \n  \n\n\n\n\n\nimport pydeck as pdk\nimport folium\n#m_air = folium.Map(location=[36.8, 127.2], tiles='cartodbpositron', zoom_start=12)\n\n\ndef color_producer(val):\n    if val <= 1:\n        return 'hotpink' \n    elif val <=2:\n        return 'orange'\n    elif val <=3:\n        return 'green'\n    elif val <=4:\n        return 'blue'\n    elif val <= 5:\n        return 'skyblue'\n    else:\n        return 'rgba(255, 255, 255, 0)'\n  \n\n\n# Add a square marker to the base map\nfor i in range(len(air_data)):\n    folium.Rectangle(\n        bounds=[\n            [air_data.iloc[i]['위도'] - 0.0005, air_data.iloc[i]['경도'] - 0.0005],\n            [air_data.iloc[i]['위도'] + 0.0005, air_data.iloc[i]['경도'] + 0.0005]\n        ],\n        color=color_producer(air_data.iloc[i]['대기종별']),\n        fill=True,\n        fill_color=color_producer(air_data.iloc[i]['대기종별']),\n        fill_opacity=0.6,\n        tooltip = air_data.iloc[i]['업소명'],\n        name = \"공장\").add_to(mm)\n\n\nfor i in range(0,len(air_data)): #각 행에 대해 반복\n    Circle(\n        location=[air_data.iloc[i]['위도'], air_data.iloc[i]['경도']], #위도 경도 지정\n        radius=100, #반지름\n        color=color_producer(air_data.iloc[i]['수질종별']),\n        tooltip = air_data.iloc[i]['업소명'],\n        name = \"공장\").add_to(mm)  \n\n\n        \n\nmm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nwater_df= pd.read_csv(\"C:/Users/jiyeo/Desktop/데이터 마이닝 프로젝트/공공폐수처리시설.csv\")\nwater_df.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      시설설치/구분\n      유역/지방/환경청\n      시/군\n      처리장명\n      주소\n      대권역명\n      중권역명\n      소권역명\n      공공폐수/처리구역/면적(ha)\n      연간운영전력비(천원)\n      공공폐수처리구역(산업,농공단지,기타)\n      시설용량(m3/일)\n      초과횟수\n      초과일자\n      위도\n      경도\n    \n  \n  \n    \n      0\n      48\n      일반\n      금강\n      공주시\n      공주탄천\n      충청남도 공주시 탄천면 탄천산업단지길 112-20\n      금강권역\n      금강공주\n      석성천\n      99.70\n      78801.0\n      탄천일반산업단지\n      1700.0\n      0.0\n      -\n      36.298718\n      127.062411\n    \n    \n      1\n      49\n      농공\n      금강\n      공주시\n      공주정안2\n      충청남도 공주시 정안면 정안농공단지길32-16\n      금강권역\n      금강공주\n      사현천\n      30.97\n      75000.0\n      정안1농공단지\n      600.0\n      0.0\n      -\n      36.617625\n      127.125448\n    \n    \n      2\n      50\n      농공\n      금강\n      공주시\n      공주검상\n      충청남도 공주시 공단길 82\n      금강권역\n      금강공주\n      어천합류후\n      39.60\n      33503.0\n      검상농공단지\n      500.0\n      0.0\n      -\n      36.431264\n      127.069392\n    \n    \n      3\n      51\n      농공\n      금강\n      공주시\n      공주보물\n      충청남도 공주시 정안면 차령로 3514\n      금강권역\n      금강공주\n      공주보\n      13.40\n      19090.0\n      보물농공단지\n      200.0\n      0.0\n      -\n      36.601928\n      127.122647\n    \n    \n      4\n      52\n      농공\n      금강\n      공주시\n      공주월미\n      충청남도 공주시 월미농공단지길 26-8\n      금강권역\n      금강공주\n      공주보\n      174.90\n      38366.0\n      월미농공단지\n      410.0\n      0.0\n      -\n      36.489992\n      127.103952\n    \n  \n\n\n\n\n\nwater_df = gpd.GeoDataFrame(water_df,geometry = gpd.points_from_xy(water_df['위도'],water_df['경도']))\nwater_df.crs = {'init': 'epsg:4326'}\nwater_df.head()\n\nc:\\Users\\jiyeo\\anaconda3\\envs\\dayeon\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      시설설치/구분\n      유역/지방/환경청\n      시/군\n      처리장명\n      주소\n      대권역명\n      중권역명\n      소권역명\n      공공폐수/처리구역/면적(ha)\n      연간운영전력비(천원)\n      공공폐수처리구역(산업,농공단지,기타)\n      시설용량(m3/일)\n      초과횟수\n      초과일자\n      위도\n      경도\n      geometry\n    \n  \n  \n    \n      0\n      48\n      일반\n      금강\n      공주시\n      공주탄천\n      충청남도 공주시 탄천면 탄천산업단지길 112-20\n      금강권역\n      금강공주\n      석성천\n      99.70\n      78801.0\n      탄천일반산업단지\n      1700.0\n      0.0\n      -\n      36.298718\n      127.062411\n      POINT (36.29872 127.06241)\n    \n    \n      1\n      49\n      농공\n      금강\n      공주시\n      공주정안2\n      충청남도 공주시 정안면 정안농공단지길32-16\n      금강권역\n      금강공주\n      사현천\n      30.97\n      75000.0\n      정안1농공단지\n      600.0\n      0.0\n      -\n      36.617625\n      127.125448\n      POINT (36.61762 127.12545)\n    \n    \n      2\n      50\n      농공\n      금강\n      공주시\n      공주검상\n      충청남도 공주시 공단길 82\n      금강권역\n      금강공주\n      어천합류후\n      39.60\n      33503.0\n      검상농공단지\n      500.0\n      0.0\n      -\n      36.431264\n      127.069392\n      POINT (36.43126 127.06939)\n    \n    \n      3\n      51\n      농공\n      금강\n      공주시\n      공주보물\n      충청남도 공주시 정안면 차령로 3514\n      금강권역\n      금강공주\n      공주보\n      13.40\n      19090.0\n      보물농공단지\n      200.0\n      0.0\n      -\n      36.601928\n      127.122647\n      POINT (36.60193 127.12265)\n    \n    \n      4\n      52\n      농공\n      금강\n      공주시\n      공주월미\n      충청남도 공주시 월미농공단지길 26-8\n      금강권역\n      금강공주\n      공주보\n      174.90\n      38366.0\n      월미농공단지\n      410.0\n      0.0\n      -\n      36.489992\n      127.103952\n      POINT (36.48999 127.10395)\n    \n  \n\n\n\n\n\nmean_value = water_df['공공폐수/처리구역/면적(ha)'].mean()\nmean_value\n\n#1헥타르는 10,000제곱미터와 동일하다. \n#헥타르를 미터로 변환하기 위해 10,000로 곱해 그 값으로 원을 정했다. \n\n83.67641288461537\n\n\n\n9971308/3600\n\n2769.8077777777776\n\n\n\nwater_df[water_df['연간운영전력비(천원)'] >= 476036]\nwater_df.count()\n\nUnnamed: 0              52\n시설설치/구분                 52\n유역/지방/환경청               52\n시/군                     52\n처리장명                    52\n주소                      52\n대권역명                    52\n중권역명                    52\n소권역명                    52\n공공폐수/처리구역/면적(ha)        52\n연간운영전력비(천원)             52\n공공폐수처리구역(산업,농공단지,기타)    52\n시설용량(m3/일)              52\n초과횟수                    52\n초과일자                    52\n위도                      52\n경도                      52\ngeometry                52\ndtype: int64\n\n\n\n#big_value = water_df[(water_df['공공폐수/처리구역/면적(ha)'] >= 83) & (water_df['연간운영전력비(천원)'] >= 476036)]\n#big_value.head()\n\n밑에 코드실행 하지 말고\n\nfrom folium.plugins import MeasureControl\n\nimport folium\nfrom folium import plugins\n\n \ntooltip = \"Click\"\n\nfor i in range(0, len(water_df)):\n    if water_df.iloc[i]['공공폐수/처리구역/면적(ha)'] >= 83 or water_df.iloc[i]['연간운영전력비(천원)'] >= 476036:\n        folium.Marker(\n            location=[water_df.iloc[i]['위도'], water_df.iloc[i]['경도']],\n            popup=f'<div style=\"width:100px\"><strong>{water_df.iloc[i][\"처리장명\"]}</strong><br>'\n                  f'초과횟수: {str(water_df.iloc[i][\"초과횟수\"])} <br>'\n                  f'초과일자: {water_df.iloc[i][\"초과일자\"]}<br></div>',\n            tooltip=tooltip,\n            icon=folium.Icon(icon='exclamation-sign', color='red')\n        ).add_to(mm)\n    else:\n        folium.Marker(\n            location=[water_df.iloc[i]['위도'], water_df.iloc[i]['경도']],\n            popup=f'<div style=\"width:100px\"><strong>{water_df.iloc[i][\"처리장명\"]}</strong><br>'\n                  f'초과횟수: {str(water_df.iloc[i][\"초과횟수\"])} <br>'\n                  f'초과일자: {water_df.iloc[i][\"초과일자\"]}<br></div>',\n            tooltip=tooltip,\n            icon=folium.Icon(icon='exclamation-sign', color='red')\n        ).add_to(mm)\n\n\nfor i in range(0, len(water_df)):\n    folium.Circle(\n        location=[water_df.iloc[i]['위도'], water_df.iloc[i]['경도']],\n        radius=1000,  # 반지름(단위: 미터)\n        color='black').add_to(mm)\n\n\n\n# 지도 출력\nmm\n\n\n# HTML 파일로 저장\n#mm.save(\"폐수처리장.html\")\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n폐수면적이 83이상이고 전력비가 약 47만원 인 폐수처리장만 표시\n\nfrom folium.plugins import MeasureControl\n\nimport folium\nfrom folium import plugins\n\n \ntooltip = \"Click\"\n\nfor i in range(0, len(water_df)):\n    if water_df.iloc[i]['공공폐수/처리구역/면적(ha)'] >= 83 or water_df.iloc[i]['연간운영전력비(천원)'] >= 476036:\n        folium.Marker(\n            location=[water_df.iloc[i]['위도'], water_df.iloc[i]['경도']],\n            popup=f'<div style=\"width:100px\"><strong>{water_df.iloc[i][\"처리장명\"]}</strong><br>'\n                  f'초과횟수: {str(water_df.iloc[i][\"초과횟수\"])} <br>'\n                  f'초과일자: {water_df.iloc[i][\"초과일자\"]}<br></div>',\n            tooltip=tooltip,\n            icon=folium.Icon(icon='exclamation-sign', color='red')\n        ).add_to(mm)\n\n\n\nfor i in range(0, len(water_df)):\n    if water_df.iloc[i]['공공폐수/처리구역/면적(ha)'] >= 83 or water_df.iloc[i]['연간운영전력비(천원)'] >= 476036:\n        folium.Circle(\n            location=[water_df.iloc[i]['위도'], water_df.iloc[i]['경도']],\n            radius=1000,  # 반지름(단위: 미터)\n            color='black').add_to(mm)\n\n\n\n# 지도 출력\nmm\n\n\n# HTML 파일로 저장\n#mm.save(\"폐수처리장.html\")\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Create a base map\n#각 지방자치 구역으로 나눠서 볼 수 있음\nmm_1 = folium.Map(location=[36.8, 127.2], tiles='cartodbpositron', zoom_start=9)\n#색 변경도 가능 gpt에 물어봐/ 패키지 설치\n# Add a choropleth map to the base map\nChoropleth(geo_data=full_data['geometry'], \n           data=count['공장수'], \n           key_on=\"feature.id\", \n           fill_color='YlGnBu',  #색지정\n           legend_name='시도별 공장수'\n          ).add_to(mm_1)\n\n# Display the map\nmm_1\n\n\n\n\n# Add a square marker to the base map\nfor i in range(len(air_data)):\n    if color_producer(air_data.iloc[i]['대기종별']) in ['hotpink', 'orange','green'] and \\\n            color_producer(air_data.iloc[i]['수질종별']) in ['hotpink', 'orange','green']:\n        Circle(\n        location=[air_data.iloc[i]['위도'], air_data.iloc[i]['경도']], #위도 경도 지정\n        radius=100, #반지름\n        color='green',\n        tooltip = air_data.iloc[i]['업소명'],\n        name = \"공장\"\n        ).add_to(mm_1)\n    else:\n        # Delete the factory from the map\n        if \"공장\" in mm_1._children:\n            for child in mm_1._children[\"공장\"].values():\n                if child.options.get('tooltip') == air_data.iloc[i]['업소명']:\n                    mm_1._children[\"공장\"].pop(child.get_name())\n\n# Remove the '공장' layer if it is empty\nif \"공장\" in mm_1._children and not mm_1._children[\"공장\"]:\n    mm_1._children.pop(\"공장\")\nelse:\n    # Remove the factories that don't meet the conditions\n    for child in mm_1._children.get(\"공장\", {}).values():\n        if color_producer(air_data.iloc[i]['대기종별']) not in ['hotpink', 'orange','green'] or \\\n                color_producer(air_data.iloc[i]['수질종별']) not in ['hotpink', 'orange','green']:\n            mm_1._children[\"공장\"].pop(child.get_name())\n\n\n\n\n\nfrom folium.plugins import MeasureControl\n\nimport folium\nfrom folium import plugins\n\n \n#tooltip = \"Click\"\n\nfor i in range(0, len(water_df)):\n    if water_df.iloc[i]['공공폐수/처리구역/면적(ha)'] >= 83 or water_df.iloc[i]['연간운영전력비(천원)'] >= 476036:\n        folium.Marker(\n            location=[water_df.iloc[i]['위도'], water_df.iloc[i]['경도']],\n            tooltip=f'<div style=\"width:100px\"><strong>{water_df.iloc[i][\"처리장명\"]}</strong><br>'\n                  f'초과횟수: {str(water_df.iloc[i][\"초과횟수\"])} <br>'\n                  f'초과일자: {water_df.iloc[i][\"초과일자\"]}<br></div>',\n            #tooltip=tooltip,\n            icon=folium.Icon(icon='exclamation-sign', color='red')\n        ).add_to(mm_1)\n\n\n\nfor i in range(0, len(water_df)):\n    if water_df.iloc[i]['공공폐수/처리구역/면적(ha)'] >= 83 or water_df.iloc[i]['연간운영전력비(천원)'] >= 476036:\n        folium.Circle(\n            location=[water_df.iloc[i]['위도'], water_df.iloc[i]['경도']],\n            radius=1000,  # 반지름(단위: 미터)\n            color='black').add_to(mm_1)\n\n\n\n# 지도 출력\nmm_1\n\n\n# HTML 파일로 저장\n#mm.save(\"폐수처리장.html\")\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "Datamining/Uber/data_analysis_uber.html",
    "href": "Datamining/Uber/data_analysis_uber.html",
    "title": "Uber",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom h3 import h3\nfrom collections import Counter\nimport pydeck as pdk"
  },
  {
    "objectID": "Datamining/Uber/data_analysis_uber.html#데이터의-기본적인-indexing-및-slicing",
    "href": "Datamining/Uber/data_analysis_uber.html#데이터의-기본적인-indexing-및-slicing",
    "title": "Uber",
    "section": "1. 데이터의 기본적인 indexing 및 slicing",
    "text": "1. 데이터의 기본적인 indexing 및 slicing\n\n# 처음 5개의 행을 출력\nprint(df.head())\n\n          Date/Time      Lat      Lon    Base\n0  4/1/2014 0:11:00  40.7690 -73.9549  B02512\n1  4/1/2014 0:17:00  40.7267 -74.0345  B02512\n2  4/1/2014 0:21:00  40.7316 -73.9873  B02512\n3  4/1/2014 0:28:00  40.7588 -73.9776  B02512\n4  4/1/2014 0:33:00  40.7594 -73.9722  B02512\n\n\n\n# 'Lat' 열의 값만 출력\nprint(df['Lat'])\n\n0         40.7690\n1         40.7267\n2         40.7316\n3         40.7588\n4         40.7594\n           ...   \n564511    40.7640\n564512    40.7629\n564513    40.7443\n564514    40.6756\n564515    40.6880\nName: Lat, Length: 564516, dtype: float64\n\n\n\n# 10행부터 20행까지 출력\nprint(df.iloc[10:21])\n\n           Date/Time      Lat      Lon    Base\n10  4/1/2014 1:19:00  40.7256 -73.9869  B02512\n11  4/1/2014 1:48:00  40.7591 -73.9684  B02512\n12  4/1/2014 1:49:00  40.7271 -73.9803  B02512\n13  4/1/2014 2:11:00  40.6463 -73.7896  B02512\n14  4/1/2014 2:25:00  40.7564 -73.9167  B02512\n15  4/1/2014 2:31:00  40.7666 -73.9531  B02512\n16  4/1/2014 2:43:00  40.7580 -73.9761  B02512\n17  4/1/2014 3:22:00  40.7238 -73.9821  B02512\n18  4/1/2014 3:35:00  40.7531 -74.0039  B02512\n19  4/1/2014 3:35:00  40.7389 -74.0393  B02512\n20  4/1/2014 3:41:00  40.7619 -73.9715  B02512\n\n\n\n# Base 갯수 확인\ndf.Base.value_counts()\n\nBase\nB02682    227808\nB02598    183263\nB02617    108001\nB02512     35536\nB02764      9908\nName: count, dtype: int64"
  },
  {
    "objectID": "Datamining/Uber/data_analysis_uber.html#결측치에-대한-처리",
    "href": "Datamining/Uber/data_analysis_uber.html#결측치에-대한-처리",
    "title": "Uber",
    "section": "2. 결측치에 대한 처리",
    "text": "2. 결측치에 대한 처리\n\n# 결측치 개수 확인\nprint(df.isnull().sum())\n\nDate/Time    0\nLat          0\nLon          0\nBase         0\ndtype: int64"
  },
  {
    "objectID": "Datamining/Uber/data_analysis_uber.html#기초적인-통계-추출-및-분석",
    "href": "Datamining/Uber/data_analysis_uber.html#기초적인-통계-추출-및-분석",
    "title": "Uber",
    "section": "3. 기초적인 통계 추출 및 분석",
    "text": "3. 기초적인 통계 추출 및 분석\n\n# 데이터 요약\nprint(df.describe())\n\n# 'Lat' 열의 평균\nprint(df['Lat'].mean())\n\n# 'Lon' 열의 중앙값\nprint(df['Lon'].median())\n\n# 'Base' 열에서 각 값의 빈도수 출력\nprint(df['Base'].value_counts())\n\n                 Lat            Lon\ncount  564516.000000  564516.000000\nmean       40.740005     -73.976817\nstd         0.036083       0.050426\nmin        40.072900     -74.773300\n25%        40.722500     -73.997700\n50%        40.742500     -73.984800\n75%        40.760700     -73.970000\nmax        42.116600     -72.066600\n40.74000520746974\n-73.9848\nBase\nB02682    227808\nB02598    183263\nB02617    108001\nB02512     35536\nB02764      9908\nName: count, dtype: int64"
  },
  {
    "objectID": "Datamining/Uber/data_analysis_uber.html#데이터에-대한-질문을-던져보고-해답을-찾기",
    "href": "Datamining/Uber/data_analysis_uber.html#데이터에-대한-질문을-던져보고-해답을-찾기",
    "title": "Uber",
    "section": "4. 데이터에 대한 질문을 던져보고 해답을 찾기",
    "text": "4. 데이터에 대한 질문을 던져보고 해답을 찾기\n\n4.1 월별/요일별 Uber 이용량 비교\n\n가장 Uber 이용량이 많은 요일은 언제일까요?\n\n\n# 3. 월별 이용량 비교\n\n#2020년 4월 데이터라서 의미가 없음\ndf['Date/Time'] = pd.to_datetime(df['Date/Time'])\ndf['month'] = df['Date/Time'].dt.month\nmothly_data = df.groupby('month').size().reset_index(name='count')\nprint(mothly_data)\n\n   month   count\n0      4  564516\n\n\n\n# 3. 일별 이용량 비교\n\n#2020년 4월 데이터라서 의미가 없음\ndf['day_of_week'] = pd.to_datetime(df['Date/Time'])\ndf['day_of_week'] = df['Date/Time'].dt.dayofweek\nweekly_data = df.groupby('month').size().reset_index(name='count')\nprint(weekly_data)\n\n   month   count\n0      4  564516\n\n\n\n# 데이터프레임에서 \"Date/Time\" 열을 datetime 형식으로 변환\ndf[\"Date/Time\"] = pd.to_datetime(df[\"Date/Time\"])\n\n# 월(month)과 요일(weekday) 열을 생성\ndf[\"month\"] = df[\"Date/Time\"].dt.month\ndf[\"weekday\"] = df[\"Date/Time\"].dt.dayofweek\n\n# 월별 및 요일별 Uber 이용량 계산\nuber_usage = df.groupby([\"month\", \"weekday\"]).size().reset_index(name=\"count\")\n\n# 가장 Uber 이용량이 많은 요일 추출\nmax_usage_day = uber_usage.loc[uber_usage[\"count\"].idxmax()]\n\n# 결과 출력\nprint(\"가장 Uber 이용량이 많은 요일은:\")\nprint(\"워: {}\".format(max_usage_day[\"month\"]))\nprint(\"요일: {}\".format(max_usage_day[\"weekday\"]))\nprint(\"이용량: {}\".format(max_usage_day[\"count\"]))\n\n가장 Uber 이용량이 많은 요일은:\n워: 4\n요일: 2\n이용량: 108631\n\n\n\ndays = [0:'Monday',1:'Tuseday',2:'Wednesday',3:'Friday',5:'Saturday',6:'Sunday']\n\n\nimport seaborn as sns\n\n#연속형을 범주형으로 \npenguins[\"bill_length_10\"] = (penguins[\"bill_length_mm\"]//10 )* 10\n\nfig, axes = plt.subplots(figsize=(8,4))\n\nsns.boxplot(x=\"bill_length_10\", y=\"bill_depth_mm\", data=penguins)\nsns.stripplot(x=\"bill_length_10\", y=\"bill_depth_mm\", color=\"black\", data=penguins,size = 4)\n\nplt.show()\n\n#fig.tight_layout()\n\n\nimport seaborn as sns\n\nplt.figure(figsize = (10,6))\n\nsns.barplot(x = \"dat_of_week\",y='count',data=weekly_data,color = 'grey')\n\nplt.xticks(rotation = 45)\n\nplt.show()\n\nValueError: Could not interpret input 'dat_of_week'\n\n\n<Figure size 1000x600 with 0 Axes>\n\n\n\n\n4.2 시간대별 이용량 비교\n\n가장 이용량이 많은 시간대는 언제일까요?\n\n\ndf['hour']=df['Date/Time']\n\n\nplt.figure(fizsize = (10,6))\n\nplt.ylabel('count')\nplt.show()\n\n\n\n4.3 Uber 수요에 대한 공간적인 시각화\n\n가장 수요가 많은 지역은 어디일까요?\n시간대/지역별로 수요의 변화를 나타내보고, 시사점을 도출해 봅시다\n\n\n# 위도와 경도를 사용하여 h3 셀 생성\ndf['h3'] = df.apply(lambda row: h3.geo_to_h3(row['Lat'], row['Lon'], 6), axis = 1) # 여기서 6은 해상도를 나타냅니다. 필요에 따라 조절 가능합니다. \n\n\ndf.h3.value_counts()\n\nh3\n862a100d7ffffff    247294\n862a1072fffffff    125962\n862a1008fffffff     60138\n862a10727ffffff     50036\n862a100dfffffff     26684\n                    ...  \n862a10cd7ffffff         1\n862a13c57ffffff         1\n862a10c0fffffff         1\n862a10397ffffff         1\n862a10b8fffffff         1\nName: count, Length: 229, dtype: int64\n\n\n\ndf.groupby('h3').size\n\n\nh3_counts = Counter(df['h3'])\n\n\ndf_h3_counts =pd.DataFrame.from_dict(h3_counts,orient='index').reset_index\ndf_h3_counts.colums = ['h3','count']\ndf_h3_counts['lat']=df_h3_counts['h3'].apply(lambda x: h3.h3_to_geo(x)[0])\ndf_h3_conts['lon']=df_h3_counts['h3'].apply(lambda x: h3.h3_to_geo(x)[1])\n\nAttributeError: 'method' object has no attribute 'colums'"
  },
  {
    "objectID": "Exercise/5. Proximity Analysis.html",
    "href": "Exercise/5. Proximity Analysis.html",
    "title": "5. Proximity Analysis",
    "section": "",
    "text": "Introduction\nYou are part of a crisis response team, and you want to identify how hospitals have been responding to crash collisions in New York City.\n\n\n\nBefore you get started, run the code cell below to set everything up.\n\nimport math\nimport pandas as pd\nimport geopandas as gpd\nfrom geopy.geocoders import Nominatim            # What you'd normally run\n#from learntools.geospatial.tools import Nominatim # Just for this exercise\n\nimport folium \nfrom folium import Marker\nfrom folium.plugins import MarkerCluster\n\nfrom folium.plugins import HeatMap\nfrom folium import Marker, GeoJson\n\nimport pandas as pd\nimport geopandas as gpd\n\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\n\nExercises\n\n1) Visualize the collision data.\nRun the code cell below to load a GeoDataFrame collisions tracking major motor vehicle collisions in 2013-2018.\n\ncollisions = gpd.read_file(\"D:/archive (1)/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp\")\ncollisions.head()\n\n\n\n\n\n  \n    \n      \n      DATE\n      TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET\n      CROSS STRE\n      OFF STREET\n      ...\n      CONTRIBU_2\n      CONTRIBU_3\n      CONTRIBU_4\n      UNIQUE KEY\n      VEHICLE TY\n      VEHICLE _1\n      VEHICLE _2\n      VEHICLE _3\n      VEHICLE _4\n      geometry\n    \n  \n  \n    \n      0\n      07/30/2019\n      0:00\n      BRONX\n      10464\n      40.841100\n      -73.784960\n      (40.8411, -73.78496)\n      NaN\n      NaN\n      121       PILOT STREET\n      ...\n      Unspecified\n      NaN\n      NaN\n      4180045\n      Sedan\n      Station Wagon/Sport Utility Vehicle\n      Station Wagon/Sport Utility Vehicle\n      NaN\n      NaN\n      POINT (1043750.211 245785.815)\n    \n    \n      1\n      07/30/2019\n      0:10\n      QUEENS\n      11423\n      40.710827\n      -73.770660\n      (40.710827, -73.77066)\n      JAMAICA AVENUE\n      188 STREET\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      4180007\n      Sedan\n      Sedan\n      NaN\n      NaN\n      NaN\n      POINT (1047831.185 198333.171)\n    \n    \n      2\n      07/30/2019\n      0:25\n      NaN\n      NaN\n      40.880318\n      -73.841286\n      (40.880318, -73.841286)\n      BOSTON ROAD\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      4179575\n      Sedan\n      Station Wagon/Sport Utility Vehicle\n      NaN\n      NaN\n      NaN\n      POINT (1028139.293 260041.178)\n    \n    \n      3\n      07/30/2019\n      0:35\n      MANHATTAN\n      10036\n      40.756744\n      -73.984590\n      (40.756744, -73.98459)\n      NaN\n      NaN\n      155       WEST 44 STREET\n      ...\n      NaN\n      NaN\n      NaN\n      4179544\n      Box Truck\n      Station Wagon/Sport Utility Vehicle\n      NaN\n      NaN\n      NaN\n      POINT (988519.261 214979.320)\n    \n    \n      4\n      07/30/2019\n      10:00\n      BROOKLYN\n      11223\n      40.600090\n      -73.965910\n      (40.60009, -73.96591)\n      AVENUE T\n      OCEAN PARKWAY\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      4180660\n      Station Wagon/Sport Utility Vehicle\n      Bike\n      NaN\n      NaN\n      NaN\n      POINT (993716.669 157907.212)\n    \n  \n\n5 rows × 30 columns\n\n\n\n“LATITUDE” 및 “LONGITUDE” 열을 사용하여 충돌 데이터를 시각화하는 대화형 지도를 만듭니다. 어떤 유형의 지도가 가장 효과적이라고 생각하십니까?\n\nm_1 = folium.Map(location=[40.7, -74], zoom_start=11) \n\nHeatMap(data=collisions[['LATITUDE', 'LONGITUDE']], radius=15).add_to(m_1)\n\nm_1\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n2) Understand hospital coverage.\nRun the next code cell to load the hospital data.\n\nhospitals = gpd.read_file(\"D:/archive (1)/nyu_2451_34494/nyu_2451_34494/nyu_2451_34494.shp\")\nhospitals.head()\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      address\n      zip\n      factype\n      facname\n      capacity\n      capname\n      bcode\n      xcoord\n      ycoord\n      latitude\n      longitude\n      geometry\n    \n  \n  \n    \n      0\n      317000001H1178\n      BRONX-LEBANON HOSPITAL CENTER - CONCOURSE DIVI...\n      1650 Grand Concourse\n      10457\n      3102\n      Hospital\n      415\n      Beds\n      36005\n      1008872.0\n      246596.0\n      40.843490\n      -73.911010\n      POINT (1008872.000 246596.000)\n    \n    \n      1\n      317000001H1164\n      BRONX-LEBANON HOSPITAL CENTER - FULTON DIVISION\n      1276 Fulton Ave\n      10456\n      3102\n      Hospital\n      164\n      Beds\n      36005\n      1011044.0\n      242204.0\n      40.831429\n      -73.903178\n      POINT (1011044.000 242204.000)\n    \n    \n      2\n      317000011H1175\n      CALVARY HOSPITAL INC\n      1740-70 Eastchester Rd\n      10461\n      3102\n      Hospital\n      225\n      Beds\n      36005\n      1027505.0\n      248287.0\n      40.848060\n      -73.843656\n      POINT (1027505.000 248287.000)\n    \n    \n      3\n      317000002H1165\n      JACOBI MEDICAL CENTER\n      1400 Pelham Pkwy\n      10461\n      3102\n      Hospital\n      457\n      Beds\n      36005\n      1027042.0\n      251065.0\n      40.855687\n      -73.845311\n      POINT (1027042.000 251065.000)\n    \n    \n      4\n      317000008H1172\n      LINCOLN MEDICAL & MENTAL HEALTH CENTER\n      234 E 149 St\n      10451\n      3102\n      Hospital\n      362\n      Beds\n      36005\n      1005154.0\n      236853.0\n      40.816758\n      -73.924478\n      POINT (1005154.000 236853.000)\n    \n  \n\n\n\n\n“위도” 및 “경도” 열을 사용하여 병원 위치를 시각화합니다.\n\nm_2 = folium.Map(location=[40.7, -74], zoom_start=11) \n\nfor idx, row in hospitals.iterrows():\n    Marker([row['latitude'], row['longitude']]).add_to(m_2)\n\n\nm_2\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n3) 가장 가까운 병원이 10km 이상 떨어진 때는 언제였습니까?\n가장 가까운 병원에서 10km 이상 떨어진 곳에서 충돌이 발생한 ’collisions’의 모든 행을 포함하는 DataFrame ’outside_range’를 만듭니다.\n‘병원’과 ’충돌’ 모두 좌표 기준 시스템으로 EPSG 2263을 사용하고 EPSG 2263의 단위는 미터입니다.\n\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nmy_union = coverage.geometry.unary_union\noutside_range = collisions.loc[~collisions[\"geometry\"].apply(lambda x: my_union.contains(x))]\n\n다음 코드 셀은 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌 비율을 계산합니다.\n\npercentage = round(100*len(outside_range)/len(collisions), 2)\nprint(\"Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(percentage))\n\nPercentage of collisions more than 10 km away from the closest hospital: 15.12%\n\n\n\n\n4) Make a recommender.\n먼 곳에서 충돌이 발생하면 부상자를 가장 가까운 병원으로 이송하는 것이 더욱 중요해집니다.\n이를 염두에 두고 다음과 같은 추천자를 만들기로 결정합니다. - 충돌 위치(EPSG 2263에서)를 입력으로 사용합니다. - 가장 가까운 병원(EPSG 2263에서 거리 계산이 수행되는 곳)을 찾고, - 가장 가까운 병원의 이름을 반환합니다.\n\ndef best_hospital(collision_location):\n    name = value_hospital\n    return name\n\n# Test your function: this should suggest CALVARY HOSPITAL INC\nprint(best_hospital(outside_range.geometry.iloc[0]))\n\nNameError: name 'value_hospital' is not defined\n\n\n\n\n5) 수요가 가장 많은 병원은?\n‘outside_range’ DataFrame의 충돌만 고려했을 때 가장 추천하는 병원은?\n응답은 4)에서 생성한 함수에서 반환한 병원 이름과 정확히 일치하는 Python 문자열이어야 합니다.\n\n\n6) Where should the city construct new hospitals?\nRun the next code cell (without changes) to visualize hospital locations, in addition to collisions that occurred more than 10 kilometers away from the closest hospital.\n\nm_6 = folium.Map(location=[40.7, -74], zoom_start=11) \n\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nfolium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m_6)\nHeatMap(data=outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m_6)\nfolium.LatLngPopup().add_to(m_6)\n\nembed_map(m_6, 'm_6.html')"
  }
]